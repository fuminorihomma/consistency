diff --git a/.gitignore b/.gitignore
index 3740917..91583fc 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,3 +1,16 @@
+.ipynb_checkpoints/
+Checkpoint_act_clf/
+Checkpoint_clf/
+LMRL.zip
+LMRL/
+TorchFly_old/
+consistency/
+*.log
+logs/
+*.ipynb
+models/
+*.pyc
+example.log
 .vector_cache
 ARDM/
 Checkpoint/
diff --git a/AgentProfile/profiles_in_dev.py b/AgentProfile/profiles_in_dev.py
index 459a836..d1ffe1e 100644
--- a/AgentProfile/profiles_in_dev.py
+++ b/AgentProfile/profiles_in_dev.py
@@ -1,13 +1,14 @@
 import re
 import sys
 sys.path.append("../")
-from classifier.pred import strategy_model
+# from classifier.pred import strategy_model
 import config as cfg
 from utils import is_repetition_with_context
 import itertools
 from AgentProfile.core import SystemAct
 from KnowledgeBase import KB
 from KnowledgeBase.KB import Domain
+import pdb
 
 from nltk.tokenize import sent_tokenize
 from copy import deepcopy
@@ -19,6 +20,7 @@ from sklearn.metrics.pairwise import cosine_similarity
 
 import torch
 import torch.nn as nn
+import pdb
 
 # HOW_ARE_YOU = 'how-are-you'
 # HEARD_OF_THE_ORG = 'heard-of-the-org'
@@ -31,7 +33,7 @@ import torch.nn as nn
 class GlobalProfile(object):
     def __init__(self, domain):
         self.domain = domain
-        self.pred_model = strategy_model(model_to_load="./classifier/best_model_state_er.pkl")
+        # self.act_clf_model = act_clf_model
         self.sentiment_analyzer = SentimentIntensityAnalyzer()
         self.sent_embedding_model = SentenceTransformer('bert-base-nli-mean-tokens', 
                                                         device=torch.device("cuda:1"))#('roberta-large-nli-stsb-mean-tokens')
@@ -412,7 +414,7 @@ class GlobalProfile(object):
                 if "yes" in sent or "i would like to donate some money" in sent or "i can donate a bit" in sent\
                     or "how much do you suggest" in sent:
                     answers['usr'] = self.domain.YES
-                elif "prefer to donate time" in sent or "next time" in sent: 
+                elif "prefer to donate time" in sent or "next time" in sent or "not today" in sent: 
                     answers['usr'] = self.domain.NO
                 else:
                     if score['compound'] >= 0.05:
@@ -433,7 +435,7 @@ class GlobalProfile(object):
                 if "yes" in sent or "i would like to donate some money" in sent or "i can donate a bit" in sent\
                     or "how much do you suggest" in sent:
                     answers['sys'] = self.domain.YES
-                elif "prefer to donate time" in sent or "next time" in sent: 
+                elif "prefer to donate time" in sent or "next time" in sent or "not today" in sent: 
                     answers['sys'] = self.domain.NO
                 else:
                     if score['compound'] >= 0.05:
@@ -452,9 +454,11 @@ class GlobalProfile(object):
             # not asked
             if who == self.domain.USR:
                 if "i would like to donate some money" in sent or "i can donate a bit" in sent\
-                    or "how much do you suggest" in sent:
+                    or "how much do you suggest" in sent or "spare" in sent or "glad to help" in sent\
+                    or "i will donate" in sent:
                     answers['usr'] = self.domain.YES
-                elif "prefer to donate time" in sent or "next time" in sent: 
+                elif "prefer to donate time" in sent or "next time" in sent or "not today" in sent\
+                or "not interested" in sent or "don't want to" in sent: 
                     answers['usr'] = self.domain.NO
 
                 # 2.1.2) asked-user speak-about system
@@ -472,8 +476,12 @@ class GlobalProfile(object):
             
                 # 1.2.2) asked-system speak-about user
                 if "thank you so much" in sent or "you are very kind" in sent \
-                or "you decided to donate " in sent or "thank you for your donation today" in sent:
+                or "you decided to donate" in sent or "you decide to donate" in sent or "thank you for your donation today" in sent or "you are willing to donate" in sent\
+                or "appreciate your donation" in sent or "appreciate your willingness to donate" in sent:
                     answers['usr'] = self.domain.YES
+                
+                elif "ould you like to donate some of your task payment" in sent:
+                    answers['usr'] = self.domain.INIT
 
         return answers
 
@@ -608,6 +616,7 @@ class GlobalProfile(object):
 
     def check_conflict(self, sents, sent_acts):
         # 1. repetition
+        # pdb.set_trace()
         fail_reason = None
         rep_status_with_sys, rep_amount_with_sys, edited_sents, edited_sent_acts = self.sys_world.check_conflict(sents, sent_acts)
         if self.last_sents is None:
@@ -622,6 +631,7 @@ class GlobalProfile(object):
         if rep_condition:
             # if it's not a repetition, then we need to check for consistency
             consis_status = self.check_consistency(edited_sents, edited_sent_acts)
+            # pdb.set_trace()
             consis_condition = consis_status in [cfg.PASS]
 
             rep_consis_condition = rep_condition and consis_condition
@@ -636,6 +646,7 @@ class GlobalProfile(object):
         return rep_consis_condition, rep_amount, edited_sents, edited_sent_acts, fail_reason
 
     def check_consistency(self, sents, sent_acts):
+        
         to_update_dic_usr, to_update_dic_sys = self.extract_info(sents, who=self.domain.SYS)
         consis_status = cfg.PASS
         for att, answer in to_update_dic_usr.items():
@@ -648,20 +659,31 @@ class GlobalProfile(object):
             if self.sys_world.sys_profile[att] != self.domain.INIT \
                and self.sys_world.sys_profile[att] != answer:
                 consis_status = cfg.INCONSISTENCY
-        
+        for sent in sents:
+            if "you decided to donate" in sent:
+                pass
+                # pdb.set_trace()
         return consis_status
 
-    def regex_label(self, sys_texts, context, turn_i):
+    def regex_label(self, model_clf, sys_texts, which_task):
         """
         regex to re-label 
         vs 
         QA    to re-label
         """
-        def regex_label_for_one_utt(utt):
-            predicted_label = self.pred_model.predict(text=utt, his=context, turn=turn_i)
-
+        predicted_labels, past = model_clf.predict(separate_sents=sys_texts, 
+                                                      which_task=which_task)
+        
+        def regex_label_for_one_utt(utt, predicted_label):
+            # try:
+            #     predicted_label = self.pred_model.predict(text=utt, his=context, turn=turn_i)
+            # except:
+            #     import pdb
+            #     pdb.set_trace()
+            
+            sent = utt.lower()
             if predicted_label in ["task-related-inquiry", "personal-related-inquiry"]:#, "have-you-heard-of-the-org"]:
-                sent = utt.lower()
+                
                 if self.sents_are_similar(sent, ['do you have children', 
                                                  'do you have kids',
                                                  'are you a parent',
@@ -693,10 +715,13 @@ class GlobalProfile(object):
                 else:
                     label = SystemAct.other_inquiry
 
-            elif predicted_label in ["have-you-heard-of-the-org"]:
+            elif predicted_label in ["greeting"]:
+                label = SystemAct.greeting_inquiry
+
+            elif predicted_label in ["source-related-inquiry"]:
                 label = SystemAct.organization_related_inquiry
 
-            elif predicted_label in ["propose-donation"]:
+            elif predicted_label in ["proposition-of-donation", "ask-donation-amount"]:
                 label = SystemAct.propose_donation_inquiry 
 
             elif "ask" in predicted_label: 
@@ -706,17 +731,24 @@ class GlobalProfile(object):
                 label = SystemAct.greeting_inquiry
             ##====== above are all inquiries =======
 
-            elif predicted_label in ['provide-org-facts', 'provide-donation-procedure']:
+            elif predicted_label in ['credibility-appeal', 'donation-information']:#['provide-org-facts', 'provide-donation-procedure']:
                 label = predicted_label
 
             else:
+                if self.sents_are_similar(sent, ['would you like to make a donation to Save the Children?', 
+                                                 'I was just wondering if you would be willing to donate a portion of your task payment to Save the Children',
+                                                 'Would you be willing to donate a portion of your payment',
+                                                 'how much do you like to donate to the charity now',
+                                                 'if you would like to consider a small portion of your payment']):
+                    label = SystemAct.propose_donation_inquiry
+
                 label = predicted_label
 
             return label
 
-        labels = [regex_label_for_one_utt(utt) for utt in sys_texts]
-
-        return labels
+        labels = [regex_label_for_one_utt(utt, predicted_label) for utt, predicted_label in zip(sys_texts, predicted_labels)]
+        # print(f"predicted: {labels}")
+        return labels, past
 
 
 class IndividualWorld(object):
@@ -800,7 +832,7 @@ class UsrWorld(IndividualWorld):
                 if self.is_inquiry_answered(sys_text, sys_label):
                     # 1.1 real repetition, 
                     # this is repetition inquiry
-                    if cfg.debug:
+                    if cfg.verbose:
                         print("{} inquiry encountered in user_profile check! {}: {}\n".format(cfg.REPETITION, sys_label, sys_text))
                     return cfg.REPETITION, repetition_ratio
                 else:
@@ -914,6 +946,7 @@ class SysWorld(IndividualWorld):
         self.sys_profile.update(to_update_dic_sys)
 
         for sys_text, sys_label in zip(sys_texts, sys_labels):
+            # pdb.set_trace()
             if sys_label in self.sent_profile:
                 self.sent_profile[sys_label].append(sys_text)
             else:
@@ -951,7 +984,8 @@ class SysWorld(IndividualWorld):
                             return cfg.PASS, repetition_ratio
                         # 2.2 real repetition
                         else:
-                            print("{} encountered! {}: {}".format(cfg.REPETITION, sys_label, sys_text))
+                            if cfg.verbose:
+                                print("{} encountered! {}: {}".format(cfg.REPETITION, sys_label, sys_text))
                             return cfg.REPETITION, repetition_ratio
 
                     else:
diff --git a/GPTModel.py b/GPTModel.py
new file mode 100644
index 0000000..78ede8f
--- /dev/null
+++ b/GPTModel.py
@@ -0,0 +1,254 @@
+from transformers import GPT2Model, GPT2LMHeadModel, GPT2Tokenizer, GPT2Config#, Block
+from torch.nn import CrossEntropyLoss
+import torch
+import torch.nn as nn
+import pdb
+
+class GPT2LMHeadModel_modified(GPT2LMHeadModel):
+    def __init__(self, config):
+        super().__init__(config)
+        self.transformer = GPT2Model_modified(config)
+        # self.past_max_len = config.n_ctx
+        # print(f"max_past_len = {self.past_max_len}")
+
+    # def forward(
+    #     self,
+    #     input_ids=None,
+    #     past=None,
+    #     attention_mask=None,
+    #     token_type_ids=None,
+    #     position_ids=None,
+    #     head_mask=None,
+    #     inputs_embeds=None,
+    #     labels=None,
+    # ):
+
+    #     input_ids_temp = input_ids
+    #     inputs_embeds_temp = inputs_embeds
+    #     token_type_ids_temp = token_type_ids
+
+    #     if past is not None:
+    #         if input_ids is not None:
+    #             input_ids_temp = input_ids[:, -1:]
+    #         if inputs_embeds is not None:
+    #             inputs_embeds_temp = inputs_embeds[:, -1:]
+    #         if token_type_ids is not None:
+    #             token_type_ids_temp = token_type_ids[:, -1:]
+            
+    #     if input_ids_temp is not None and inputs_embeds_temp is not None:
+    #         raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
+    #     elif input_ids_temp is not None:
+    #         input_shape = input_ids_temp.size()
+    #         input_ids_temp = input_ids_temp.view(-1, input_shape[-1])
+    #         batch_size = input_ids_temp.shape[0]
+    #     elif inputs_embeds_temp is not None:
+    #         input_shape = inputs_embeds_temp.size()[:-1]
+    #         batch_size = inputs_embeds_temp.shape[0]
+    #     else:
+    #         raise ValueError("You have to specify either input_ids or inputs_embeds")
+
+    #     if past is None:
+    #         past_length = 0
+    #         # past = [None] * len(self.transformer.h)
+    #     else:
+    #         past_length = past[0][0].size(-2)
+
+    #     import pdb
+        
+
+    #     if input_shape[-1] + past_length >= self.past_max_len:
+    #         pdb.set_trace()
+    #         truncated_past = [p for p in past]
+ 
+    #     try:
+    #         transformer_outputs = self.transformer(
+    #             input_ids,
+    #             past=past,
+    #             attention_mask=attention_mask,
+    #             token_type_ids=token_type_ids,
+    #             position_ids=position_ids,
+    #             head_mask=head_mask,
+    #             inputs_embeds=inputs_embeds,
+    #             # use_cache=use_cache,
+    #         )
+    #     except:
+    #         pdb.set_trace()
+    #     hidden_states = transformer_outputs[0]
+
+    #     lm_logits = self.lm_head(hidden_states)
+
+    #     outputs = (lm_logits,) + transformer_outputs[1:]
+    #     if labels is not None:
+    #         # Shift so that tokens < n predict n
+    #         shift_logits = lm_logits[..., :-1, :].contiguous()
+    #         shift_labels = labels[..., 1:].contiguous()
+    #         # Flatten the tokens
+    #         loss_fct = CrossEntropyLoss()
+    #         loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
+    #         outputs = (loss,) + outputs
+
+    #     return outputs  # (loss), lm_logits, presents, (all hidden_states), (attentions)
+
+
+class GPT2Model_modified(GPT2Model):
+    def __init__(self, config, split_into=4):
+        super().__init__(config)
+        self.past_max_len = config.n_ctx
+        print(f"max_past_len = {self.past_max_len}")
+
+    def forward(
+        self,
+        input_ids=None,
+        past=None,
+        attention_mask=None,
+        token_type_ids=None,
+        position_ids=None,
+        head_mask=None,
+        inputs_embeds=None,
+    ):
+        r"""
+    Return:
+        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.GPT2Config`) and inputs:
+        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):
+            Sequence of hidden-states at the last layer of the model.
+        past (:obj:`List[torch.FloatTensor]` of length :obj:`config.n_layers` with each tensor of shape :obj:`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`):
+            Contains pre-computed hidden-states (key and values in the attention blocks).
+            Can be used (see `past` input) to speed up sequential decoding. The token ids which have their past given to this model
+            should not be passed as input ids as they have already been computed.
+        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):
+            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
+            of shape :obj:`(batch_size, sequence_length, hidden_size)`.
+
+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
+        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):
+            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape
+            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.
+
+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
+            heads.
+
+    Examples::
+
+        from transformers import GPT2Tokenizer, GPT2Model
+        import torch
+
+        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
+        model = GPT2Model.from_pretrained('gpt2')
+        input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute", add_special_tokens=True)).unsqueeze(0)  # Batch size 1
+        outputs = model(input_ids)
+        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple
+
+        """
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
+        elif input_ids is not None:
+            input_shape = input_ids.size()
+            input_ids = input_ids.view(-1, input_shape[-1])
+        elif inputs_embeds is not None:
+            input_shape = inputs_embeds.size()[:-1]
+        else:
+            raise ValueError("You have to specify either input_ids or inputs_embeds")
+
+        if token_type_ids is not None:
+            token_type_ids = token_type_ids.view(-1, input_shape[-1])
+        if position_ids is not None:
+            position_ids = position_ids.view(-1, input_shape[-1])
+
+        if past is None:
+            past_length = 0
+            past = [None] * len(self.h)
+        else:
+            past_length = past[0][0].size(-2)
+        if position_ids is None:
+            device = input_ids.device if input_ids is not None else inputs_embeds.device
+            if input_shape[-1] + past_length > self.past_max_len:
+                past = [p[:, :, :, -(self.past_max_len-input_shape[-1]):, :] for p in past]
+                position_ids = torch.arange(self.past_max_len - input_shape[-1], self.past_max_len, dtype=torch.long, device=device)
+            else:
+                position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)
+            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
+
+        # Attention mask.
+        if attention_mask is not None:
+            attention_mask = attention_mask.view(-1, input_shape[-1])
+            # We create a 3D attention mask from a 2D tensor mask.
+            # Sizes are [batch_size, 1, 1, to_seq_length]
+            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
+            # this attention mask is more simple than the triangular masking of causal attention
+            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
+            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
+
+            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
+            # masked positions, this operation will create a tensor which is 0.0 for
+            # positions we want to attend and -10000.0 for masked positions.
+            # Since we are adding it to the raw scores before the softmax, this is
+            # effectively the same as removing these entirely.
+            attention_mask = attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility
+            attention_mask = (1.0 - attention_mask) * -10000.0
+
+        # Prepare head mask if needed
+        # 1.0 in head_mask indicate we keep the head
+        # attention_probs has shape bsz x n_heads x N x N
+        # head_mask has shape n_layer x batch x n_heads x N x N
+        if head_mask is not None:
+            if head_mask.dim() == 1:
+                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)
+                head_mask = head_mask.expand(self.config.n_layer, -1, -1, -1, -1)
+            elif head_mask.dim() == 2:
+                head_mask = (
+                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)
+                )  # We can specify head_mask for each layer
+            head_mask = head_mask.to(
+                dtype=next(self.parameters()).dtype
+            )  # switch to fload if need + fp16 compatibility
+        else:
+            head_mask = [None] * self.config.n_layer
+
+        if inputs_embeds is None:
+            inputs_embeds = self.wte(input_ids)
+        position_embeds = self.wpe(position_ids)
+        if token_type_ids is not None:
+            token_type_embeds = self.wte(token_type_ids)
+        else:
+            token_type_embeds = 0
+        hidden_states = inputs_embeds + position_embeds + token_type_embeds
+        hidden_states = self.drop(hidden_states)
+
+        output_shape = input_shape + (hidden_states.size(-1),)
+
+        presents = ()
+        all_attentions = []
+        all_hidden_states = ()
+        for i, (block, layer_past) in enumerate(zip(self.h, past)):
+            if self.output_hidden_states:
+                all_hidden_states = all_hidden_states + (hidden_states.view(*output_shape),)
+
+            outputs = block(
+                hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i]
+            )
+
+            hidden_states, present = outputs[:2]
+            if self.output_past:
+                presents = presents + (present,)
+
+            if self.output_attentions:
+                all_attentions.append(outputs[2])
+
+        hidden_states = self.ln_f(hidden_states)
+
+        hidden_states = hidden_states.view(*output_shape)
+        # Add last hidden state
+        if self.output_hidden_states:
+            all_hidden_states = all_hidden_states + (hidden_states,)
+
+        outputs = (hidden_states,)
+        if self.output_past:
+            outputs = outputs + (presents,)
+        if self.output_hidden_states:
+            outputs = outputs + (all_hidden_states,)
+        if self.output_attentions:
+            # let the number of heads free (-1) so we can extract attention even after head pruning
+            attention_output_shape = input_shape[:-1] + (-1,) + all_attentions[0].shape[-2:]
+            all_attentions = tuple(t.view(*attention_output_shape) for t in all_attentions)
+            outputs = outputs + (all_attentions,)
+        return outputs  # last hidden state, (presents), (all hidden_states), (attentions)
diff --git a/GPTModel1.py b/GPTModel1.py
new file mode 100644
index 0000000..a9bae36
--- /dev/null
+++ b/GPTModel1.py
@@ -0,0 +1,348 @@
+from transformers import GPT2Model, GPT2LMHeadModel, GPT2Tokenizer, GPT2Config#, Block
+from torch.nn import CrossEntropyLoss
+import torch
+import torch.nn as nn
+import pdb
+
+def move_to_device(past, target):
+    try:
+        try:
+            target_device = target.device
+        except:
+            target_device = next(target.named_parameters())[1].device
+        
+        if past is not None:
+            if type(past) is list:
+                if past[0].device != target_device:
+                    past = [p.to(target_device) for p in past]
+            else:
+                if past.device != target_device:
+                    past = past.to(target_device)
+        return past
+    except:
+        pdb.set_trace()
+
+class GPT2LMHeadModel_modified(GPT2LMHeadModel):
+    def __init__(self, config):
+        super().__init__(config)
+        # self.device = device
+        # self.config = config
+        # self.past_max_len = config.n_ctx
+        # print(f"max_past_len = {self.past_max_len}")
+
+    def forward(self, input_ids=None, past=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None,
+                labels=None):
+        transformer_outputs = self.transformer(input_ids,
+                                               past=past,
+                                               attention_mask=attention_mask,
+                                               token_type_ids=token_type_ids,
+                                               position_ids=position_ids,
+                                               head_mask=head_mask,
+                                               inputs_embeds=inputs_embeds)
+        hidden_states = transformer_outputs[0]
+
+        hidden_states = move_to_device(hidden_states, self.lm_head)
+        lm_logits = self.lm_head(hidden_states)
+
+        outputs = (lm_logits,) + transformer_outputs[1:]
+        if labels is not None:
+            # Shift so that tokens < n predict n
+            shift_logits = lm_logits[..., :-1, :].contiguous()
+            shift_labels = labels[..., 1:].contiguous()
+            # Flatten the tokens
+            loss_fct = CrossEntropyLoss(ignore_index=-1)
+            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),
+                            shift_labels.view(-1))
+            outputs = (loss,) + outputs
+
+        return outputs  # (loss), lm_logits, presents, (all hidden_states), (attentions)
+
+    def set_variables(self, device, split_into):
+        self.device = device
+        self.transformer = GPT2Model_modified(self.config, device, split_into)
+
+    def to(self, device):
+        self.transformer.to()
+        self.lm_head.to(device)
+    # def forward(
+    #     self,
+    #     input_ids=None,
+    #     past=None,
+    #     attention_mask=None,
+    #     token_type_ids=None,
+    #     position_ids=None,
+    #     head_mask=None,
+    #     inputs_embeds=None,
+    #     labels=None,
+    # ):
+
+    #     input_ids_temp = input_ids
+    #     inputs_embeds_temp = inputs_embeds
+    #     token_type_ids_temp = token_type_ids
+
+    #     if past is not None:
+    #         if input_ids is not None:
+    #             input_ids_temp = input_ids[:, -1:]
+    #         if inputs_embeds is not None:
+    #             inputs_embeds_temp = inputs_embeds[:, -1:]
+    #         if token_type_ids is not None:
+    #             token_type_ids_temp = token_type_ids[:, -1:]
+            
+    #     if input_ids_temp is not None and inputs_embeds_temp is not None:
+    #         raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
+    #     elif input_ids_temp is not None:
+    #         input_shape = input_ids_temp.size()
+    #         input_ids_temp = input_ids_temp.view(-1, input_shape[-1])
+    #         batch_size = input_ids_temp.shape[0]
+    #     elif inputs_embeds_temp is not None:
+    #         input_shape = inputs_embeds_temp.size()[:-1]
+    #         batch_size = inputs_embeds_temp.shape[0]
+    #     else:
+    #         raise ValueError("You have to specify either input_ids or inputs_embeds")
+
+    #     if past is None:
+    #         past_length = 0
+    #         # past = [None] * len(self.transformer.h)
+    #     else:
+    #         past_length = past[0][0].size(-2)
+
+    #     import pdb
+        
+
+    #     if input_shape[-1] + past_length >= self.past_max_len:
+    #         pdb.set_trace()
+    #         truncated_past = [p for p in past]
+ 
+    #     try:
+    #         transformer_outputs = self.transformer(
+    #             input_ids,
+    #             past=past,
+    #             attention_mask=attention_mask,
+    #             token_type_ids=token_type_ids,
+    #             position_ids=position_ids,
+    #             head_mask=head_mask,
+    #             inputs_embeds=inputs_embeds,
+    #             # use_cache=use_cache,
+    #         )
+    #     except:
+    #         pdb.set_trace()
+    #     hidden_states = transformer_outputs[0]
+
+    #     lm_logits = self.lm_head(hidden_states)
+
+    #     outputs = (lm_logits,) + transformer_outputs[1:]
+    #     if labels is not None:
+    #         # Shift so that tokens < n predict n
+    #         shift_logits = lm_logits[..., :-1, :].contiguous()
+    #         shift_labels = labels[..., 1:].contiguous()
+    #         # Flatten the tokens
+    #         loss_fct = CrossEntropyLoss()
+    #         loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
+    #         outputs = (loss,) + outputs
+
+    #     return outputs  # (loss), lm_logits, presents, (all hidden_states), (attentions)
+
+
+class GPT2Model_modified(GPT2Model):
+    def __init__(self, config, device, split_into):
+        super().__init__(config)
+        self.past_max_len = config.n_ctx
+        # self.config = config
+        print(f"max_past_len = {self.past_max_len}")
+
+        total_num_devices = torch.cuda.device_count()
+        print(f"{total_num_devices} devices are available!")
+        if split_into > total_num_devices:
+            print(f"try to split to {split_into} devices")
+            raise Exception("split into more than available devices")
+
+        self.device = device
+        self.split_into = split_into
+        if device.type == "cuda":
+            devices = range(device.index, device.index+split_into)
+            self.devices = [f"cuda:{d%total_num_devices}" for d in devices]
+        else:
+            devices = ["cpu"]*split_into
+
+    def to(self):
+        if self.split_into > 1:
+            device_i = 0
+            self.wte.to(self.devices[0])
+            self.wpe.to(self.devices[0])
+            # self.drop = nn.Dropout(config.embd_pdrop)
+            
+            assert self.config.n_layer%self.split_into == 0
+            N = int(self.config.n_layer/self.split_into)        
+            device_i += 1
+            
+            for i, block in enumerate(self.h):
+                block.to(self.devices[device_i])
+                if (i+1) % N == 0:
+                    device_i = (device_i+1)%(self.split_into)
+            
+            self.ln_f.to(self.devices[device_i])
+        
+
+    def forward(
+        self,
+        input_ids=None,
+        past=None,
+        attention_mask=None,
+        token_type_ids=None,
+        position_ids=None,
+        head_mask=None,
+        inputs_embeds=None,
+    ):
+        r"""
+    Return:
+        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.GPT2Config`) and inputs:
+        last_hidden_state (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`):
+            Sequence of hidden-states at the last layer of the model.
+        past (:obj:`List[torch.FloatTensor]` of length :obj:`config.n_layers` with each tensor of shape :obj:`(2, batch_size, num_heads, sequence_length, embed_size_per_head)`):
+            Contains pre-computed hidden-states (key and values in the attention blocks).
+            Can be used (see `past` input) to speed up sequential decoding. The token ids which have their past given to this model
+            should not be passed as input ids as they have already been computed.
+        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):
+            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
+            of shape :obj:`(batch_size, sequence_length, hidden_size)`.
+
+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
+        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):
+            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape
+            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.
+
+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
+            heads.
+
+    Examples::
+
+        from transformers import GPT2Tokenizer, GPT2Model
+        import torch
+
+        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
+        model = GPT2Model.from_pretrained('gpt2')
+        input_ids = torch.tensor(tokenizer.encode("Hello, my dog is cute", add_special_tokens=True)).unsqueeze(0)  # Batch size 1
+        outputs = model(input_ids)
+        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple
+
+        """
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError("You cannot specify both input_ids and inputs_embeds at the same time")
+        elif input_ids is not None:
+            input_shape = input_ids.size()
+            input_ids = input_ids.view(-1, input_shape[-1])
+        elif inputs_embeds is not None:
+            input_shape = inputs_embeds.size()[:-1]
+        else:
+            raise ValueError("You have to specify either input_ids or inputs_embeds")
+
+        if token_type_ids is not None:
+            token_type_ids = token_type_ids.view(-1, input_shape[-1])
+        if position_ids is not None:
+            position_ids = position_ids.view(-1, input_shape[-1])
+
+        if past is None:
+            past_length = 0
+            past = [None] * len(self.h)
+        else:
+            past_length = past[0][0].size(-2)
+        if position_ids is None:
+            device = input_ids.device if input_ids is not None else inputs_embeds.device
+            if input_shape[-1] + past_length > self.past_max_len:
+                past = [p[:, :, :, -(self.past_max_len-input_shape[-1]):, :] for p in past]
+                position_ids = torch.arange(self.past_max_len - input_shape[-1], self.past_max_len, dtype=torch.long, device=device)
+            else:
+                position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)
+            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])
+
+        # Attention mask.
+        if attention_mask is not None:
+            attention_mask = attention_mask.view(-1, input_shape[-1])
+            # We create a 3D attention mask from a 2D tensor mask.
+            # Sizes are [batch_size, 1, 1, to_seq_length]
+            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]
+            # this attention mask is more simple than the triangular masking of causal attention
+            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.
+            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
+
+            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
+            # masked positions, this operation will create a tensor which is 0.0 for
+            # positions we want to attend and -10000.0 for masked positions.
+            # Since we are adding it to the raw scores before the softmax, this is
+            # effectively the same as removing these entirely.
+            attention_mask = attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility
+            attention_mask = (1.0 - attention_mask) * -10000.0
+
+        # Prepare head mask if needed
+        # 1.0 in head_mask indicate we keep the head
+        # attention_probs has shape bsz x n_heads x N x N
+        # head_mask has shape n_layer x batch x n_heads x N x N
+        if head_mask is not None:
+            if head_mask.dim() == 1:
+                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)
+                head_mask = head_mask.expand(self.config.n_layer, -1, -1, -1, -1)
+            elif head_mask.dim() == 2:
+                head_mask = (
+                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)
+                )  # We can specify head_mask for each layer
+            head_mask = head_mask.to(
+                dtype=next(self.parameters()).dtype
+            )  # switch to fload if need + fp16 compatibility
+        else:
+            head_mask = [None] * self.config.n_layer
+
+        if inputs_embeds is None:
+            input_ids = move_to_device(input_ids, self.wte)
+            inputs_embeds = self.wte(input_ids)
+        position_ids = move_to_device(position_ids, self.wpe)
+        position_embeds = self.wpe(position_ids)
+        if token_type_ids is not None:
+            token_type_ids = move_to_device(token_type_ids, self.wte)            
+            token_type_embeds = self.wte(token_type_ids)
+        else:
+            token_type_embeds = 0
+        hidden_states = inputs_embeds + position_embeds + token_type_embeds
+        hidden_states = self.drop(hidden_states)
+
+        output_shape = input_shape + (hidden_states.size(-1),)
+
+        presents = ()
+        all_attentions = []
+        all_hidden_states = ()
+        for i, (block, layer_past) in enumerate(zip(self.h, past)):
+            hidden_states = move_to_device(hidden_states, block)      
+            layer_past = move_to_device(layer_past, block)      
+            if self.output_hidden_states:
+                all_hidden_states = all_hidden_states + (hidden_states.view(*output_shape),)
+
+            outputs = block(
+                hidden_states, layer_past=layer_past, attention_mask=attention_mask, head_mask=head_mask[i]
+            )
+
+            hidden_states, present = outputs[:2]
+            if self.output_past:
+                presents = presents + (present,)
+
+            if self.output_attentions:
+                all_attentions.append(outputs[2])
+
+        hidden_states = move_to_device(hidden_states, self.ln_f)  
+        hidden_states = self.ln_f(hidden_states)
+
+        hidden_states = hidden_states.view(*output_shape)
+        # Add last hidden state
+        if self.output_hidden_states:
+            all_hidden_states = all_hidden_states + (hidden_states,)
+
+        outputs = (hidden_states,)
+        if self.output_past:
+            outputs = outputs + (presents,)
+        if self.output_hidden_states:
+            outputs = outputs + (all_hidden_states,)
+        if self.output_attentions:
+            # let the number of heads free (-1) so we can extract attention even after head pruning
+            attention_output_shape = input_shape[:-1] + (-1,) + all_attentions[0].shape[-2:]
+            all_attentions = tuple(t.view(*attention_output_shape) for t in all_attentions)
+            outputs = outputs + (all_attentions,)
+        return outputs  # last hidden state, (presents), (all hidden_states), (attentions)
+
diff --git a/KnowledgeBase/KB.py b/KnowledgeBase/KB.py
index df7a550..821389b 100644
--- a/KnowledgeBase/KB.py
+++ b/KnowledgeBase/KB.py
@@ -52,28 +52,36 @@ class HumanRule(object):
                int: one candidate selected
                str: no candidate selected, should append the returned sentence to the end
         """
-        print("\n\n\n--------- rule enforce --------------")
+        # if cfg.rl_finetune:
+        #     return None
+        if cfg.verbose:
+            print("\n\n\n--------- rule enforce --------------")
         if self.chatbot.turn_i >= cfg.HAVE_TO_PROPOSE:
             # have to propose donation at this turn if it hasn't proposed yet
             enforced_acts = [SystemAct.propose_donation_inquiry, SystemAct.PROVIDE_DONATION_PROCEDURE]
             enforced_templates = self.sys_template.get_template(enforced_acts)
-            if SystemAct.propose_donation_inquiry not in self.chatbot.global_profile.sys_world.sent_profile.keys():
+            if (self.chatbot.global_profile.usr_world.usr_profile[self.chatbot.domain.WANT_TO_DONATE] != self.chatbot.domain.INIT)\
+                or SystemAct.propose_donation_inquiry not in self.chatbot.global_profile.sys_world.sent_profile.keys():
+            # if SystemAct.propose_donation_inquiry not in self.chatbot.global_profile.sys_world.sent_profile.keys():
                 # we should enforce rule
                 # we should check the enforced templates are not repetition
                 is_repetition, repetition_score = is_repetition_with_context(enforced_templates[0], 
                                               itertools.chain(*self.chatbot.global_profile.sys_world.sent_profile.values()), 
                                               threshold=cfg.repetition_threshold)
                 if is_repetition:
-                    print("case 1")
-                    print(enforced_templates[0])
+                    if cfg.verbose:
+                        print("case 1")
+                        print(enforced_templates[0])
                     return None
                 else:
                     # for i, acts in enumerate(sent_act_candidates):
                     for act in sent_acts:
                         if act == SystemAct.propose_donation_inquiry:
-                            print("case 2")
+                            if cfg.verbose:
+                                print("case 2")
                             return True
-                    print("case 3")
+                    if cfg.verbose:
+                        print("case 3")
                     return enforced_templates, enforced_acts # didn't find appropriate candidates, so we append this sentence 
 
                 # edited_enforced_templates = []
@@ -90,10 +98,12 @@ class HumanRule(object):
                 
 
             else:
-                print("case 4")
+                if cfg.verbose:
+                    print("case 4")
                 return None
         
-        print("case 5")
+        if cfg.verbose:
+            print("case 5")
         return None
                 
  
\ No newline at end of file
diff --git a/PPO.py b/PPO.py
index d5e3d41..f0862bd 100644
--- a/PPO.py
+++ b/PPO.py
@@ -1,5 +1,14 @@
 #!/usr/bin/env python
 # coding: utf-8
+# logging is important
+import logging
+logging.basicConfig(filename='ppo1.log', level=logging.INFO)
+# logging.basicConfig(filename='hello2.log', level=logging.INFO)
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.INFO)
+import pdb
+# pdb.set_trace()
+from tqdm import tqdm
 
 # In[37]:
 import os
@@ -36,8 +45,9 @@ def run_demo(demo_fn, world_size):
 
 
 from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config
-from gpt_model import GPT2SimpleLM, GPT2MultipleChoiceHead
-from pytorch_pretrained_bert import GPT2Tokenizer, OpenAIAdam, GPT2Model
+# from gpt_model import GPT2SimpleLM, GPT2MultipleChoiceHead
+from GPTModel1 import GPT2LMHeadModel_modified
+from pytorch_pretrained_bert import OpenAIAdam
 from PersuasionInteract import PersuasiveBot
 from nltk.tokenize import sent_tokenize
 import config as cfg
@@ -55,6 +65,7 @@ import pdb
 
 #from transformers import WarmupLinearSchedule
 from apex.optimizers import FusedLAMB, FusedAdam
+from transformers import AdamW
 # from torchfly.transformers import UnifiedTokenizer, GPT2SimpleLM
 from torchfly.modules.losses import SequenceFocalLoss, SequenceCrossEntropyLoss
 # from torchfly.decode import top_filtering
@@ -63,10 +74,6 @@ from torchfly.modules.losses import SequenceFocalLoss, SequenceCrossEntropyLoss
 # In[38]:
 
 
-# logging is important
-logging.basicConfig()
-logger = logging.getLogger(__name__)
-logger.setLevel(logging.INFO)
 
 class PpoParams:
     ppo_epoch = 2
@@ -94,36 +101,42 @@ class PersuadeDataset(Dataset):
         self.tokenizer.max_len = 1500
         # tokenizer weird behavior
         self.turn_ending = [628, 198]
-        # tokenizer.encode("\n\n\n")
-        
+        # tokenizer.encode("\n\n\n")        
     def __len__(self):
-        return len(self.data)
-    
+        return len(self.data)    
     def __getitem__(self, index):
-        dial_tokens = [tokenizer.encode(item) + self.turn_ending for item in self.data[index]]
+        dial_tokens = [self.tokenizer.encode(item[:2]) + self.tokenizer.encode(item[2:]) + self.turn_ending for item in self.data[index]]
         dial_sents = [item[2:] for item in self.data[index]]#self.data[index]#[item[2:] for item in self.data[index]]
         role_ids = [0 if item.startswith("A:") else 1 for item in self.data[index]]
-        return role_ids, dial_tokens, dial_sents
-        
+        return role_ids, dial_tokens, dial_sents        
     def collate(self, unpacked_data):
         return unpacked_data
 
-
-tokenizer = GPT2Tokenizer.from_pretrained("gpt2")#torch.load(tokenizer_dir)
-device1 = torch.device("cuda:2")
-device2 = torch.device("cuda:3")
-
-def load_model(cfg):
+def load_model(cfg, device1, device2, split_into, model_A_dir=None):
     if cfg.model_size == "small":
-        model_A = GPT2LMHeadModel.from_pretrained("gpt2", output_hidden_states=True)
-        model_B = GPT2LMHeadModel.from_pretrained("gpt2", output_hidden_states=True)
+        # lm_config1 = GPT2Config().from_pretrained('gpt2')
+        # lm_config2 = GPT2Config().from_pretrained('gpt2')
+        # lm_config1.output_hidden_states = True
+        # lm_config2.output_hidden_states = True
+        # model_A = GPT2LMHeadModel_modified(config=lm_config1, device=device1, split_into=split_into)
+        # model_B = GPT2LMHeadModel_modified(config=lm_config2, device=device2, split_into=split_into)
+        model_A = GPT2LMHeadModel_modified.from_pretrained("gpt2", output_hidden_states=True)
+        model_A.set_variables(device=device1, split_into=split_into)
+        model_B = GPT2LMHeadModel_modified.from_pretrained("gpt2", output_hidden_states=True)
+        model_B.set_variables(device=device2, split_into=split_into)
     elif cfg.model_size == "medium":
-        model_A = GPT2LMHeadModel.from_pretrained("gpt2-medium", output_hidden_states=True)
-        model_B = GPT2LMHeadModel.from_pretrained("gpt2-medium", output_hidden_states=True)
-
-    model_A = model_A.to(device1)
-    model_B = model_B.to(device2)
-
+        # lm_config1 = GPT2Config().from_pretrained('gpt2-medium')
+        # lm_config2 = GPT2Config().from_pretrained('gpt2-medium')
+        # lm_config1.output_hidden_states = True
+        # lm_config2.output_hidden_states = True
+        # model_A = GPT2LMHeadModel_modified(config=lm_config1, device=device1, split_into=split_into)
+        # model_B = GPT2LMHeadModel_modified(config=lm_config2, device=device2, split_into=split_into)
+        model_A = GPT2LMHeadModel_modified.from_pretrained("gpt2-medium", output_hidden_states=True)
+        model_A.set_variables(device=device1, split_into=split_into)
+        model_B = GPT2LMHeadModel_modified.from_pretrained("gpt2-medium", output_hidden_states=True)
+        model_B.set_variables(device=device2, split_into=split_into)
+
+   # pdb.set_trace()
     # load the model
     if cfg.model_size == "small":
         model_A_states, model_B_states = torch.load(cfg.new_small_model_dir)
@@ -131,16 +144,29 @@ def load_model(cfg):
         if cfg.use_old_model:
             model_A_states, model_B_states = torch.load(cfg.old_medium_model_dir)
             model_A_states['transformer.wte.weight'] = model_A_states['transformer.wte.weight'][:50257,:]
+            model_A_states['lm_head.weight'] = model_A_states['lm_head.decoder.weight'][:50257,:]
             model_B_states['transformer.wte.weight'] = model_B_states['transformer.wte.weight'][:50257,:]
+            model_B_states['lm_head.weight'] = model_B_states['lm_head.decoder.weight'][:50257,:]
+            print("loaded old medium model")
         else:
             model_A_states, model_B_states = torch.load(cfg.new_medium_model_dir)
 
-    model_A.load_state_dict(model_A_states, strict=False)
-    model_B.load_state_dict(model_B_states, strict=False)
-
+    if model_A_dir is not None:
+        model_A_states = torch.load(model_A_dir)
+        print("loaded RL-NEW model!!!")
+    if cfg.use_old_model:
+        strict = False
+    else:
+        strict = True
+    model_A.load_state_dict(model_A_states, strict=strict)
+    model_B.load_state_dict(model_B_states, strict=strict)
+
+    # to device
+    model_A.to(device1)
+    model_B.to(device2)
+ 
     return model_A, model_B
 
-model_A, model_B = load_model(cfg)
 
 
 # In[42]:
@@ -240,8 +266,8 @@ class CustomRewardFunc:
 class Actor(PersuasiveBot):
     """Text Generation
     """
-    def __init__(self, model_A, model_B, tokenizer):
-        super().__init__(model_A=model_A, model_B=model_B, tokenizer=tokenizer)
+    def __init__(self, model_A, model_B, tokenizer, device1, device2):
+        super().__init__(model_A=model_A, model_B=model_B, tokenizer=tokenizer, device1=device1, device2=device2)
         
         train_data = torch.load("DataProcess/train_dialogs.pkl")
         val_data = torch.load("DataProcess/val_dialogs.pkl")
@@ -275,112 +301,149 @@ class Actor(PersuasiveBot):
         self.model_A.eval()
         self.model_B.eval()
         
-        contexts, sents, rewards, context_ids = [], [], [], []
-        for _ in range(sample_size):
-            if mode is None:
-                mode = np.random.choice([cfg.self_play_mode, cfg.supervised_mode], replace=False, 
-                                        p=[PpoParams.self_play_prob, 1-PpoParams.self_play_prob])
-            print(f"in mode: {mode}")
-            if mode == cfg.supervised_mode:
-                batch = self.train_dataset[np.random.choice(len(self.train_dataset))]
-                role_ids, dial_tokens, dial_sents = batch
-                dial_inputs = [torch.LongTensor(item).unsqueeze(0).to(device) for item in dial_tokens]
-
-                print(f"len: {len(role_ids)}")
-                for role_id, dial_turn_inputs, dial_sent in zip(role_ids, dial_inputs, dial_sents):
-                    print(f"turn #: {self.turn_i}\n\n\n")
-                    # pdb.set_trace()
-                    if self.turn_i > 9:
-                        break
-                    
-                    if role_id == 0:
+        final_contexts, final_sents, final_rewards, final_context_ids = [], [], [], []
+        with torch.no_grad():
+            for _ in range(sample_size):
+                if mode is None:
+                    if self.dialog_i > 0:#True: #self.dialog_i > 0:
+                        mode = np.random.choice([cfg.self_play_mode, cfg.supervised_mode], replace=False, 
+                                                p=[PpoParams.self_play_prob, 1-PpoParams.self_play_prob])
+                    else:
+                        mode = cfg.self_play_mode
+                logger.info(f"in mode: {mode}")
+                if mode == cfg.supervised_mode:
+                    batch = self.train_dataset[np.random.choice(len(self.train_dataset))]
+                    role_ids, dial_tokens, dial_sents = batch
+                    dial_inputs = []
+                    for item in dial_tokens:
+                        if item[0] == 32:
+                            dial_inputs.append(torch.LongTensor(item).unsqueeze(0).to(self.device1))
+                        else:
+                            dial_inputs.append(torch.LongTensor(item).unsqueeze(0).to(self.device2))
+
+                    print(f"len: {len(role_ids)}")
+                    for role_id, dial_turn_inputs, dial_sent in zip(role_ids, dial_inputs, dial_sents):
+                        print(f"turn #: {self.turn_i}\n\n\n")
+                        # pdb.set_trace()
+                        # if self.turn_i > 9:
+                        #     break
+                        # if dial_turn_inputs[0]
+                        if role_id == 0:
+                            if self.past is None:
+                                user_text = ""
+                            response, [sents_success, sents_failed], have_enough_candidates = self.chat(input_text=user_text, mode=mode)
+                            ground_truth = dial_sent
+                            try:
+                                assert not ground_truth.startswith("A:")
+                            except:
+                                pdb.set_trace()
+                            cur_rewards = self.reward_func([ground_truth, sents_success, sents_failed], have_enough_candidates, with_ground_truth=True)
+
+                            # print(f"truth: {ground_truth}")
+                            # print(f"sent_success: \n{sents_success}")
+                            # print(f"sent_failed: \n{sents_failed}")
+                            # update
+                            ground_truth_sents = sent_tokenize(ground_truth)                    
+                            sent_acts, _ = self.global_profile.regex_label(self.model_clf,
+                                                                ground_truth_sents, 
+                                                                which_task="A")
+                            self.global_profile.update(sents=ground_truth_sents, sent_labels=sent_acts, who=self.domain.SYS) #self.last_sys_labels = self.sys_profile.update(sys_texts=sents, sys_labels=sent_acts)
+                            
+                            # pdb.set_trace()
+                            try:
+                                assert self.tokenizer.decode(dial_turn_inputs[0][:2].tolist()) == "A:"
+                            except:
+                                pdb.set_trace()
+                            if self.past is not None and self.model_A.device != self.past[0].device:
+                                past = [p.to(self.model_A.device) for p in self.past]
+                                self.past = past
+                            _, self.past, hidrden_states = self.model_A(dial_turn_inputs, past=self.past)
+                            self.model_clf.set_past(sent=ground_truth, 
+                                                    which_task="A")
+
+                            # put in replay buffer
+                            for sent, reward in zip([ground_truth] + sents_success + sents_failed, cur_rewards):
+                                final_contexts.append(deepcopy(self.contexts))
+                                final_sents.append("A:"+sent)
+                                final_rewards.append(reward)
+                                final_context_ids.append(f"{self.dialog_i}-{self.turn_i}")
+                                # self.replay_buffer.add([deepcopy(self.contexts), "A:"+sent, reward])
+
+                            # update contexts
+                            logging.info(f"sys: {ground_truth}")
+                            self.contexts.append("A:"+ground_truth)
+
+                        else:
+                            # breakpoint()
+                            user_text = dial_sent
+                            try:
+                                assert not user_text.startswith("B:")
+                            except:
+                                pdb.set_trace()
+                            self.contexts.append("B:"+user_text)
+                            print(f"user: {user_text}")
+                            logging.info(f"user: {user_text}")
+                            # logits, past = model_B(dial_turn_inputs, past=past)
+                            # all_logits.append(logits)
+
+                    # finish tail
+                    if role_id == 1: # the last sent is user
+                        # throw away the last user sentence
+                        pass
+
+                else:
+                    # is_end = False
+                    while True:
+                        print(f"turn #: {self.turn_i}\n\n\n")
+                        # pdb.set_trace()
                         if self.past is None:
                             user_text = ""
-                        response, [sents_success, sents_failed], have_enough_candidates = self.chat(input_text=user_text, mode=mode)
-                        ground_truth = dial_sent
-                        cur_rewards = self.reward_func([ground_truth, sents_success, sents_failed], have_enough_candidates, with_ground_truth=True)
-
-                        print(f"truth: {ground_truth}")
-                        print(f"sent_success: \n{sents_success}")
-                        print(f"sent_failed: \n{sents_failed}")
-                        # update
-                        sents = sent_tokenize(ground_truth)                    
-                        sent_acts = self.global_profile.regex_label(sents, self.last_sent, self.turn_i)
-                        self.global_profile.update(sents=sents, sent_labels=sent_acts, who=self.domain.SYS) #self.last_sys_labels = self.sys_profile.update(sys_texts=sents, sys_labels=sent_acts)
-                        _, self.past, hidden_states = self.model_A(dial_turn_inputs, past=self.past)
+                        else:
+                            user_text = self.generate_user_utt_self_play()                
+                        # system-side
+                            if "bye" in user_text.lower() or "have a great day" in user_text.lower() \
+                                or "have a great night" in user_text.lower() \
+                                or "have a good day" in user_text.lower() \
+                                or "have a good night" in user_text.lower() \
+                                or "have a nice day" in user_text.lower() \
+                                or "have a nice night" in user_text.lower() \
+                                or self.turn_i >= 10:
+                                break
+
+                            self.contexts.append("B:"+user_text)
+                            print(f"user: {user_text}")
+                            logging.info(f"user: {user_text}")
+                        sys_sent, [sents_success, sents_failed], have_enough_candidates = self.sys_respond_and_update(mode=mode)
+                        cur_rewards = self.reward_func([sents_success, sents_failed], have_enough_candidates, with_ground_truth=False)
 
                         # put in replay buffer
-                        for sent, reward in zip([ground_truth] + sents_success + sents_failed, cur_rewards):
-                            contexts.append(deepcopy(self.contexts))
-                            sents.append("A:"+sent)
-                            rewards.append(reward)
-                            context_ids.append(f"{self.dialog_i}-{self.turn_i}")
+                        for sent, reward in zip(sents_success + sents_failed, cur_rewards):
+                            final_contexts.append(deepcopy(self.contexts))
+                            final_sents.append("A:"+sent)
+                            final_rewards.append(reward)
+                            final_context_ids.append(f"{self.dialog_i}-{self.turn_i}")
                             # self.replay_buffer.add([deepcopy(self.contexts), "A:"+sent, reward])
 
                         # update contexts
-                        self.contexts.append("A:"+ground_truth)
+                        self.contexts.append("A:"+sys_sent)
+                        print(f"sys: {sys_sent}")
+                        logging.info(f"sys: {sys_sent}")
 
-                    else:
-                        # breakpoint()
-                        user_text = dial_sent
-                        self.contexts.append("B:"+user_text)
-                        print(f"user: {user_text}")
-                        # logits, past = model_B(dial_turn_inputs, past=past)
-                        # all_logits.append(logits)
-
-                # finish tail
-                if role_id == 1: # the last sent is user
-                    # throw away the last user sentence
-                    pass
+                        turn_responses = ["usr: "+user_text,
+                                        "sys: "+sys_sent]
 
-            else:
-                # is_end = False
-                while True:
-                    print(f"turn #: {self.turn_i}\n\n\n")
-                    # pdb.set_trace()
-                    if self.past is None:
-                        user_text = ""
-                    else:
-                        user_text = self.generate_user_utt_self_play()                
-                    # system-side
-                        if "bye" in user_text.lower() or "have a great day" in user_text.lower() \
-                            or "have a great night" in user_text.lower() \
-                            or "have a good day" in user_text.lower() \
-                            or "have a good night" in user_text.lower() \
-                            or self.turn_i >= 10:
-                            break
-
-                        self.contexts.append("B:"+user_text)
-                        print(f"user: {user_text}")
-                    sys_sent, [sents_success, sents_failed], have_enough_candidates = self.sys_respond_and_update(mode=mode)
-                    cur_rewards = self.reward_func([sents_success, sents_failed], have_enough_candidates, with_ground_truth=False)
-
-                    # put in replay buffer
-                    for sent, reward in zip(sents_success + sents_failed, cur_rewards):
-                        contexts.append(deepcopy(self.contexts))
-                        sents.append("A:"+sent)
-                        rewards.append(reward)
-                        context_ids.append(f"{self.dialog_i}-{self.turn_i}")
-                        # self.replay_buffer.add([deepcopy(self.contexts), "A:"+sent, reward])
-
-                    # update contexts
-                    self.contexts.append("A:"+sys_sent)
-                    print(f"sys: {sys_sent}")
-
-                    turn_responses = ["usr: "+user_text,
-                                    "sys: "+sys_sent]
-
-                    self.logs['global_profiles'].append(self.global_profile.get_profiles())
-                    self.logs['responses'].append(turn_responses)
-
-            self.dialog_i += 1
-            self.reload()
-        return contexts, sents, rewards, context_ids
+                        self.logs['global_profiles'].append(self.global_profile.get_profiles())
+                        self.logs['responses'].append(turn_responses)
+
+                self.dialog_i += 1
+                self.reload()
+            assert len(final_contexts) == len(final_sents) == len(final_rewards) == len(final_context_ids)
+            return final_contexts, final_sents, final_rewards, final_context_ids
 
 class Trainer:
     """Reinforcement Learning Trainer
     """
-    def __init__(self, actor):
+    def __init__(self, actor, model_A, model_B, device1, device2):
         self.sample_size = 1 # num of dialog to sample at a time
         self.maxlen = 1024
         self.replay_buffer = ReplayBuffer(self.maxlen)
@@ -389,19 +452,30 @@ class Trainer:
         self.turn_ending = [628, 198]
         self.model_A = model_A
         self.model_B = model_B
+        self.device1 = device1
+        self.device2 = device2
+        assert self.model_A.device is self.device1
+        assert self.model_B.device is self.device2
+
+        self.trained_steps = 0
         # self.reward_func = CustomRewardFunc(tokenizer)
         
     def collect_generations(self, total_size=64, normalize_reward=True):
+        assert self.model_A.device is self.device1
+        assert self.model_B.device is self.device2
+
         logger.info("Collecting Samples")
         # define storage
         all_contexts, all_sents, all_encoded_sents, all_rewards, all_context_ids = [], [], [], [], []
         
         while total_size > 0:
+            assert self.model_A.device is self.device1
+            assert self.model_B.device is self.device2
+
             real_sample_size = min(self.sample_size, total_size)
             
             # sample sequences
-            contexts, sents, rewards, context_ids = self.actor.sample_generations(sample_size=real_sample_size,
-                                                                                  mode=cfg.self_play_mode)
+            contexts, sents, rewards, context_ids = self.actor.sample_generations(sample_size=real_sample_size)
             # actor.replay_buffer[0]
             # [[], 'A:Good morning, how are you this Sunday morning?', 2]
             
@@ -409,7 +483,12 @@ class Trainer:
             # pdb.set_trace()
             all_contexts.extend(contexts)
             all_sents.extend(sents)
-            encoded_sents = [self.tokenizer.encode(s) + self.turn_ending for s in sents]
+            encoded_sents = []
+            for s in sents:
+                if s.startswith("A:") or s.startswith("B:"):
+                    encoded_sents.append(self.tokenizer.encode(s[:2]) + self.tokenizer.encode(s[2:]) + self.turn_ending)
+                else:
+                    pdb.set_trace()
             all_encoded_sents.extend(encoded_sents)
             all_rewards.extend(rewards)
             all_context_ids.extend(context_ids)
@@ -426,6 +505,7 @@ class Trainer:
             final_rewards = all_rewards
         
         # pdb.set_trace()
+        assert len(all_contexts) == len(all_sents) == len(all_encoded_sents) == len(final_rewards) == len(all_context_ids)
         self.replay_buffer.add(zip(all_contexts, all_sents, all_encoded_sents, final_rewards, all_context_ids))
         
         # logging
@@ -434,6 +514,9 @@ class Trainer:
         return all_rewards
 
     def calculate_old_logprobs(self, buffer_contexts, buffer_context_ids, buffer_sents, buffer_encoded_sents):
+        assert self.model_A.device is self.device1
+        assert self.model_B.device is self.device2
+
         start = time.time()
         buffer_old_logprobs = [None] * len(buffer_sents)
         indices = np.arange(len(buffer_sents))
@@ -454,13 +537,17 @@ class Trainer:
                 batch_sents = make_batch_sequences(batch_sents, padding_value=PAD_TOKEN)
                 mask = batch_sents.ne(PAD_TOKEN).float()
                 # to device
-                batch_sents = batch_sents.to(device)
-                mask = mask.to(device)
+                batch_sents = batch_sents.to(self.device1)
+                mask = mask.to(self.device1)
 
                 # calculate past
-                past = self.make_past(context_map[context_id]['contexts'])                
+                past = self.make_past(context_map[context_id]['contexts']) 
+                # pdb.set_trace()               
                 if past is not None:
-                    past = [p.repeat(1, batch_sents.shape[0], 1, 1, 1) for p in past]
+                    past = [p.repeat(1, batch_sents.shape[0], 1, 1, 1).to(self.model_A.device) for p in past]
+                # if past is not None and self.model_A.device != past[0].device:
+                #     past = [p.to(self.model_A.device) for p in past]
+                
                 logits, past, hidden_states = self.model_A(batch_sents, past=past)
 
                 # prepare the loss func inputs
@@ -479,16 +566,26 @@ class Trainer:
         speed = (end - start) / len(buffer_sents)
 
         print(f"calculate_old_logprobs: {speed} per turn")
+        logger.info(f"calculate_old_logprobs: {speed} per turn")
         return buffer_old_logprobs
 
     def make_past(self, contexts):
         past = None
         for context in contexts:
-            encoded_context = self.tokenizer.encode(context) + self.turn_ending
-            encoded_context = torch.LongTensor(encoded_context).unsqueeze(0).to(device)
+            try:
+                assert context.startswith("A:") or context.startswith("B:")
+            except:
+                pdb.set_trace()
+            encoded_context = self.tokenizer.encode(context[:2]) + self.tokenizer.encode(context[2:]) + self.turn_ending
             if context.startswith("A:"):
+                encoded_context = torch.LongTensor(encoded_context).unsqueeze(0).to(self.device1)
+                if past is not None and self.model_A.device != past[0].device:
+                    past = [p.to(self.model_A.device) for p in past]
                 logits, past, _ = self.model_A(encoded_context, past=past)
             elif context.startswith("B:"):
+                encoded_context = torch.LongTensor(encoded_context).unsqueeze(0).to(self.device2)
+                if past is not None and self.model_B.device != past[0].device:
+                    past = [p.to(self.model_B.device) for p in past]
                 logits, past, _ = self.model_B(encoded_context, past=past)
             else:
                 raise ValueError(f"context: {context}")
@@ -496,21 +593,25 @@ class Trainer:
         return past
 
     def train_steps(self, total_steps):
-        for total_steps in range(total_steps):
+        for total_steps in tqdm(range(total_steps)):
+            self.trained_steps += 1
             start = time.time()
             
             all_rewards = trainer.collect_generations(total_size=PpoParams.num_dialogs_to_sample)
             
             self.model_A.train()
+            self.model_B.train()
             buffer_contexts, buffer_sents, buffer_encoded_sents, buffer_rewards, buffer_context_ids = zip(*trainer.replay_buffer)
             buffer_old_logprobs = self.calculate_old_logprobs(buffer_contexts, buffer_context_ids, buffer_sents, buffer_encoded_sents)
-            
-            for ppo_epoch in range(PpoParams.ppo_epoch):
+
+            for ppo_epoch in tqdm(range(PpoParams.ppo_epoch)):
                 indices = np.arange(len(buffer_rewards))
                 np.random.shuffle(indices)
 
                 for i in range(PpoParams.batchsize // PpoParams.mini_batchsize):
                     sampled_indices = indices[i * PpoParams.mini_batchsize : (i + 1) * PpoParams.mini_batchsize]
+                    if len(sampled_indices) == 0:
+                        break
                     sampled_contexts = [buffer_contexts[j] for j in sampled_indices]
                     sampled_sents = [buffer_sents[j] for j in sampled_indices]
                     sampled_encoded_sents = [buffer_encoded_sents[j] for j in sampled_indices]
@@ -521,9 +622,12 @@ class Trainer:
                     # logits_list = []
                     # target_list = []
                     # mask_list = []
-                    torch.cuda.empty_cache()
+                    # torch.cuda.empty_cache()
                     sequence_logprob_list = []
-                    batch_encoded_sents = make_batch_sequences(sampled_encoded_sents, padding_value=PAD_TOKEN)
+                    try:
+                        batch_encoded_sents = make_batch_sequences(sampled_encoded_sents, padding_value=PAD_TOKEN)
+                    except:
+                        pdb.set_trace()
                     for i, (contexts, old_logprob, reward) in enumerate(zip(sampled_contexts, sampled_old_logprobs, sampled_rewards)):
                         past = self.make_past(contexts)
                         encoded_sent = batch_encoded_sents[i, :].unsqueeze(0)
@@ -534,11 +638,13 @@ class Trainer:
                         mask = encoded_sent.ne(PAD_TOKEN).float()
 
                         # to device
-                        encoded_sent = encoded_sent.to(device)
+                        encoded_sent = encoded_sent.to(self.device1)
                         # old_logprob = old_logprob.to(device)
                         # reward = reward.to(device)
-                        mask = mask.to(device)
-                        
+                        mask = mask.to(self.device1)
+                        if past is not None and self.model_A.device != past[0].device:
+                            past = [p.to(self.model_A.device) for p in past]
+
                         logits, past, hidden_states = self.model_A(encoded_sent, past=past)
 
                         # prepare the loss func inputs
@@ -551,15 +657,15 @@ class Trainer:
                         # logits_list.append(logits)
                         # target_list.append(target)
                         # mask_list.append(mask)
-                        torch.cuda.empty_cache()
+                        # torch.cuda.empty_cache()
                     
                     # pdb.set_trace()
                     # logits = torch.cat(logits_list)
                     # target = torch.cat(target_list)
                     # mask = torch.cat(mask_list)
     
-                    old_logprobs = torch.FloatTensor(sampled_old_logprobs).to(device)
-                    sampled_rewards = torch.FloatTensor(sampled_rewards).to(device)
+                    old_logprobs = torch.FloatTensor(sampled_old_logprobs).to(self.device1)
+                    sampled_rewards = torch.FloatTensor(sampled_rewards).to(self.device1)
 
                     # calc advantages
                     advantages = sampled_rewards
@@ -569,7 +675,8 @@ class Trainer:
 
                     # here we need to calculate for the each token in the sequence
                     entropy = - (sequences_logprobs.exp() * sequences_logprobs).sum(1)
-                    print(f"entropy: {entropy}")
+                    # print(f"entropy: {entropy}")
+                    logger.info(f"entropy: {entropy}")
                     # entropy = entropy.clamp_min(min_entropy)
                     logprobs = sequences_logprobs.sum(1)
 
@@ -589,6 +696,7 @@ class Trainer:
 
                     # print the final clipfrac and approxl
                     print(f"Approx KL {approx_kl.item()}, Clip Frac {clipfrac.item()}")
+                    logger.info(f"Approx KL {approx_kl.item()}, Clip Frac {clipfrac.item()}")
 
                     # get the final loss
                     loss = policy_loss.mean() - entropy_coef * entropy.mean()
@@ -599,12 +707,30 @@ class Trainer:
                     # must clip the gradient for stability
                     torch.nn.utils.clip_grad_norm_(self.model_A.parameters(), 0.5)
                     optimizer.step()
+                    # scheduler.step()
                   
             print("Mean Reward", np.mean(all_rewards))
+            logger.info(f"Mean Reward: {np.mean(all_rewards)}")
             end = time.time()
             speed = (end - start) / PpoParams.batchsize
             print("Speed", speed)
+            logger.info(f"Speed: {speed}")
+
+            print(f"max memory A: {torch.cuda.max_memory_allocated(model_A.device)}")
+            logger.info(f"max memory A: {torch.cuda.max_memory_allocated(model_A.device)}")
+            print(f"max memory B: {torch.cuda.max_memory_allocated(model_B.device)}")
+            logger.info(f"max memory B: {torch.cuda.max_memory_allocated(model_B.device)}")
+            torch.save((self.model_A.state_dict(), self.model_B.state_dict()), f"Checkpoint/{self.trained_steps}_steps_{np.mean(all_rewards)}_reward_model_A.pth")
+
+NEW_MODEL_A_DIR = None#"Checkpoint/20_steps_0.049586776859504134_reward_model_A.pth"
 
+tokenizer = GPT2Tokenizer.from_pretrained("gpt2")#torch.load(tokenizer_dir)
+DEVICE1 = torch.device("cuda:1")
+DEVICE2 = torch.device("cuda:0")
+SPLIT_INTO = 2
+
+model_A, model_B = load_model(cfg, device1=DEVICE1, device2=DEVICE2, split_into=SPLIT_INTO, model_A_dir=NEW_MODEL_A_DIR)
+pdb.set_trace()
 
 PAD_TOKEN = tokenizer.encoder["<|endoftext|>"]
 clip_range = 0.2
@@ -613,24 +739,42 @@ min_entropy = 10.0 # depends on the task
 criterion = SequenceCrossEntropyLoss()
 
 
+actor = Actor(model_A=model_A, model_B=model_B, tokenizer=tokenizer, device1=DEVICE1, device2=DEVICE2)
+
+pdb.set_trace()
 # optimizer
 num_epochs = 10
 num_gradients_accumulation = 1
 num_train_optimization_steps = 1000
 
-param_optimizer = list(model_A.named_parameters())
+param_optimizer = list(model_A.named_parameters())# + list(model_B.named_parameters())
 no_decay = ['ln', 'bias', 'LayerNorm']
 optimizer_grouped_parameters = [
     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
     ]
 
-optimizer = FusedAdam(optimizer_grouped_parameters, 
-                    lr=1e-6,
-                    eps=1e-06,
-                    bias_correction=False)
-
+TOTAL_STEPS = 50
+num_train_optimization_steps = PpoParams.ppo_epoch * (PpoParams.batchsize // PpoParams.mini_batchsize) * TOTAL_STEPS
 
+from pytorch_pretrained_bert import OpenAIAdam
+optimizer = OpenAIAdam(optimizer_grouped_parameters,
+                       lr=2e-5,
+                       warmup=0.1,
+                       max_grad_norm=1.0,
+                       weight_decay=0.01,
+                       t_total=num_train_optimization_steps)
+from transformers import get_linear_schedule_with_warmup
+# optimizer = AdamW(optimizer_grouped_parameters,
+#                 lr=3e-5,
+#                 eps=1e-06)
+# scheduler = get_linear_schedule_with_warmup(optimizer,
+#                                  warmup_steps=100,
+#                                  num_training_steps=num_train_optimization_steps)
+# optimizer = FusedAdam(optimizer_grouped_parameters, 
+#                     lr=1e-6,
+#                     eps=1e-06,
+#                     bias_correction=False)
 
 
 
@@ -638,325 +782,31 @@ optimizer = FusedAdam(optimizer_grouped_parameters,
 
 
 
-actor = Actor(model_A, model_B, tokenizer)
-trainer = Trainer(actor=actor)
 
-trainer.train_steps(total_steps=1)
-pdb.set_trace()
-
-
-
-
-
-if False:
-
-
-
-    # In[47]:
-
-
-
-
-    # In[48]:
 
+trainer = Trainer(actor=actor, model_A=model_A, model_B=model_B, device1=DEVICE1, device2=DEVICE2)
 
-    sample_size = 16
-    actor = Actor(model, tokenizer)
-
-
-    # In[49]:
-
-
-    # optimizer
-    num_epochs = 10
-    num_gradients_accumulation = 1
-    num_train_optimization_steps = 1000
-
-    param_optimizer = list(model.named_parameters())
-    no_decay = ['ln', 'bias', 'LayerNorm']
-    optimizer_grouped_parameters = [
-        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
-        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
-        ]
-
-    optimizer = FusedAdam(optimizer_grouped_parameters, 
-                        lr=1e-6,
-                        eps=1e-06,
-                        bias_correction=False)
-
-    # scheduler = WarmupLinearSchedule(optimizer,
-    #                                  warmup_steps=300,
-    #                                  t_total=num_train_optimization_steps)
-
-
-    # In[50]:
-
-
-    
+if NEW_MODEL_A_DIR is None:
+    try:
+        trainer.train_steps(total_steps=TOTAL_STEPS)
+        print(torch.cuda.memory_summary(device=model_A.device, abbreviated=False))
+        print(torch.cuda.memory_summary(device=model_B.device, abbreviated=False))
 
+    except:
+        print(torch.cuda.memory_summary(device=model_A.device, abbreviated=False))
+        print(torch.cuda.memory_summary(device=model_B.device, abbreviated=False))
+else:
+    usr_text = ""
+    while True:
+        # interactive test
+        sys_sent, [sents_success, sents_failed], have_enough_candidates = actor.chat(usr_text)
+        print(f"sys: {sys_sent}\n")
+        # print(f"success:\n {sents_success}")
+        # print(f"failed:\n {sents_failed}")
 
-    # In[51]:
-
-
-    trainer = Trainer()
-
-
-    # In[52]:
-
-
-
-    # In[53]:
-
-
-
-
-
-    # In[54]:
-
-
-
-
-    # In[55]:
-
-
-    for total_steps in range(1000):
-        start = time.time()
-        
-        all_rewards = trainer.collect_generations(total_size=PpoParams.batchsize)
-        
-        buffer_contexts, buffer_sents, buffer_encoded_sents, buffer_rewards, buffer_context_ids = zip(*trainer.replay_buffer)
-        buffer_old_logprobs = calculate_old_logprobs(buffer_contexts, buffer_context_ids, buffer_sents, buffer_encoded_sents, )
-        
-        for ppo_epoch in range(PpoParams.ppo_epoch):
-            indices = np.arange(len(buffer_rewards))
-            np.random.shuffle(indices)
-
-            for i in range(PpoParams.batchsize // PpoParams.mini_batchsize):
-                sampled_indices = indices[i * PpoParams.mini_batchsize : (i + 1) * PpoParams.mini_batchsize]
-                sampled_contexts = [buffer_contexts[j] for j in sampled_indices]
-                sampled_sents = [buffer_sents[j] for j in sampled_indices]
-                sampled_encoded_sents = [buffer_encoded_sents[j] for j in sampled_indices]
-                sampled_old_logprobs = [buffer_old_logprobs[j] for j in sampled_indices]
-                sampled_rewards = [buffer_rewards[j] for j in sampled_indices]
-
-                # make batches
-                batch_pasts = [self.make_past(contexts) for contexts in sampled_contexts]
-                batch_sents = make_batch_sequences(sampled_encoded_sents, padding_value=PAD_TOKEN)
-                old_logprobs = torch.FloatTensor(sampled_old_logprobs)
-                sampled_rewards = torch.FloatTensor(sampled_rewards)
-                mask = batch_sequences.ne(PAD_TOKEN).float()
-
-                # to device
-                batch_sents = batch_sents.to(device)
-                old_logprobs = old_logprobs.to(device)
-                sampled_rewards = sampled_rewards.to(device)
-                mask = mask.to(device)
-
-                logits, past, hidden_states = self.model_A(batch_sents, past=past)
-
-                #logits, past = model(batch_sequences)
-
-                # prepare the loss func inputs
-                logits = logits[:, :-1].contiguous()
-                target = batch_sequences[:, 1:].contiguous()
-                mask = mask[:, 1:].contiguous()
-
-                # calc advantages
-                advantages = sampled_rewards
-
-                sequences_logprobs = - criterion(logits, target, mask)
-
-                # here we need to calculate for the each token in the sequence
-                entropy = - (sequences_logprobs.exp() * sequences_logprobs).sum(1)
-                entropy = entropy.clamp_min(min_entropy)
-                logprobs = sequences_logprobs.sum(1)
-
-                # shape: (batch)
-                ratio = (logprobs - old_logprobs).exp()
-                # shape: (batch)
-                policy_loss1 = - advantages * ratio
-                # shape: (batch)
-                policy_loss2 = - advantages * ratio.clamp(1.0 - clip_range, 1.0 + clip_range)
-                # shape: (batch)
-                policy_loss = torch.max(policy_loss1, policy_loss2)
-
-                # recording variables. no gradient!
-                with torch.no_grad():
-                    clipfrac = ((ratio - 1.0).abs() > clip_range).float().mean()
-                    approx_kl = (logprobs - old_logprobs).pow(2).mean()
-
-                # print the final clipfrac and approxl
-                print(f"Approx KL {approx_kl.item()}, Clip Frac {clipfrac.item()}")
-
-                # get the final loss
-                loss = policy_loss.mean() - entropy_coef * entropy.mean()
-
-                # update the model
-                optimizer.zero_grad()
-                loss.backward()
-                # must clip the gradient for stability
-                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
-                optimizer.step()
-            
-        print("Mean Reward", np.mean(all_rewards))
-        end = time.time()
-        speed = (end - start) / PpoParams.batchsize
-        print("Speed", speed)
-
-
-    # In[56]:
-
-
-    tokenizer.decode(sampled_sequences[2])
-
-
-    # In[57]:
-
-
-    eval_sample_size = 16
-    pred_sentences, logprobs = actor.sample_generations(eval_sample_size)
-
-    for i in range(eval_sample_size):
-        print(tokenizer.decode(pred_sentences[i]))
-        print()
-
-
-    # In[78]:
-
-
-
-
-
-    # In[58]:
-
-
-    old_logprobs
-
-
-    # In[139]:
-
-
-
-
-
-    # In[ ]:
-
-
-
-
-
-    # In[59]:
-
-
-    sampled_generated_sequences = generated_sequences[16:32]
-    sampled_rewards = rewards[16:32]
-
-    batch_sequences = make_batch_sequences(sampled_generated_sequences)
-    batch_sequences = batch_sequences.to(device)
-    mask = batch_sequences.ne(tokenizer.encoder["<pad>"]).float()
-    sampled_rewards = torch.FloatTensor(sampled_rewards).to(device)
-
-
-    # In[60]:
-
-
-    logits, past = model(batch_sequences)
-
-    # prepare the loss func inputs
-    logits = logits[:, :-1].contiguous()
-    target = batch_sequences[:, 1:].contiguous()
-    mask = mask[:, 1:].contiguous()
-
-
-    # In[61]:
-
-
-    # some temporary variables
-    advantages = sampled_rewards
-    clip_range = 0.2
-    entropy_coef = 1e-5
-    min_entropy = 10.0 # depends on the task
-
-
-    # In[62]:
-
-
-    sequences_logprobs = - criterion(logits, target, mask)
-    # here we need to calculate for the each token in the sequence
-    entropy = - (sequences_logprobs.exp() * sequences_logprobs).sum(1)
-    entropy = entropy.clamp_min(min_entropy)
-    logprobs = sequences_logprobs.sum(1)
-
-
-    # In[63]:
-
-
-    mask.shape
-
-
-    # In[64]:
-
-
-    # old_logprobs = logprobs
-
-
-    # In[65]:
-
-
-    # shape: (batch)
-    ratio = (logprobs - old_logprobs.detach()).exp()
-    # shape: (batch)
-    policy_loss1 = - advantages * ratio
-    # shape: (batch)
-    policy_loss2 = - advantages * ratio.clamp(1.0 - clip_range, 1.0 + clip_range)
-    # shape: (batch)
-    policy_loss = torch.max(policy_loss1, policy_loss2)
-
-
-    # In[66]:
-
-
-    print(ratio)
-
-
-    # In[67]:
-
-
-    # recording variables. no gradient!
-    with torch.no_grad():
-        clipfrac = ((ratio - 1.0).abs() > clip_range).float().mean()
-        approxl = 0.5 * (logprobs - old_logprobs).pow(2).mean()
-
-
-    # In[68]:
-
-
-    loss = policy_loss.mean() - entropy_coef * entropy.mean()
-
-
-    # In[69]:
-
-
-    optimizer.zero_grad()
-    loss.backward()
-    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
-    optimizer.step()
-
-
-    # In[70]:
-
-
-    print(clipfrac.item())
-    print(approxl.item())
-
-
-    # In[71]:
-
-
-    from torch.utils.tensorboard import SummaryWriter
-
+        usr_text = input("usr: ")
+# pdb.set_trace()
 
-    # In[ ]:
 
 
 
diff --git a/PersuasionInteract.py b/PersuasionInteract.py
index e901d3f..57e079b 100644
--- a/PersuasionInteract.py
+++ b/PersuasionInteract.py
@@ -1,9 +1,10 @@
 import os
-os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"
+os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,4,5,6"
 
 import re
 import dialog_config
 from AgentProfile.profiles_in_dev import GlobalProfile
+from model_clf import build_model_classifier
 
 import torch
 import torch.nn as nn
@@ -27,8 +28,10 @@ import os
 import csv
 import pickle as pkl
 
-from gpt_model import GPT2SimpleLM
-from pytorch_pretrained_bert import GPT2Tokenizer
+# from gpt_model import GPT2SimpleLM
+# from pytorch_pretrained_bert import GPT2Tokenizer
+from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config
+from GPTModel import GPT2LMHeadModel_modified
 import config as cfg
 from utils import is_repetition_with_context
 
@@ -40,47 +43,6 @@ import logging
 
 from copy import deepcopy
 
-# def handler(signal_received, frame):
-#     # Handle any cleanup here
-#     print('SIGINT or CTRL-C detected. Exiting gracefully')
-#     bot.save()
-#     exit(0)
-
-# In[3]:
-tokenizer_dir = "/home/wyshi/persuasion/consistency/ARDM/persuasion/special3_gpt2_tokenizer.pkl"
-model_dir = "/home/wyshi/persuasion/consistency/ARDM/persuasion/persuasion_medium_3.th"
-class PersuadeDataset(Dataset):
-    def __init__(self, data, tokenizer):
-        self.data = data
-        self.tokenizer = tokenizer
-        self.tokenizer.max_len = 1500
-        self.turn_ending = tokenizer.encode("\n\n\n")
-        self.dialog_ending = [tokenizer.encoder["[EOS]"]]
-        
-    def __len__(self):
-        return len(self.data)
-    
-    def __getitem__(self, index):
-        dial_tokens = [tokenizer.encode(item) + self.turn_ending for item in self.data[index]]
-        role_ids = [0 if item[0] == 32 else 1 for item in dial_tokens]
-        dial_tokens[-1] = dial_tokens[-1][:-2] + self.dialog_ending
-        return role_ids, dial_tokens
-        
-
-class Collate_Function:
-    """This function handles batch collate.
-    """
-    def __init__(self, tokenizer):
-        self.tokenizer = tokenizer
-        self.EOS = self.tokenizer.encoder["[EOS]"]
-
-    def __call__(self, unpacked_data):
-        return unpacked_data
-
-
-# In[4]:
-
-
 def top_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):
     """ Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering
         Args:
@@ -114,70 +76,33 @@ def top_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):
     
     return logits
 
-tokenizer = torch.load(tokenizer_dir)
-
-class GPT2SmallConfig:
-    vocab_size = 50257 + len(tokenizer.__special_tokens__)
-    n_special = len(tokenizer.__special_tokens__)
-    n_positions = 1024
-    n_ctx = 1024
-    n_embd = 768
-    n_class = 2
-    n_layer = 12
-    n_head = 12
-    resid_pdrop = 0.1
-    embd_pdrop = 0.1
-    attn_pdrop = 0.1
-    layer_norm_epsilon = 1e-5
-    initializer_range = 0.02
-    gradient_checkpointing = False
-    
-class GPT2MediumConfig:
-    vocab_size = 50257 + len(tokenizer.__special_tokens__)
-    n_special = len(tokenizer.__special_tokens__)
-    n_positions = 1024
-    n_ctx = 1024
-    n_embd = 1024
-    n_class = 2
-    n_layer = 24
-    n_head = 16
-    resid_pdrop = 0.1
-    embd_pdrop = 0.1
-    attn_pdrop = 0.1
-    layer_norm_epsilon = 1e-5
-    initializer_range = 0.02
-    gradient_checkpointing = True
-
+class ModelClassifierConfig:
+    model_dir = cfg.model_clf_dir
+    device1 = torch.device(cfg.model_clf_device1)
+    device2 = torch.device(cfg.model_clf_device2)
 
 class PersuasiveBot:
-    def __init__(self):
-        self.tokenizer = torch.load(tokenizer_dir)
-        # In[10]:
-
-        # model_A = GPT2SimpleLM(GPT2SmallConfig)
-        # model_B = GPT2SimpleLM(GPT2SmallConfig)
-        # model_A_states, model_B_states = torch.load("CheckpointMedium/model_state_epoch_3.th")
-
-        # define the model
-        self.model_A = GPT2SimpleLM(GPT2MediumConfig)
-        self.model_B = GPT2SimpleLM(GPT2MediumConfig)
-        
-        # load the model
-        model_A_states, model_B_states = torch.load(model_dir)
-        self.model_A.load_state_dict(model_A_states)
-        self.model_B.load_state_dict(model_B_states)
-        
-        self.device = torch.device("cuda:2")
-        self.model_A = self.model_A.to(self.device)
-        self.model_B = self.model_B.to(self.device)
-
-        self.model_A.eval()
-        self.model_B.eval()
+    def __init__(self, model_A=None, model_B=None, tokenizer=None, device1=None, device2=None):
+        if tokenizer is None:
+            self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")#torch.load(tokenizer_dir)
+        else:
+            self.tokenizer = tokenizer
+        self.max_sequence_len = cfg.max_sequence_len
+        if device1 is None:
+            self.device1 = torch.device("cuda:0")
+            self.device2 = torch.device("cuda:0")
+        else:
+            self.device1 = device1
+            self.device2 = device2
+        self.load_models(model_A, model_B)
 
-        self.eos = self.tokenizer.encode("\n\n\n")
+        # tokenizer weird behavior 
+        # sep = tokenizer.encode("\n\n\n")
+        self.eos = [628, 198]
         self.temperature = 0.7
         
         # Memory
+        # self.need_reload = True
         self.past = None
         self.b_hidden_states = None
         self.human_demonstrations = []
@@ -194,53 +119,159 @@ class PersuasiveBot:
                      'failed_candidates':[],
                      'global_profiles': [],
                      'responses': []}
+        self.reload()
 
-        
-    def chat(self, input_text, sid):
-        sid = 0
-        
-        past_is_None = (self.past is None)
-        
+    def load_models(self, model_A, model_B):
+        # define the model
+
+        self.model_clf = build_model_classifier(ModelClassifierConfig.model_dir, 
+                                                ModelClassifierConfig.device1, 
+                                                ModelClassifierConfig.device2)
+
+        if model_A is None and model_B is None:
+            if cfg.model_size == "small":
+                self.model_A = GPT2LMHeadModel_modified.from_pretrained("gpt2", output_hidden_states=True)
+                self.model_B = GPT2LMHeadModel_modified.from_pretrained("gpt2", output_hidden_states=True)
+            elif cfg.model_size == "medium":
+                self.model_A = GPT2LMHeadModel_modified.from_pretrained("gpt2-medium", output_hidden_states=True)
+                self.model_B = GPT2LMHeadModel_modified.from_pretrained("gpt2-medium", output_hidden_states=True)
+
+            # load the model
+            if cfg.model_size == "small":
+                model_A_states, model_B_states = torch.load(cfg.new_small_model_dir)
+            elif cfg.model_size == "medium":
+                if cfg.use_old_model:
+                    model_A_states, model_B_states = torch.load(cfg.old_medium_model_dir)
+                    model_A_states['transformer.wte.weight'] = model_A_states['transformer.wte.weight'][:50257,:]
+                    model_A_states['lm_head.weight'] = model_A_states['lm_head.decoder.weight'][:50257,:]
+                    model_B_states['transformer.wte.weight'] = model_B_states['transformer.wte.weight'][:50257,:]
+                    model_B_states['lm_head.weight'] = model_B_states['lm_head.decoder.weight'][:50257,:]
+                else:
+                    model_A_states, model_B_states = torch.load(cfg.new_medium_model_dir)
+
+            if cfg.use_old_model:
+                strict = False
+            else:
+                strict = True
+            self.model_A.load_state_dict(model_A_states, strict=strict)
+            self.model_B.load_state_dict(model_B_states, strict=strict)
+
+            self.model_A = self.model_A.to(self.device1)
+            self.model_B = self.model_B.to(self.device2)
+            self.model_A.device = self.device1
+            self.model_B.device = self.device2
+
+            self.model_A.eval()
+            self.model_B.eval()
+
+        else:
+            assert model_A is not None and model_B is not None
+            print("loaded predefined models!!!!!!\n\n\n")
+            self.model_A = model_A
+            self.model_B = model_B
+
+    def chat(self, input_text=None, mode=cfg.interactive_mode):
+        # sid = 0        
         # pdb.set_trace()
         if self.past is None:
-            sys_sent = self.sys_respond_and_update(past_is_None=past_is_None)
+            sys_sent, [sents_success, sents_failed], have_enough_candidates = self.sys_respond_and_update(mode=mode)
             turn_responses = ["sys: "+sys_sent]
         else:
             # user-side
-            if input_text == "quit":
-                self.past = None
-                return "ARDM MEMORY RESTARTS!"
-            
-            input_texts = sent_tokenize(input_text)
-            input_texts_labels = [None]*len(input_texts)
-            self.global_profile.update(sents=input_texts, sent_labels=input_texts_labels, who=self.domain.USR) #self.usr_profile.update(input_text, self.last_sys_labels)
-            self.context = input_text
-            self.turn_i += 1
-            user = self.tokenizer.encode("B:" + input_text)
-            prev_input = user + self.eos
-            prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device)
+            if mode != cfg.self_play_mode:
+                assert input_text is not None
+                if input_text == "quit":
+                    # del self.past
+                    self.past = None
+                    # self.reload()
+                    return
+                
+                input_texts = sent_tokenize(input_text)
+                # input_texts_labels = [None]*len(input_texts)
+                input_texts_labels, _ = self.global_profile.regex_label(self.model_clf,
+                                                                        input_texts, 
+                                                                        which_task="B")
+                self.model_clf.set_past(sent=input_text, 
+                                        which_task="B")
+                
+                self.global_profile.update(sents=input_texts, sent_labels=input_texts_labels, who=self.domain.USR) #self.usr_profile.update(input_text, self.last_sys_labels)
+                self.last_sent = input_text
+                self.turn_i += 1
+                user = self.tokenizer.encode("B:" + input_text)
+                prev_input = user + self.eos
+                prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device2)
+
+                if self.past is not None and self.model_B.device != self.past[0].device:
+                    past = [p.to(self.model_B.device) for p in self.past]
+                    self.past = past
+
+                _, self.past, self.b_hidden_states = self.model_B(prev_input, past=self.past)
+
 
-            _, self.past, self.b_hidden_states = self.model_B(prev_input, past=self.past)
 
+            elif mode == cfg.self_play_mode:
+                input_text = self.generate_user_utt_self_play()
+
+                if "bye" in input_text.lower() or "have a great day" in input_text.lower() \
+                    or "have a great night" in input_text.lower() \
+                    or self.turn_i >= 10:
+                    print(f"user: {input_text}\n$$$$$$$$")
+                    self.past = None
+                    return
+                    # return "ARDM MEMORY RESTARTS!"
+
+                print(f"user: {input_text}\n$$$$$$$$")
             # system-side
-            sys_sent = self.sys_respond_and_update(past_is_None=past_is_None)
+            sys_sent, [sents_success, sents_failed], have_enough_candidates = self.sys_respond_and_update(mode=mode)
             turn_responses = ["usr: "+input_text,
                               "sys: "+sys_sent]
         
         self.logs['global_profiles'].append(self.global_profile.get_profiles())
         self.logs['responses'].append(turn_responses)
-        return sys_sent
+        return sys_sent, [sents_success, sents_failed], have_enough_candidates
+
+    def generate_user_utt_self_play(self):
+        input_text, self.past, self.b_hidden_states = self.sample_one_sent(past=self.past, model=self.model_B, prefix="B:")
+        # 
+        # input("pause")
+
+        input_texts = sent_tokenize(input_text)
+        input_texts_labels, _ = self.global_profile.regex_label(self.model_clf,
+                                                                input_texts, 
+                                                                which_task="B")
+        self.model_clf.set_past(sent=input_text, 
+                                which_task="B")
+        self.global_profile.update(sents=input_texts, sent_labels=input_texts_labels, who=self.domain.USR) #self.usr_profile.update(input_text, self.last_sys_labels)
+        self.last_sent = input_text
+        self.turn_i += 1
+
+        # finish tail
+        if self.past is not None and self.model_B.device != self.past[0].device:
+            past = [p.to(self.model_B.device) for p in self.past]
+            self.past = past
 
-    def sample_one_sent(self, past):
-        prev_input = self.tokenizer.encode("A:")
-        prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device)
+        prev_input = torch.LongTensor(self.eos).unsqueeze(0).to(self.device2)
+        _, self.past, self.b_hidden_states = self.model_B(prev_input, past=self.past)
 
+        return input_text
+
+    def sample_one_sent(self, past, model, prefix="A:"):
+        prev_input = self.tokenizer.encode(prefix)
+        if prefix == "A:":
+           prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device1)
+        else:
+           prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device2)
+
+        if past is not None and model.device != past[0].device:
+            past = [p.to(model.device) for p in past]
         """Sampling based method"""
         sent = []
-        pdb.set_trace()
+        # pdb.set_trace()
         with torch.no_grad():
-            for i in range(200):
-                logits, past, hidden_states = self.model_A(prev_input, past=past)
+            import pdb
+            # pdb.set_trace()
+            for i in range(self.max_sequence_len):
+                logits, past, hidden_states = model(prev_input, past=past)
                 logits = logits[:, -1, :] / self.temperature
                 logits = top_filtering(logits, top_k=500, top_p=0.9)
                 # prev_input = logits.argmax(-1).unsqueeze(1)
@@ -252,15 +283,82 @@ class PersuasiveBot:
 
                 if prev_word == 628:
                     break
-                elif prev_word == self.tokenizer.encoder["[EOS]"]:
-                    past = None
-                    return "ARDM MEMORY RESTARTS!", past
-                    break
+                # elif prev_word == self.tokenizer.encoder["[EOS]"]:
+                #     past = None
+                #     return "ARDM MEMORY RESTARTS!", past
+                #     break
                 else:
                     sent.append(prev_word)
         return self.tokenizer.decode(sent), past, hidden_states
 
+    def sample_one_sent_test(self, past, model, sent, prefix="A:"):
+        prev_input = self.tokenizer.encode(prefix+sent)
+        prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device1)
+        
+        """Sampling based method"""
+        sent = []
+        # pdb.set_trace()
+        with torch.no_grad():
+            logits, past, hidden_states = model(prev_input, past=past)
+            # for i in range(200):
+            #     logits, past, hidden_states = model(prev_input, past=past)
+            #     logits = logits[:, -1, :] / self.temperature
+            #     logits = top_filtering(logits, top_k=500, top_p=0.9)
+            #     # prev_input = logits.argmax(-1).unsqueeze(1)
+            #     probs = F.softmax(logits, -1)
+            
+            #     prev_input = torch.multinomial(probs, num_samples=1)
+            #     prev_word = prev_input.item()
+                
+
+            #     if prev_word == 628:
+            #         break
+            #     # elif prev_word == self.tokenizer.encoder["[EOS]"]:
+            #     #     past = None
+            #     #     return "ARDM MEMORY RESTARTS!", past
+            #     #     break
+            #     else:
+            #         sent.append(prev_word)
+        return past, hidden_states
+
+    def sample_one_sent_test_2(self, past, model, sent, prefix="A:"):
+        prev_input = self.tokenizer.encode(prefix)
+        prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device1)
+        
+        encoded_sent = self.tokenizer.encode(sent)
+        """Sampling based method"""
+        sent = []
+        # pdb.set_trace()
+        with torch.no_grad():
+            logits, past, hidden_states = model(prev_input, past=past)
+            for tok in encoded_sent:
+                prev_input = torch.LongTensor(tok).unsqueeze(0).to(self.device1)
+                logits, past, hidden_states = model(prev_input, past=past)
+                # torch.cuda.empty_cache()
+            # for i in range(200):
+            #     logits, past, hidden_states = model(prev_input, past=past)
+            #     logits = logits[:, -1, :] / self.temperature
+            #     logits = top_filtering(logits, top_k=500, top_p=0.9)
+            #     # prev_input = logits.argmax(-1).unsqueeze(1)
+            #     probs = F.softmax(logits, -1)
+            
+            #     prev_input = torch.multinomial(probs, num_samples=1)
+            #     prev_word = prev_input.item()
+                
+
+            #     if prev_word == 628:
+            #         break
+            #     # elif prev_word == self.tokenizer.encoder["[EOS]"]:
+            #     #     past = None
+            #     #     return "ARDM MEMORY RESTARTS!", past
+            #     #     break
+            #     else:
+            #         sent.append(prev_word)
+        return past, hidden_states
+
     def reload(self):
+        # torch.cuda.empty_cache()
+        self.model_clf.reload()
         self.past = None
         self.b_hidden_states = None
         self.human_demonstrations = []
@@ -271,7 +369,7 @@ class PersuasiveBot:
         self.last_sys_labels = None
 
         # initialize params
-        self.context = '<Start>'
+        self.last_sent = '<Start>'
         self.turn_i = 0
         self.cnt = 0
         
@@ -280,37 +378,111 @@ class PersuasiveBot:
                      'global_profiles': [],
                      'responses': []}
 
-        print("reloaded")
+        print("reloaded\n\n")
 
-    def print_candidates(self, candidates, edited_candidates, sent_act_candidates, scores=None):
+    def print_candidates(self, candidates, edited_candidates, sent_act_candidates, scores=None, failed_candidates=None):
         log_this_turn = []
-        print("=== candidates, len={} ===".format(len(candidates)))
+        sents_success, sents_failed = [], []
+        if cfg.print_candidates:
+            print("=== candidates, len={} ===".format(len(candidates)))
         log_this_turn.append("=== candidates, len={} ===".format(len(candidates)))
         
-        for c, edited_c, act, s in zip(candidates, edited_candidates, sent_act_candidates, scores):
-            c = " ".join(c)
-            edited_c = " ".join(edited_c)
-            if c != edited_c:
-                print("--------- different from edited candidates: act: {}, score: {}----------".format(act, s))
-                print(c)
-                print(edited_c)
-                log_this_turn.append("--------- different from edited candidates: act: {}, score: {}----------".format(act, s))
-                log_this_turn.append(c)
-                log_this_turn.append(edited_c)
-            else:
-                print("----------------- act: {}, score : {}---------------------------".format(act, s))
-                print(edited_c)
-                log_this_turn.append("----------------- act: {}, score : {}---------------------------".format(act, s))
-                log_this_turn.append(edited_c)
-        print("==================")
-        log_this_turn.append("==================")
-        self.logs['candidates'].append(log_this_turn)
+        if type(scores[0]) is not bool:
+            for c, edited_c, act, s in zip(candidates, edited_candidates, sent_act_candidates, scores):
+                c = " ".join(c)
+                edited_c = " ".join(edited_c)
+                if c != edited_c:
+                    if cfg.print_candidates:
+                        print("--------- different from edited candidates: act: {}, score: {}----------".format(act, s))
+                        print(c)
+                        print(edited_c)
+                    sents_success.append(edited_c)
+                    log_this_turn.append("--------- different from edited candidates: act: {}, score: {}----------".format(act, s))
+                    log_this_turn.append(c)
+                    log_this_turn.append(edited_c)
+                else:
+                    if cfg.print_candidates:
+                        print("----------------- act: {}, score : {}---------------------------".format(act, s))
+                        print(edited_c)
+                    log_this_turn.append("----------------- act: {}, score : {}---------------------------".format(act, s))
+                    log_this_turn.append(edited_c)
+                    sents_success.append(edited_c)
+            if cfg.print_candidates:
+                print("==================")
+            log_this_turn.append("==================")
+            self.logs['candidates'].append(log_this_turn)
+
+            if failed_candidates:
+                i = 0
+                for sent, act, reason, past, hidden_states in failed_candidates:
+                    sent = " ".join(sent)
+                    sents_failed.append(sent)
+                    if cfg.print_candidates:
+                        print("----------------- failed candidates: reason: {}  ---------------------------".format(reason))
+                        print("{}".format(sent))
+                    i += 1
+        else:
+            scores, failed_scores = scores[:len(edited_candidates)], scores[len(edited_candidates):]
+            for c, edited_c, act, s in zip(candidates, edited_candidates, sent_act_candidates, scores):
+                c = " ".join(c)
+                edited_c = " ".join(edited_c)
+                if c != edited_c:
+                    to_print = "--------- different from edited candidates: act: {}, score: {}----------".format(act, s)
+                    if s is True:
+                        to_print = "SELECTED " + to_print 
+                        sents_success.append(edited_c)
+                    else:
+                        sents_failed.append(edited_c)
+                    if cfg.print_candidates:
+                        print(to_print)
+                        print(c)
+                        print(edited_c)
+                    log_this_turn.append(to_print)
+                    log_this_turn.append(c)
+                    log_this_turn.append(edited_c)
+                    
+                else:
+                    to_print = "----------------- act: {}, score : {} ---------------------".format(act, s)
+                    if s is True:
+                        to_print = "SELECTED " + to_print 
+                        sents_success.append(edited_c)
+                    else:
+                        sents_failed.append(edited_c)
+                    if cfg.print_candidates:
+                        print(to_print)
+                        print(edited_c)
+                    log_this_turn.append(to_print)
+                    log_this_turn.append(edited_c)
+                    
+            if cfg.print_candidates:    
+                print("==================")
+            log_this_turn.append("==================")
+            self.logs['candidates'].append(log_this_turn)
+
+            if failed_candidates:
+                try:
+                    i = 0
+                    for sent, act, reason, past, hidden_states in failed_candidates:
+                        sent = " ".join(sent)
+                        to_print = "----------------- failed candidates: reason: {}  -------------------".format(reason)
+                        if failed_scores[i] is True:
+                            to_print = "SELECTED " + to_print
+                        if cfg.print_candidates:
+                            print(to_print)
+                            print("{}".format(sent))
+                        i += 1
+                        sents_failed.append(sent)
+                except:
+                    import pdb
+                    pdb.set_trace()
+
+        return sents_success, sents_failed
 
     def print_all_generated_candidates(self, candidates, edited_candidates, sent_act_candidates,
                                              failed_candidates):
-        # log_this_turn = []
+        log_this_turn = []
         print("=== candidates, len={} ===".format(len(candidates)))
-        # log_this_turn.append("=== candidates, len={} ===".format(len(candidates)))
+        log_this_turn.append("=== candidates, len={} ===".format(len(candidates)))
         
         i = 0
         for c, edited_c, act in zip(candidates, edited_candidates, sent_act_candidates):
@@ -319,15 +491,16 @@ class PersuasiveBot:
             if c != edited_c:
                 print("--------- different from edited candidates: act: {} ----------".format(act))
                 print("{}). {}".format(i, c))
+                print("edit to -->")
                 print(edited_c)
-                # log_this_turn.append("--------- different from edited candidates: act: {} ----------".format(act))
-                # log_this_turn.append(c)
-                # log_this_turn.append(edited_c)
+                log_this_turn.append("--------- different from edited candidates: act: {} ----------".format(act))
+                log_this_turn.append(c)
+                log_this_turn.append(edited_c)
             else:
                 print("----------------- act: {} ---------------------------".format(act))
                 print("{}). {}".format(i, edited_c))
-                # log_this_turn.append("----------------- act: {} ---------------------------".format(act))
-                # log_this_turn.append(edited_c)
+                log_this_turn.append("----------------- act: {} ---------------------------".format(act))
+                log_this_turn.append(edited_c)
             i += 1
         print("==================")
 
@@ -336,8 +509,8 @@ class PersuasiveBot:
             print("{}). {}".format(i, sent))
             i += 1
 
-        # log_this_turn.append("==================")
-        # self.logs['candidates'].append(log_this_turn)
+        log_this_turn.append("==================")
+        self.logs['candidates'].append(log_this_turn)
 
     def select_candidates(self, sent_candidates, sent_candidate_conflict_scores, sent_act_candidates, past_candidates):
         
@@ -348,7 +521,7 @@ class PersuasiveBot:
                 one_minus_score = 1 - np.array(sent_candidate_conflict_scores)
                 normalized_score = one_minus_score/(one_minus_score.sum())
 
-                if cfg.candidate_select_strategy == cfg.REPETITION_RATIO:
+                if cfg.candidate_select_strategy in [cfg.REPETITION_RATIO, cfg.IMITATION_LEARNING_SELECTION]:
                     # if cfg.debug:
                     #     print("~~~~~~~~in select_candidates~~~~~~~~~")
                     #     print("normalized_score: {}".format(normlized_score))
@@ -358,6 +531,8 @@ class PersuasiveBot:
                                         replace=False, p=normalized_score)[0], normalized_score
                 elif cfg.candidate_select_strategy == cfg.FIRST_OF_CANDIDATES:
                     return 0, normalized_score
+                else:
+                    raise ValueError(f"{cfg.candidate_select_strategy} is not supported in select_index()")
 
         selected_by_func = select_index()
         sents, sent_acts, past, scores = self.apply_human_rule(sent_candidates, sent_act_candidates, past_candidates,
@@ -370,7 +545,7 @@ class PersuasiveBot:
         rule_results = [self.human_rule.enforce(sents, sent_acts, past)
                         for sents, sent_acts, past in zip(sent_candidates, sent_act_candidates, past_candidates)]
         
-        if cfg.debug:
+        if cfg.verbose:
             pass
             print("rule_result:\n")
             print(rule_results)
@@ -395,7 +570,11 @@ class PersuasiveBot:
 
                 # encode the enforced rule sentences
                 prev_input = self.tokenizer.encode(" "+" ".join(enforced_sents))
-                prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device)
+                prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device1)
+
+                if past is not None and self.model_A.device != past[0].device:
+                    past = [p.to(self.model_A.device) for p in past]
+
                 _, past, hidden_state = self.model_A(prev_input, past=past)
 
                 # concatenate the enforced sentences from rule
@@ -436,7 +615,7 @@ class PersuasiveBot:
         failed_rule_results = [self.human_rule.enforce(sents, sent_acts, past) \
                               for sents, sent_acts, fail_reason, past, hidden_states in failed_candidates]
         
-        if cfg.debug:
+        if cfg.verbose:
             pass
             print("rule_result:\n")
             print(rule_results)
@@ -447,7 +626,11 @@ class PersuasiveBot:
                 enforced_sents, enforced_acts = rule_result
                 # encode the enforced rule sentences
                 prev_input = self.tokenizer.encode(" "+" ".join(enforced_sents))
-                prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device)
+                prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device1)
+                
+                if past is not None and self.model_A.device != past[0].device:
+                    past = [p.to(self.model_A.device) for p in past]
+
                 _, past, hidden_state = self.model_A(prev_input, past=past)
 
                 # concatenate the enforced sentences from rule
@@ -465,7 +648,11 @@ class PersuasiveBot:
                 enforced_sents, enforced_acts = rule_result
                 # encode the enforced rule sentences
                 prev_input = self.tokenizer.encode(" "+" ".join(enforced_sents))
-                prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device)
+                prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device1)
+
+                if past is not None and self.model_A.device != past[0].device:
+                    past = [p.to(self.model_A.device) for p in past]
+
                 _, past, hidden_state = self.model_A(prev_input, past=past)
 
                 # concatenate the enforced sentences from rule
@@ -487,8 +674,9 @@ class PersuasiveBot:
         except:
             return usr_input        
 
-    def sys_respond_and_update(self, past_is_None):
+    def sys_respond_and_update(self, mode):
         # start A's utterance
+        past_is_None = (self.past is None)
         sent_candidates, edited_sent_candidates, sent_candidate_conflict_scores, sent_act_candidates, past_candidates, hidden_states_candidates = [], [], [], [], [], []
         have_enough_candidates = False
         num_rounds = 0
@@ -496,11 +684,18 @@ class PersuasiveBot:
         while not have_enough_candidates and num_rounds < int(cfg.MAX_NUM_CANDIDATES/cfg.NUM_CANDIDATES):
             num_rounds += 1
             for _ in range(cfg.NUM_CANDIDATES):
-                sent, past, hidden_states = self.sample_one_sent(past=self.past)
+                sent, past, hidden_states = self.sample_one_sent(past=self.past, model=self.model_A)                
+                # past_test, hidden_states_test = self.sample_one_sent_test(past=self.past, model=self.model_A, sent=sent)
+                # # past_test_2, hidden_states_test_2 = self.sample_one_sent_test_2(past=self.past, model=self.model_A, sent=sent)
+                # if self.turn_i in [0, 5] and _ in [0, 9]:
+                #     pdb.set_trace()
                 sents = sent_tokenize(sent)
                 
                 # use regex to re-label
-                sent_acts = self.global_profile.regex_label(sents, self.context, self.turn_i)
+                sent_acts, _ = self.global_profile.regex_label(self.model_clf,
+                                                               sents, 
+                                                               which_task="A")
+                                                               
 
                 # check conflict condition
                 # conflict_status_with_sys, conflict_amount_with_sys, edited_sents, edited_sent_acts = self.sys_profile.check_conflict(sents, sent_acts)
@@ -523,21 +718,24 @@ class PersuasiveBot:
                     failed_candidates.append([sents, sent_acts, fail_reason, past, hidden_states])
 
             have_enough_candidates = (len(past_candidates) > 0)
-        if not have_enough_candidates:
+        if (not have_enough_candidates):
             # as long as it's not a contradiction, randomly pick one 
-                if cfg.debug:
-                    print("no enough candidates! randomly generate the next one!")
-                sent, past, hidden_states = self.sample_one_sent(past=self.past)
-                sents = sent_tokenize(sent)
+            if cfg.debug:
+                print("no enough candidates! randomly generate the next one!")
+            sent, past, hidden_states = self.sample_one_sent(past=self.past, model=self.model_A)
+            sents = sent_tokenize(sent)
+
+            sent_acts, _ = self.global_profile.regex_label(self.model_clf,
+                                                           sents, 
+                                                           which_task="A")
+            sent_candidates.append(sents)
+            edited_sent_candidates.append(sents)
+            sent_candidate_conflict_scores.append(0)
+            sent_act_candidates.append(sent_acts)
+            past_candidates.append(past)
+            hidden_states_candidates.append(hidden_states)
+
 
-                sent_acts = self.global_profile.regex_label(sents, self.context, self.turn_i)
-                sent_candidates.append(sents)
-                edited_sent_candidates.append(sents)
-                sent_candidate_conflict_scores.append(0)
-                sent_act_candidates.append(sent_acts)
-                past_candidates.append(past)
-                hidden_states_candidates.append(hidden_states)
-        
         self.logs['failed_candidates'].append(failed_candidates)
 
         # check consistency and pick one candidate
@@ -550,50 +748,152 @@ class PersuasiveBot:
                                                 failed_candidates)
             human_selected_ids = self.human_select_candidates()
             if type(human_selected_ids) is list:
-                selected_i = human_selected_ids[0]
-                if True:
-                    sents, sent_acts, past = sent_candidates[selected_i], sent_act_candidates[selected_i], past_candidates[selected_i]
+                selected_i = np.random.choice(human_selected_ids)
+                if False:
+                    if selected_i < len(sent_candidates):
+                        sents, sent_acts, past = sent_candidates[selected_i], sent_act_candidates[selected_i], past_candidates[selected_i]
+                    else:
+                        failed_candidate = failed_candidates[selected_i-len(sent_candidates)]
+                        sents, sent_acts, past = failed_candidate[0], failed_candidate[1], failed_candidate[3]
                 else:
-                    sents, sent_acts, past = edited_sent_candidates[selected_i], sent_act_candidates[selected_i], past_candidates[selected_i]
-                self.log_human_demonstration(sent_candidates, edited_sent_candidates, sent_act_candidates, past_candidates, hidden_states_candidates,
+                    if selected_i < len(edited_sent_candidates):
+                        sents, sent_acts, past = edited_sent_candidates[selected_i], sent_act_candidates[selected_i], past_candidates[selected_i]
+                    else:
+                        failed_candidate = failed_candidates[selected_i-len(edited_sent_candidates)]
+                        sents, sent_acts, past = failed_candidate[0], failed_candidate[1], failed_candidate[3]
+                sents_success, sents_failed = self.log_human_demonstration(sent_candidates, edited_sent_candidates, sent_act_candidates, past_candidates, 
+                                            hidden_states_candidates,
                                             failed_candidates, human_selected_ids)
             else:
                 prev_input = self.tokenizer.encode("A:" + human_selected_ids)
-                prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device)
+                prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(self.device1)
+
+                if self.past is not None and self.model_A.device != self.past[0].device:
+                    past = [p.to(self.model_A.device) for p in self.past]
+                    self.past = past
 
                 _, past, hidden_state = self.model_A(prev_input, past=self.past)
 
                 sent = human_selected_ids
                 sents = sent_tokenize(sent)
-                sent_acts = self.global_profile.regex_label(sents, self.context, self.turn_i)
+                sent_acts, _ = self.global_profile.regex_label(self.model_clf,
+                                                               sents, 
+                                                               which_task="A")
                 
-                self.log_human_demonstration(sent_candidates, edited_sent_candidates, sent_act_candidates, past_candidates, hidden_states_candidates,
+                sents_success, sents_failed = self.log_human_demonstration(sent_candidates, edited_sent_candidates, sent_act_candidates, past_candidates, hidden_states_candidates,
                                             failed_candidates, human_selected_ids=[], human_added_candidates=[[sents, sent_acts, past, hidden_state]])
                 
             
+        elif cfg.candidate_select_strategy == cfg.IMITATION_LEARNING_SELECTION:
+            sent_candidates, edited_sent_candidates, sent_act_candidates, past_candidates,\
+            failed_candidates = self.edit_with_human_rule(sent_candidates, edited_sent_candidates, sent_act_candidates, past_candidates,\
+                             failed_candidates)            
+            model_selected_id_for_success_candidates, model_selected_id_for_failed_candidates = self.il_model_select_candidates(sent_candidates, 
+                                                              edited_sent_candidates, 
+                                                              sent_act_candidates, 
+                                                              past_candidates, 
+                                                              hidden_states_candidates,
+                                                              failed_candidates)
+            selected_T_F = [True if i in model_selected_id_for_success_candidates + model_selected_id_for_failed_candidates else False \
+                            for i in range(len(edited_sent_candidates)+len(failed_candidates))]
+            if len(model_selected_id_for_success_candidates) > 0:
+                selected_i = np.random.choice(model_selected_id_for_success_candidates)
+                sents, sent_acts, past = edited_sent_candidates[selected_i], sent_act_candidates[selected_i], \
+                                                 past_candidates[selected_i]
+
+            # elif len(model_selected_id_for_failed_candidates) > 0:
+            #     selected_i = np.random.choice(model_selected_id_for_failed_candidates)
+            #     failed_candidate = failed_candidates[selected_i-len(edited_sent_candidates)]
+            #     sents, sent_acts, past = failed_candidate[0], failed_candidate[1], failed_candidate[3]
+
+            else:
+                sents, sent_acts, past, scores = self.select_candidates(edited_sent_candidates, sent_candidate_conflict_scores, sent_act_candidates, past_candidates)
+            # if not cfg.self_play_mode:
+            sents_success, sents_failed = self.print_candidates(sent_candidates, edited_sent_candidates, sent_act_candidates, selected_T_F, failed_candidates)
         else:
             sents, sent_acts, past, scores = self.select_candidates(edited_sent_candidates, sent_candidate_conflict_scores, sent_act_candidates, past_candidates)
-            self.print_candidates(sent_candidates, edited_sent_candidates, sent_act_candidates, scores)
+            # if not cfg.self_play_mode:
+            sents_success, sents_failed = self.print_candidates(sent_candidates, edited_sent_candidates, sent_act_candidates, scores, failed_candidates)
         
         # check conflict within the sents
         sents, sent_acts = self.check_conflict_within_selected_sents(sents, sent_acts)
         
-        self.global_profile.update(sents=sents, sent_labels=sent_acts, who=self.domain.SYS) #self.last_sys_labels = self.sys_profile.update(sys_texts=sents, sys_labels=sent_acts)
 
-        # join sentences! finally!
-        sent = " ".join(sents)
-        if sent == "ARDM MEMORY RESTARTS!":
-            self.past = None
-            return "ARDM MEMORY RESTARTS!"
-        self.past = past
+        # update
+        if mode != cfg.supervised_mode:
+            self.global_profile.update(sents=sents, sent_labels=sent_acts, who=self.domain.SYS) #self.last_sys_labels = self.sys_profile.update(sys_texts=sents, sys_labels=sent_acts)
 
-        # print("A:" + tokenizer.decode(sent))
-        # finish tail
-        prev_input = torch.LongTensor(self.eos).unsqueeze(0).to(self.device)
-        pdb.set_trace()
-        _, self.past, hidden_states = self.model_A(prev_input, past=self.past)
+            # join sentences! finally!
+            sent = " ".join(sents)
+            # if sent == "ARDM MEMORY RESTARTS!":
+            #     self.past = None
+            #     return "ARDM MEMORY RESTARTS!"
+            self.past = past
+            # set past for model_clf
+            self.model_clf.set_past(sent=sent, 
+                                    which_task="A")
+
+            # print("A:" + tokenizer.decode(sent))
+            # finish tail
+            prev_input = torch.LongTensor(self.eos).unsqueeze(0).to(self.device1)
+            # pdb.set_trace()
+            if self.past is not None and self.model_A.device != self.past[0].device:
+                past = [p.to(self.model_A.device) for p in self.past]
+                self.past = past
+
+            _, self.past, hidden_states = self.model_A(prev_input, past=self.past)
+
+
+        else:
+            sent = " ".join(sents)
+
+        return sent, [sents_success, sents_failed], have_enough_candidates
+
+    def il_model_select_candidates(self, sent_candidates, 
+                                   edited_sent_candidates, 
+                                   sent_act_candidates, 
+                                   past_candidates,
+                                   hidden_states_candidates, 
+                                   failed_candidates):
+        left_id_for_success_candidates = []
+        left_id_for_failed_candidates = []
+        i = 0
+        for sent, edited_sent, sent_act, past, hidden_state in zip(sent_candidates, edited_sent_candidates,
+                                                                   sent_act_candidates, past_candidates, hidden_states_candidates):
+            # pdb.set_trace()
+            if type(edited_sent) is not list:
+                edited_sent = [edited_sent]
+            clf_predicted_acts, clf_past = self.model_clf.predict(separate_sents=edited_sent, 
+                                                                  which_task="TF")
+            
+            # if hidden_state[-1] is not None:
+            #     if hidden_state[-1].device != self.model_clf.device:
+            #         last_hidden_state = hidden_state[-1].to(self.model_clf.device)
+            #     else:
+            #         last_hidden_state = hidden_state[-1]
+            # outputs = self.model_clf(hidden_states=last_hidden_state)
+            # _, predicted_label = torch.max(outputs, 1)
+
+            if clf_predicted_acts == 1:#type(predicted_label.item()) is int and predicted_label.item() == 1:
+                left_id_for_success_candidates.append(i)
+            i += 1
         
-        return sent
+        for sent, sent_act, reason, past, hidden_state in failed_candidates:
+            # if hidden_state[-1] is not None and hidden_state[-1].device != self.model_clf.device:
+            #     last_hidden_state = hidden_state[-1].to(self.model_clf.device)
+            # outputs = self.model_clf(hidden_states=last_hidden_state)
+            # _, predicted_label = torch.max(outputs, 1)
+
+            # if type(predicted_label.item()) is int and predicted_label.item() == 1:
+            #     left_id_for_failed_candidates.append(i)
+            if type(sent) is not list:
+                sent = [sent]
+            clf_predicted_acts, clf_past = self.model_clf.predict(separate_sents=sent, 
+                                                                  which_task="TF")
+            if clf_predicted_acts == 1:
+                left_id_for_failed_candidates.append(i)
+
+        return left_id_for_success_candidates, left_id_for_failed_candidates
 
     def check_conflict_within_selected_sents(self, sents, sent_acts):
         statuses = [True]*len(sents)
@@ -615,29 +915,37 @@ class PersuasiveBot:
         return edited_sents, edited_sent_acts
 
     def save(self):
-        print("\n")
+        # print("\n")
         logging.debug("\n")
         logging.debug("*************************** new dialog ****************************************")
         for turn_i in range(len(self.logs['responses'])):
             for k in ['responses', 'candidates', 'failed_candidates', 'global_profiles']:
-                print("{}\n".format(k))
-                print("-"*100)
+                # print("{}\n".format(k))
+                # print("-"*100)
                 logging.debug("{}\n".format(k))
                 if k in ['responses', 'candidates', 'failed_candidates']:
-                    for a in self.logs[k][turn_i]:
-                        print(a)
-                        logging.debug(a)
+                    try:
+                        if k == "failed_candidates":
+                            for a in self.logs[k][turn_i]:
+                                sents, sent_acts, fail_reason, past, hidden_states = a
+                                logging.debug("------------failed_candidates: {} - reason: {}-----------".format(sent_acts, fail_reason))
+                                logging.debug(sents)
+                        for a in self.logs[k][turn_i]:
+                            # print(a)
+                            logging.debug(a)
+                    except:
+                        pdb.set_trace()
                 else:
                     for world in self.logs[k][turn_i]:                        
                         for profile in self.logs[k][turn_i][world]:
-                            print("*******{}: {}*******".format(world, profile))
+                            # print("*******{}: {}*******".format(world, profile))
                             logging.debug("*******{}: {}*******".format(world, profile))
                             for key, value in self.logs[k][turn_i][world][profile].items():
-                                print("{}: {}".format(key, value))
+                                # print("{}: {}".format(key, value))
                                 logging.debug("{}: {}".format(key, value))
-                            print("")
+                            # print("")
                             logging.debug("")
-            print("\n")
+            # print("\n")
             logging.debug("\n")
         
         logging.debug("*************************** dialog end ****************************************")
@@ -646,10 +954,11 @@ class PersuasiveBot:
                                       failed_candidates, human_selected_ids,
                                       human_added_candidates=None):
         # records = {}
+        sents_success, sents_failed = [], []
         csv_records = []
         context = deepcopy(self.global_profile.history)
         context_act = deepcopy(self.global_profile.history_label)
-        hidden_states_before_generation = deepcopy(self.b_hidden_states) if self.b_hidden_states is None else self.b_hidden_states.clone().detach()#deepcopy(self.b_hidden_states)
+        hidden_states_before_generation = deepcopy(self.b_hidden_states) if self.b_hidden_states is None else self.b_hidden_states[-1].clone().detach()#deepcopy(self.b_hidden_states)
         past_before_generation = deepcopy(self.past) if self.past is None else self.past[-1].clone().detach()#[p.clone().detach() for p in self.past]#deepcopy(self.past)
         sys_world_sys_profile = deepcopy(self.global_profile.sys_world.sys_profile)
         sys_world_usr_profile = deepcopy(self.global_profile.sys_world.usr_profile)
@@ -669,8 +978,9 @@ class PersuasiveBot:
         i = 0
         for sent, edited_sent, sent_act, past, hidden_state in zip(sent_candidates, edited_sent_candidates,
                                                                    sent_act_candidates, past_candidates, hidden_states_candidates):
+            # pdb.set_trace()
             record = {
-                      'hidden_states_after_generation': hidden_state.clone().detach(),#deepcopy(hidden_state),
+                      'hidden_states_after_generation': hidden_state[-1].clone().detach(),#deepcopy(hidden_state),
                       'past_after_generation': past[-1].clone().detach(),#[p.clone().detach() for p in past],
                       'sent': sent,
                       'edited_sent': edited_sent,
@@ -684,10 +994,14 @@ class PersuasiveBot:
             records['individual_features'].append(record)
             csv_records.append(csv_record)
             i += 1
+            if i in human_selected_ids:
+                sents_success.append(edited_sent)
+            else:
+                sents_failed.append(edited_sent)
 
         for sent, sent_act, reason, past, hidden_state in failed_candidates:
             record = {
-                      'hidden_states_after_generation': hidden_state.clone().detach(),#deepcopy(hidden_state),
+                      'hidden_states_after_generation': hidden_state[-1].clone().detach(),#deepcopy(hidden_state),
                       'past_after_generation': past[-1].clone().detach(),#[p.clone().detach() for p in past],,
                       'sent': sent,
                       'edited_sent': sent,
@@ -701,11 +1015,16 @@ class PersuasiveBot:
             records['individual_features'].append(record)
             csv_records.append(csv_record)
             i += 1
+            if i in human_selected_ids:
+                sents_success.append(sent)
+            else:
+                sents_failed.append(sent)
+
 
         if human_added_candidates is not None:
             for sent, sent_act, past, hidden_state in human_added_candidates:
                 record = {
-                        'hidden_states_after_generation': hidden_state.clone().detach(),#deepcopy(hidden_state),
+                        'hidden_states_after_generation': hidden_state[-1].clone().detach(),#deepcopy(hidden_state),
                         'past_after_generation': past[-1].clone().detach(),#[p.clone().detach() for p in past],,
                         'sent': sent,
                         'edited_sent': sent,
@@ -715,10 +1034,11 @@ class PersuasiveBot:
                         'pick_or_not': True, 
                         }
                 csv_record = [context, context_act, sys_world_sys_profile, sys_world_usr_profile, usr_world_sys_profile, usr_world_usr_profile, 
-                            sent, sent, sent_act, reason, True, i in human_selected_ids]
+                            sent, sent, sent_act, "human_added_sentence", False, True]
                 records['individual_features'].append(record)
                 csv_records.append(csv_record)
                 i += 1
+                sents_success.append(sent)
 
 
         if not os.path.exists(cfg.demonstration_csv):
@@ -745,35 +1065,52 @@ class PersuasiveBot:
             with open(cfg.demonstration_pkl, "wb") as fh:
                 pkl.dump([records], fh)
         
+        return sents_success, sents_failed
 
 if __name__ == "__main__":
     logging.basicConfig(filename=cfg.log_file,level=logging.DEBUG)
     
 
     bot = PersuasiveBot()
-    bot.reload()
+    pdb.set_trace()
+    # bot.reload()
     user_text = ""
-    signal.signal(signal.SIGINT, signal.default_int_handler)
+    # signal.signal(signal.SIGINT, signal.default_int_handler)
 
-    while True:
+    MAX_DIALOGS = 2
+    dial_i = 0
+    while dial_i < MAX_DIALOGS:
         try:
             if bot.past is not None:
-                user_text  = input("user: ")
+                if cfg.mode != cfg.self_play_mode:
+                    user_text  = input("user: ")
+                else:
+                    user_text = None
             else:
+                dial_i += 1
+                print("\n\n\n")
                 print("INIT MEMORY!")
                 bot.save()
                 bot.reload()
             
-            response = bot.chat(user_text, 0)
+
+            result = bot.chat(input_text=user_text, mode=cfg.mode)
+            if result is not None:
+                response, [sents_success, sents_failed], have_enough_candidates = result
             if cfg.candidate_select_strategy != cfg.HUMAN_SELECTION:
-                bot.global_profile.print()
+                if cfg.verbose:
+                    bot.global_profile.print()
             
-            if response == "ARDM MEMORY RESTARTS!":
-                print("ARDM MEMORY RESTARTS!")
-            else:
+            # if response == "ARDM MEMORY RESTARTS!":
+            #     print("ARDM MEMORY RESTARTS!")
+            # else:
+            if result is not None:
+                print("Turn {}".format(bot.turn_i))
                 print("system: ", response)
-            print("$$$$$$$$$$$$$$$$$$$$$\n\n\n\n\n\n")
+            print("$$$$$$$$$$$$$$$$$$$$$")
 
         except KeyboardInterrupt:
             bot.save()
-            sys.exit()
\ No newline at end of file
+            sys.exit()
+
+        
\ No newline at end of file
diff --git a/Train.py b/Train.py
index 7e7f431..a9d5251 100644
--- a/Train.py
+++ b/Train.py
@@ -19,9 +19,10 @@ import time
 from torch.utils.tensorboard import SummaryWriter
 from apex import amp
 from allennlp.training.checkpointer import Checkpointer
-from gpt_model import GPT2SimpleLM
-from pytorch_pretrained_bert import GPT2Tokenizer, OpenAIAdam, GPT2Model
+# from gpt_model import GPT2SimpleLM
+# from pytorch_pretrained_bert import GPT2Tokenizer, OpenAIAdam, GPT2Model
 # from torchfly.criterions import SequenceFocalLoss, SequenceCrossEntropyLoss
+from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW, WarmupLinearSchedule
 from torchfly.modules.losses import SequenceFocalLoss, SequenceCrossEntropyLoss
 from UnlikelihoodLoss import SequenceUnlikelihoodLoss
 # In[2]:
@@ -37,13 +38,45 @@ os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,4,5,6,7"
 # In[3]:
 
 
+# class PersuadeDataset(Dataset):
+#     def __init__(self, data, tokenizer):
+#         self.data = data
+#         self.tokenizer = tokenizer
+#         self.tokenizer.max_len = 1500
+#         self.turn_ending = tokenizer.encode("\n\n\n")
+#         self.dialog_ending = [tokenizer.encoder["[EOS]"]]
+        
+#     def __len__(self):
+#         return len(self.data)
+    
+#     def __getitem__(self, index):
+#         dial_tokens = [tokenizer.encode(item) + self.turn_ending for item in self.data[index]]
+#         role_ids = [0 if item[0] == 32 else 1 for item in dial_tokens]
+#         dial_tokens[-1] = dial_tokens[-1][:-2] + self.dialog_ending
+#         return role_ids, dial_tokens
+        
+
+# class Collate_Function:
+#     """This function handles batch collate.
+#     """
+#     def __init__(self, tokenizer):
+#         self.tokenizer = tokenizer
+#         self.EOS = self.tokenizer.encoder["[EOS]"]
+        
+#     def __call__(self, unpacked_data):
+#         return unpacked_data
+
+
+# # In[4]:
+
 class PersuadeDataset(Dataset):
     def __init__(self, data, tokenizer):
         self.data = data
         self.tokenizer = tokenizer
         self.tokenizer.max_len = 1500
-        self.turn_ending = tokenizer.encode("\n\n\n")
-        self.dialog_ending = [tokenizer.encoder["[EOS]"]]
+        # tokenizer weird behavior
+        self.turn_ending = [628, 198]
+        # tokenizer.encode("\n\n\n")
         
     def __len__(self):
         return len(self.data)
@@ -51,64 +84,53 @@ class PersuadeDataset(Dataset):
     def __getitem__(self, index):
         dial_tokens = [tokenizer.encode(item) + self.turn_ending for item in self.data[index]]
         role_ids = [0 if item[0] == 32 else 1 for item in dial_tokens]
-        dial_tokens[-1] = dial_tokens[-1][:-2] + self.dialog_ending
         return role_ids, dial_tokens
         
-
-class Collate_Function:
-    """This function handles batch collate.
-    """
-    def __init__(self, tokenizer):
-        self.tokenizer = tokenizer
-        self.EOS = self.tokenizer.encoder["[EOS]"]
-        
-    def __call__(self, unpacked_data):
+    def collate(self, unpacked_data):
         return unpacked_data
 
-
-# In[4]:
-
-
-tokenizer = torch.load("DataProcess/special3_gpt2_tokenizer.pkl")
-
-class GPT2SmallConfig:
-    vocab_size = 50257 + len(tokenizer.__special_tokens__)
-    n_special = len(tokenizer.__special_tokens__)
-    n_positions = 1024
-    n_ctx = 1024
-    n_embd = 768
-    n_layer = 12
-    n_head = 12
-    resid_pdrop = 0.1
-    embd_pdrop = 0.1
-    attn_pdrop = 0.1
-    layer_norm_epsilon = 1e-5
-    initializer_range = 0.02
-    gradient_checkpointing = False
+tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
+
+# class GPT2SmallConfig:
+#     vocab_size = 50257 + len(tokenizer.__special_tokens__)
+#     n_special = len(tokenizer.__special_tokens__)
+#     n_positions = 1024
+#     n_ctx = 1024
+#     n_embd = 768
+#     n_layer = 12
+#     n_head = 12
+#     resid_pdrop = 0.1
+#     embd_pdrop = 0.1
+#     attn_pdrop = 0.1
+#     layer_norm_epsilon = 1e-5
+#     initializer_range = 0.02
+#     gradient_checkpointing = False
     
-class GPT2MediumConfig:
-    vocab_size = 50257 + len(tokenizer.__special_tokens__)
-    n_special = len(tokenizer.__special_tokens__)
-    n_positions = 1024
-    n_ctx = 1024
-    n_embd = 1024
-    n_layer = 24
-    n_head = 16
-    resid_pdrop = 0.1
-    embd_pdrop = 0.1
-    attn_pdrop = 0.1
-    layer_norm_epsilon = 1e-5
-    initializer_range = 0.02
-    gradient_checkpointing = True
+# class GPT2MediumConfig:
+#     vocab_size = 50257 + len(tokenizer.__special_tokens__)
+#     n_special = len(tokenizer.__special_tokens__)
+#     n_positions = 1024
+#     n_ctx = 1024
+#     n_embd = 1024
+#     n_layer = 24
+#     n_head = 16
+#     resid_pdrop = 0.1
+#     embd_pdrop = 0.1
+#     attn_pdrop = 0.1
+#     layer_norm_epsilon = 1e-5
+#     initializer_range = 0.02
+#     gradient_checkpointing = True
 
 
 # In[5]:
 
 
-model_A = GPT2SimpleLM(GPT2SmallConfig)
-model_B = GPT2SimpleLM(GPT2SmallConfig)
-model_A_states, model_B_states = torch.load("Checkpoint/best.th", map_location="cuda:5")#torch.load("CheckpointMedium/model_state_epoch_3.th")
-print("load success")
+
+model_A = GPT2LMHeadModel.from_pretrained("gpt2")
+model_B = GPT2LMHeadModel.from_pretrained("gpt2")
+# model_A_states, model_B_states = torch.load("Checkpoint/best.th", map_location="cuda:5")#torch.load("CheckpointMedium/model_state_epoch_3.th")
+# print("load success")
+
 # model_A.load_state_dict(torch.load("/home/qingyang/Desktop/GPT2_Modification/special3_gpt2_small.pth"))
 # model_B.load_state_dict(torch.load("/home/qingyang/Desktop/GPT2_Modification/special3_gpt2_small.pth"))
 
@@ -130,16 +152,15 @@ train_dataset = PersuadeDataset(train_data, tokenizer)
 val_dataset = PersuadeDataset(val_data, tokenizer)
 
 batch_size = 1
-collate_func = Collate_Function(tokenizer)
 
 train_dataloader = DataLoader(dataset=train_dataset, 
                               shuffle=True, 
                               batch_size=batch_size, 
-                              collate_fn=collate_func)
+                              collate_fn=train_dataset.collate)
 val_dataloader = DataLoader(dataset=val_dataset, 
                             shuffle=False, 
                             batch_size=batch_size, 
-                            collate_fn=collate_func)
+                            collate_fn=train_dataset.collate)
 
 
 # ## Define the model
diff --git a/Trainer.py b/Trainer.py
new file mode 100644
index 0000000..594f69c
--- /dev/null
+++ b/Trainer.py
@@ -0,0 +1,231 @@
+from PersuasionInteract import PersuasiveBot
+
+
+
+[A:"hello ",   
+B:"hello how are you", 
+A:"I am good"]
+
+past = None
+
+    for turn_num, dial_turn_inputs in enumerate(dial_inputs):
+        if role_ids[turn_num] == 0:
+            # breakpoint()
+            # [can1: "hi", 'hello', 'how are you'] + [] ---> buffer
+            [can1: 'I am good', 1
+            can2: 'I am fine', 2] + 
+            state: [A:"hello ",    past = model_A("hello", past=None)
+                                                B:"hello how are you", 
+                                    logits, _ = model_A(can1, past=past)
+                                    criterion(logits, reward) #if ppo, old_prob & new_prob
+                                                              #if not ppo, reward*log(sent)
+                                                ]
+            logits, _ = model_A(dial_turn_inputs, past=past)
+            model_A(dial_turn_inputs, past=)
+            all_logits.append(logits)
+            candidates = model_A.generate_candidates()
+            target = dial_turn_inputs[1:]
+            turn_loss = torch.mean([SequenceLoss(logits, c) for c in candidates+[target]])
+        else:
+            # breakpoint()
+            logits, past = model_B(dial_turn_inputs, past=past)
+            all_logits.append(logits)
+
+
+
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.nn.utils.rnn import pad_sequence
+import time
+import logging
+import numba
+
+#from transformers import WarmupLinearSchedule
+from apex.optimizers import FusedLAMB, FusedAdam
+# from torchfly.transformers import UnifiedTokenizer, GPT2SimpleLM
+# from torchfly.criterions import SequenceCrossEntropyLoss
+# from torchfly.decode import top_filtering
+
+# logging is important
+logging.basicConfig()
+logger = logging.getLogger(__name__)
+logger.setLevel(logging.INFO)
+
+
+# define model
+model = GPT2SimpleLM(GPT2SmallConfig)
+# load model
+model.load_state_dict(torch.load("../supervised_warmup/Checkpoint/model_state_epoch_1.th"))
+
+tokenizer = UnifiedTokenizer()
+device = torch.device("cuda")
+model = model.to(device)
+
+class Trainer:
+    """Reinforcement Learning Trainer
+    """
+    def __init__(self):
+        self.sample_size = 32
+        self.maxlen = 128
+        self.replay_buffer = ReplayBuffer(self.maxlen)
+        self.actor = actor
+        
+        self.reward_func = CustomRewardFunc(tokenizer)
+        
+    def collect_generations(self, total_size=64, normalize_reward=True):
+        logger.info("Collecting Samples")
+        # define storage
+        all_generated_sequences = []
+        all_old_logprobs = []
+        all_rewards = []
+        
+        while total_size > 0:
+            real_sample_size = min(self.sample_size, total_size)
+            
+            # sample sequences
+            generated_sequences, old_logprobs = self.actor.sample_generations(real_sample_size)
+            # calculate reward
+            rewards = self.reward_func(generated_sequences)
+            # add tuple into replay buffer
+            all_generated_sequences.extend(generated_sequences)
+            all_old_logprobs.extend(old_logprobs)
+            all_rewards.extend(rewards)
+                    
+            # decrease
+            total_size -= self.sample_size
+            logger.info(f"{total_size} samples remaining")
+        
+        all_rewards = np.array(all_rewards)
+
+        if normalize_reward:
+            final_rewards = (all_rewards - all_rewards.mean()) / (all_rewards.std() + 1e-5)
+        else:
+            final_rewards = all_rewards
+        
+        self.replay_buffer.add(zip(all_generated_sequences, all_old_logprobs, final_rewards))
+        
+        # logging
+        logger.info("Collecting Samples finished!")
+        
+        return all_rewards
+
+
+def calculate_old_logprobs(buffer_sequences):
+
+    buffer_old_logprobs = []
+    indices = np.arange(len(buffer_sequences))
+
+    with torch.no_grad():
+        for i in range(PpoParams.batchsize // PpoParams.mini_batchsize):
+            batch_indices = indices[i * PpoParams.mini_batchsize : (i + 1) * PpoParams.mini_batchsize]
+            batch_sequences = [buffer_sequences[j] for j in batch_indices]
+
+            # make batch
+            batch_sequences = make_batch_sequences(batch_sequences, padding_value=pad_token)
+            mask = batch_sequences.ne(pad_token).float()
+
+            # to device
+            batch_sequences = batch_sequences.to(device)
+            mask = mask.to(device)
+
+            logits, past = model(batch_sequences)
+
+             # prepare the loss func inputs
+            logits = logits[:, :-1].contiguous()
+            target = batch_sequences[:, 1:].contiguous()
+            mask = mask[:, 1:].contiguous()
+
+            sequences_logprobs = - criterion(logits, target, mask)
+            old_logprobs = sequences_logprobs.sum(1)
+
+            # store
+            buffer_old_logprobs.extend(old_logprobs.tolist())
+            
+    return buffer_old_logprobs
+
+
+def train_one_iter(batch, update_count, mode, fp16=False):
+    role_ids, dialog_tokens = batch
+    dial_inputs = [torch.LongTensor(item).unsqueeze(0).to(device) for item in dialog_tokens]
+    
+    past = None
+    all_logits = []
+    
+    for turn_num, dial_turn_inputs in enumerate(dial_inputs):
+        if role_ids[turn_num] == 0:
+            # breakpoint()
+            logits, past = model_A(dial_turn_inputs, past=past)
+            all_logits.append(logits)
+        else:
+            # breakpoint()
+            logits, past = model_B(dial_turn_inputs, past=past)
+            all_logits.append(logits)
+
+    all_logits = torch.cat(all_logits, dim=1) # torch.Size([1, 505, 50260]), 505 = sum of tokens from 21 sentences
+    
+    
+    
+    # target
+    all_logits = all_logits[:, :-1].contiguous() # torch.Size([1, 504, 50260])
+    target = torch.cat(dial_inputs, dim=1)[:, 1:].contiguous()# torch.Size([1, 504])
+    target_mask = torch.ones_like(target).float()# torch.Size([1, 504])
+    
+    if False:
+        loss = criterion(all_logits, target, target_mask, label_smoothing=0.02, reduce=True) # torch.Size([])
+    else:
+        loss = unlikelihood_criterion(all_logits, target)
+    loss /= num_gradients_accumulation
+    
+    if fp16:
+        with amp.scale_loss(loss, optimizer) as scaled_loss:
+            scaled_loss.backward()
+    else:
+        loss.backward()
+        
+    record_loss = loss.item() * num_gradients_accumulation
+    # print("record_loss: {}".format(record_loss))
+    perplexity = np.exp(record_loss)
+    
+    return record_loss, perplexity
+
+
+
+
+    bot = PersuasiveBot()
+    bot.reload()
+    user_text = ""
+    # signal.signal(signal.SIGINT, signal.default_int_handler)
+
+    MAX_DIALOGS = 2
+    i = 0
+    while i < MAX_DIALOGS:
+        try:
+            if bot.past is not None:
+                if not cfg.self_play_mode:
+                    user_text  = input("user: ")
+                else:
+                    user_text = None
+            else:
+                i += 1
+                print("\n\n\n")
+                print("INIT MEMORY!")
+                bot.save()
+                bot.reload()
+            
+            response, [sents_success, sents_failed] = bot.chat(0, user_text)
+            if cfg.candidate_select_strategy != cfg.HUMAN_SELECTION:
+                if cfg.verbose:
+                    bot.global_profile.print()
+            
+            if response == "ARDM MEMORY RESTARTS!":
+                print("ARDM MEMORY RESTARTS!")
+            else:
+                print("Turn {}".format(bot.turn_i))
+                print("system: ", response)
+            print("$$$$$$$$$$$$$$$$$$$$$")
+
+        except KeyboardInterrupt:
+            bot.save()
+            sys.exit()
diff --git a/act_classifier.py b/act_classifier.py
new file mode 100644
index 0000000..0d21d6b
--- /dev/null
+++ b/act_classifier.py
@@ -0,0 +1,428 @@
+
+from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel
+import pandas as pd
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.utils.data import DataLoader, Dataset
+from torch.nn.utils.rnn import pad_sequence
+import numpy as np
+import regex as re
+import random
+import itertools
+import tqdm
+import time
+import os
+
+import config as cfg
+from torch.nn import Identity
+from torch.utils.tensorboard import SummaryWriter
+from apex import amp
+from allennlp.training.checkpointer import Checkpointer
+# from gpt_model import GPT2SimpleLM
+from pytorch_pretrained_bert import OpenAIAdam
+# from torchfly.criterions import SequenceFocalLoss, SequenceCrossEntropyLoss
+from transformers import GPT2Model, GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, AdamW#, WarmupLinearSchedule
+# from transformers.modeling_gpt2 import SequenceSummary
+from torchfly.modules.losses import SequenceFocalLoss, SequenceCrossEntropyLoss
+import pickle as pkl
+import pdb
+# In[2]:
+def save_pkl(obj, dir):
+    with open(dir, "wb") as fh:
+        pkl.dump(obj, fh)
+
+def load_pkl(dir):
+    with open(dir, "rb") as fh:
+        obj = pkl.load(fh)
+    return obj
+
+import tqdm
+def split_train_val():
+    import pickle as pkl
+    df = pd.read_excel("training/data/300_dialog.xlsx")
+    from sklearn import preprocessing
+    le = preprocessing.LabelEncoder()
+    labels = le.fit_transform(df[df['B4']==0]['er_label_1'])
+    df['er_label_1_num'] = None#labels
+    df['er_label_1_num'].loc[df['B4']==0] = labels
+    save_pkl(le, "training/data/labelencoder.pkl")
+    def extract_data(df_dialogs):
+        data = []
+        for i in tqdm.trange(len(df_dialogs)):
+            line = df.iloc[i]
+            if line["B4"] == 0:
+                text = line["Unit"].strip()
+                data.append([text, line['er_label_1_num']])
+            # else:
+            #     text = "B:" + line["Unit"].strip()
+            #     data[line["B2"]].append([text, line['ee_label_1']])                
+        return data
+    all_data = extract_data(df)
+    import random
+    random.seed(123)
+    random.shuffle(all_data)
+    train_data = all_data[:int(len(all_data)*0.8)]
+    val_data = all_data[int(len(all_data)*0.8):]
+    save_pkl(train_data, "training/data/train_data.pkl")
+    save_pkl(val_data, "training/data/val_data.pkl")
+
+class SequenceSummary(nn.Module):
+    r""" Compute a single vector summary of a sequence hidden states according to various possibilities:
+        Args of the config class:
+            summary_type:
+                - 'last' => [default] take the last token hidden state (like XLNet)
+                - 'first' => take the first token hidden state (like Bert)
+                - 'mean' => take the mean of all tokens hidden states
+                - 'cls_index' => supply a Tensor of classification token position (GPT/GPT-2)
+                - 'attn' => Not implemented now, use multi-head attention
+            summary_use_proj: Add a projection after the vector extraction
+            summary_proj_to_labels: If True, the projection outputs to config.num_labels classes (otherwise to hidden_size). Default: False.
+            summary_activation: 'tanh' => add a tanh activation to the output, Other => no activation. Default
+            summary_first_dropout: Add a dropout before the projection and activation
+            summary_last_dropout: Add a dropout after the projection and activation
+    """
+    def __init__(self, config):
+        super().__init__()
+        self.summary_type = config.summary_type if hasattr(config, "summary_type") else "last"
+        if self.summary_type == "attn":
+            # We should use a standard multi-head attention module with absolute positional embedding for that.
+            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276
+            # We can probably just use the multi-head attention module of PyTorch >=1.1.0
+            raise NotImplementedError
+        self.summary = Identity()
+        if hasattr(config, "summary_use_proj") and config.summary_use_proj:
+            if hasattr(config, "summary_proj_to_labels") and config.summary_proj_to_labels and config.num_labels > 0:
+                print(f"num_class: {config.num_labels}")
+                num_classes = config.num_labels
+            else:
+                print(f"num_class here: {config.hidden_size}")
+                num_classes = config.hidden_size
+            self.summary = nn.Linear(config.hidden_size, num_classes)
+        self.activation = nn.Tanh()
+        if hasattr(config, "summary_activation") and config.summary_activation == "tanh":
+            self.activation = nn.Tanh()
+        self.first_dropout = Identity()
+        if hasattr(config, "summary_first_dropout") and config.summary_first_dropout > 0:
+            self.first_dropout = nn.Dropout(config.summary_first_dropout)
+        self.last_dropout = Identity()
+        if hasattr(config, "summary_last_dropout") and config.summary_last_dropout > 0:
+            self.last_dropout = nn.Dropout(config.summary_last_dropout)
+
+    def forward(self, hidden_states, cls_index=None):
+        """ hidden_states: float Tensor in shape [bsz, ..., seq_len, hidden_size], the hidden-states of the last layer.
+            cls_index: [optional] position of the classification token if summary_type == 'cls_index',
+                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.
+                if summary_type == 'cls_index' and cls_index is None:
+                    we take the last token of the sequence as classification token
+        """
+        if self.summary_type == "last":
+            output = hidden_states[:, -1]
+        elif self.summary_type == "first":
+            output = hidden_states[:, 0]
+        elif self.summary_type == "mean":
+            output = hidden_states.mean(dim=1)
+        elif self.summary_type == "cls_index":
+            if cls_index is None:
+                cls_index = torch.full_like(hidden_states[..., :1, :], hidden_states.shape[-2] - 1, dtype=torch.long)
+            else:
+                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)
+                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))
+            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states
+            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)
+        elif self.summary_type == "attn":
+            raise NotImplementedError
+        pdb.set_trace()
+        output = self.first_dropout(output)
+        output = self.summary(output)
+        output = self.activation(output)
+        output = self.last_dropout(output)
+        return output
+
+
+
+torch.backends.cudnn.benchmark = True
+torch.manual_seed(123)
+np.random.seed(123)
+import os
+os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,4,5,6,7"
+
+class PersuadeDataset(Dataset):
+    def __init__(self, data, tokenizer):
+        self.data = data
+        self.tokenizer = tokenizer
+        self.tokenizer.max_len = 1500
+        # tokenizer weird behavior
+        self.turn_ending = tokenizer.cls_token_id#[628, 198]
+        # tokenizer.encode("\n\n\n")        
+    def __len__(self):
+        return len(self.data)    
+    def __getitem__(self, index):
+        dial_tokens = tokenizer.encode(self.data[index][0]) + [self.turn_ending]
+        cls_token_location = dial_tokens.index(self.tokenizer.cls_token_id)
+        dial_act = self.data[index][1]
+        return dial_tokens, cls_token_location, dial_act        
+    def collate(self, unpacked_data):
+        return unpacked_data
+
+tokenizer = GPT2Tokenizer.from_pretrained("gpt2-medium")
+tokenizer.add_special_tokens({'cls_token': '[CLS]'})
+
+
+class GPT2DoubleHeadsModel_modified(GPT2DoubleHeadsModel):
+    def __init__(self, config):
+        super().__init__(config)
+        # config.num_labels = 1
+        config.num_labels = le.classes_.shape[0]
+        self.transformer = GPT2Model(config)
+        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
+        self.multiple_choice_head = SequenceSummary(config)
+        self.init_weights()
+
+config = GPT2Config()
+config = config.from_pretrained('gpt2-medium')
+le = load_pkl("training/data/labelencoder.pkl")
+config.num_labels = le.classes_.shape[0]
+config.summary_first_dropout = 0.2
+model_A = GPT2DoubleHeadsModel_modified(config)
+model_A.resize_token_embeddings(len(tokenizer)) 
+# model_B = GPT2DoubleHeadsModel.from_pretrained('gpt2')
+
+device = torch.device("cuda:5")
+torch.cuda.set_device(device)
+model_A = model_A.to(device)
+
+# model_A_states, model_B_states = torch.load(cfg.old_medium_model_dir)
+# model_A_states['transformer.wte.weight'] = torch.cat([model_A_states['transformer.wte.weight'][:50257,:],
+#                                                      torch.randn([1, 1024]).to(device)], dim=0)
+model_A_states = torch.load("Checkpoint_act_clf/best_acc_-inf_f1_0.609478966219062.pth")
+model_A.load_state_dict(model_A_states, strict=False)
+
+# model_B_states['transformer.wte.weight'] = model_B_states['transformer.wte.weight'][:50257,:]
+
+
+# load training data
+train_data = load_pkl("training/data/train_data.pkl")
+val_data = load_pkl("training/data/val_data.pkl")
+
+train_dataset = PersuadeDataset(train_data, tokenizer)
+val_dataset = PersuadeDataset(val_data, tokenizer)
+
+batch_size = 1
+
+train_dataloader = DataLoader(dataset=train_dataset, 
+                              shuffle=True, 
+                              batch_size=batch_size, 
+                              collate_fn=train_dataset.collate)
+val_dataloader = DataLoader(dataset=val_dataset, 
+                            shuffle=False, 
+                            batch_size=batch_size, 
+                            collate_fn=train_dataset.collate)
+
+
+
+
+
+# define the losses
+import torch.nn as nn
+criterion = nn.CrossEntropyLoss()
+# eval_criterion = SequenceCrossEntropyLoss()
+
+# In[9]:
+
+
+def train_one_iter(batch, update_count, fp16=False):
+    dial_tokens, cls_token_location, dial_act = batch
+    input_ids = torch.LongTensor(dial_tokens).unsqueeze(0).to(device)
+    # [torch.LongTensor(item).unsqueeze(0).to(device) for item in dialog_tokens]
+    mc_token_ids = torch.tensor(cls_token_location).unsqueeze(0).to(device)
+    mc_labels = torch.tensor(dial_act).unsqueeze(0).to(device)
+
+    outputs = model_A(input_ids, mc_token_ids=mc_token_ids)
+    lm_prediction_scores, mc_logits = outputs[:2]
+    loss = criterion(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1))
+
+    # past = None
+    # all_logits = []
+   
+    # for turn_num, (dial_turn_inputs, dialog_turn_act) in enumerate(zip(dial_inputs, dialog_acts)):
+    #     # if role_ids[turn_num] == 0:
+    #     #     # breakpoint()
+    #     #     logits, past = model_A(dial_turn_inputs, past=past)
+    #     #     all_logits.append(logits)
+    #     # else:
+    #     #     # breakpoint()
+    #     #     logits, past = model_B(dial_turn_inputs, past=past)
+    #     #     all_logits.append(logits)
+
+    # all_logits = torch.cat(all_logits, dim=1) # torch.Size([1, 505, 50260]), 505 = sum of tokens from 21 sentences
+    
+    
+    
+    # # target
+    # all_logits = all_logits[:, :-1].contiguous() # torch.Size([1, 504, 50260])
+    # target = torch.cat(dial_inputs, dim=1)[:, 1:].contiguous()# torch.Size([1, 504])
+    # target_mask = torch.ones_like(target).float()# torch.Size([1, 504])
+    
+    loss /= num_gradients_accumulation
+    
+    if fp16:
+        with amp.scale_loss(loss, optimizer) as scaled_loss:
+            scaled_loss.backward()
+    else:
+        loss.backward()
+        
+    record_loss = loss.item() * num_gradients_accumulation
+    # print("record_loss: {}".format(record_loss))
+    # perplexity = np.exp(record_loss)
+    
+    return record_loss#, perplexity
+
+from sklearn.metrics import f1_score
+from sklearn.metrics import confusion_matrix
+from utils import print_cm
+
+def validate(dataloader):
+    with torch.no_grad():
+        pbar = progress_bar(dataloader)
+
+        correct = 0
+        total = 0
+        x, y_true, y_pred = [], [], []
+
+        for batch in pbar:
+            
+            dial_tokens, cls_token_location, dial_act = batch[0]
+            x.append(tokenizer.decode(dial_tokens[:-1]))
+            input_ids = torch.LongTensor(dial_tokens).unsqueeze(0).to(device)
+            # [torch.LongTensor(item).unsqueeze(0).to(device) for item in dialog_tokens]
+            mc_token_ids = torch.tensor(cls_token_location).unsqueeze(0).to(device)
+            labels = torch.tensor(dial_act).unsqueeze(0).to(device)
+
+            outputs = model_A(input_ids, mc_token_ids=mc_token_ids)
+            lm_prediction_scores, mc_logits = outputs[:2]
+            y_true.extend([dial_act])
+            # pdb.set_trace()
+            _, predicted_labels = torch.max(mc_logits, 1)
+            y_pred.extend(predicted_labels.tolist())
+
+            total += labels.size(0)
+            correct += (predicted_labels == labels).sum().item()
+        f1 = f1_score(y_true, y_pred, average="weighted")
+        # pdb.set_trace()
+        pd.DataFrame(zip(x, le.inverse_transform(y_true).tolist(), le.inverse_transform(y_pred).tolist()),
+                     columns=['sent', 'y_true', 'y_pred']).to_csv("act_classifier_val_results.csv", index=None)
+        print(f"Epcoh {ep} Validation accuracy: {correct/total}, f1: {f1}")
+        print_cm(confusion_matrix(y_true, y_pred, labels=range(len(le.classes_))), labels=[l[:] for l in le.classes_.tolist()])
+        return correct/total, f1
+
+
+# ### Training
+
+# In[10]:
+
+
+checkpointer = Checkpointer(serialization_dir="Checkpoint_act_clf", 
+                            keep_serialized_model_every_num_seconds=3600*2, 
+                            num_serialized_models_to_keep=5)
+
+
+# In[11]:
+
+
+# optimizer
+num_epochs = 10
+num_gradients_accumulation = 1
+num_train_optimization_steps = num_train_optimization_steps = len(train_dataset) * num_epochs // batch_size // num_gradients_accumulation
+
+param_optimizer = list(model_A.named_parameters()) 
+no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
+optimizer_grouped_parameters = [
+    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
+    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
+    ]
+
+
+optimizer = OpenAIAdam(optimizer_grouped_parameters,
+                       lr=2e-5,
+                       warmup=0.1,
+                       max_grad_norm=1.0,
+                       weight_decay=0.01,
+                       t_total=num_train_optimization_steps)
+
+
+# In[12]:
+
+
+# support fp16
+# [model_A, model_B], optimizer = amp.initialize([model_A, model_B], optimizer, opt_level="O1")
+
+
+# In[13]:
+
+import tqdm 
+update_count = 0
+progress_bar = tqdm.tqdm
+start = time.time()
+best_acc = -float('Inf')
+best_f1 = -float('Inf')
+
+if False:
+    for ep in tqdm.tqdm(range(num_epochs)):
+
+        "Training"
+        pbar = progress_bar(train_dataloader)
+        model_A.train()
+        # model_B.train()
+        
+        for batch in pbar:
+            batch = batch[0]
+            # without relative position
+            # if sum([len(item) for item in batch[1]]) > 1024:
+            #     continue
+                
+            record_loss = train_one_iter(batch, update_count, fp16=False)
+            
+            update_count += 1
+
+            if update_count % num_gradients_accumulation == num_gradients_accumulation - 1:
+                # update for gradient accumulation
+                optimizer.step()
+                optimizer.zero_grad()
+                
+                # speed measure
+                end = time.time()
+                speed = batch_size * num_gradients_accumulation / (end - start)
+                start = end
+                
+                # show progress
+                pbar.set_postfix(loss=record_loss, speed=speed)
+
+        "Evaluation"
+        model_A.eval()
+        # model_B.eval()
+        val_acc, val_f1 = validate(val_dataloader)
+        print(f"val f1: {val_f1}, valid acc: {val_acc}")
+        is_best_so_far = val_f1 > best_f1
+        if is_best_so_far:
+            best_acc = val_acc
+        #     # torch.save(model_clf.state_dict(), f"Checkpoint_clf/best_acc_{best_acc}_f1_{val_f1}_with_past.pth")
+        if is_best_so_far:
+            best_f1 = val_f1
+            torch.save(model_A.state_dict(), f"Checkpoint_act_clf/best_acc_{best_acc}_f1_{best_f1}.pth")
+            # checkpointer.save_checkpoint(ep, model_A.state_dict(), {"None": None}, is_best_so_far)
+
+    print("best acc: {}, best f1: {}".format(best_acc, best_f1))
+
+else:
+    model_A.eval()
+    ep=-1
+    val_acc, val_f1 = validate(val_dataloader)
+
+
+
+# In[ ]:
+
+
+
+
diff --git a/config.py b/config.py
index 6a4201d..2809273 100644
--- a/config.py
+++ b/config.py
@@ -1,7 +1,25 @@
-class Config:
-    dialogAct_model_dir = "./classifier/best_model_state_er.pkl"
+model_clf_dir = "Checkpoint_act_clf/epoch7_multitask_TF_best_acc_0.7944444444444444_f1_0.7861271676300577_A_acc_0.687741935483871_f1_0.6602596916886914_B_acc_0.6437699680511182_f1_0.6186370327752058.pth"#"Checkpoint_act_clf/multitask_TF_best_acc_0.7777777777777778_f1_0.776536312849162_A_acc_0.6954838709677419_f1_0.6707423935799665_B_acc_0.6166134185303515_f1_0.5898033645875225.pth"
+model_clf_device1 = "cuda:0"
+model_clf_device2 = "cuda:0"
+
+rl_finetune = True
+
+self_play_mode = "self_play_mode"
+supervised_mode = "supervised_mode"
+interactive_mode = "interactive_mode"
+mode = interactive_mode
+
+max_sequence_len = 200
+model_size = "medium"
+use_old_model = True
+old_medium_model_dir = "/home/wyshi/persuasion/consistency/ARDM/persuasion/persuasion_medium_3.th"
+old_tokenizer_dir = "/home/wyshi/persuasion/consistency/ARDM/persuasion/special3_gpt2_tokenizer.pkl"
+new_medium_model_dir = "models/persuasion-gpt2-medium.pth"
+new_small_model_dir = "models/persuasion-gpt2-small.pth"
 
 debug = True
+verbose = False
+print_candidates = True
 repetition_threshold = 0.5
 similarity_threshold = 0.707
 
@@ -10,7 +28,8 @@ RANDOM_SELECT = "random_select"
 REPETITION_RATIO = "repetition_ratio"
 FIRST_OF_CANDIDATES = "first_of_candidates"
 HUMAN_SELECTION = "human_selection"
-candidate_select_strategy = FIRST_OF_CANDIDATES#HUMAN_SELECTION
+IMITATION_LEARNING_SELECTION = "imitation_learning_selection"
+candidate_select_strategy = IMITATION_LEARNING_SELECTION#HUMAN_SELECTION#FIRST_OF_CANDIDATES#HUMAN_SELECTION
 
 # num of candidates to select from
 NUM_CANDIDATES = 10
@@ -30,7 +49,13 @@ SYSTEM_CORRECTION = "system_correction"
 
 domain = "persuasion"
 
-log_file = 'example.log'
 
-demonstration_csv = "demonstration.csv"
-demonstration_pkl = "demonstration.pkl"
\ No newline at end of file
+
+if use_old_model:
+    log_file = 'logs/old_model/example.log'
+    demonstration_csv = "demonstration/old_model/demonstration.csv"
+    demonstration_pkl = "demonstration/old_model/demonstration.pkl"
+else:
+    log_file = 'logs/new_model/example.log'
+    demonstration_csv = "demonstration/new_model/demonstration.csv"
+    demonstration_pkl = "demonstration/new_model/demonstration.pkl"
diff --git a/dagger.py b/dagger.py
new file mode 100644
index 0000000..b346607
--- /dev/null
+++ b/dagger.py
@@ -0,0 +1,2 @@
+from PersuasionInteract import PersuasiveBot
+
diff --git a/gpt_model.py b/gpt_model.py
index 313359d..4a06630 100755
--- a/gpt_model.py
+++ b/gpt_model.py
@@ -8,6 +8,18 @@ import math
 from apex.normalization.fused_layer_norm import FusedLayerNorm as LayerNorm
 import pdb
 # pylint:disable=no-member
+try:
+    from torch.nn import Identity
+except ImportError:
+    # Older PyTorch compatibility
+    class Identity(nn.Module):
+        r"""A placeholder identity operator that is argument-insensitive.
+        """
+        def __init__(self, *args, **kwargs):
+            super(Identity, self).__init__()
+
+        def forward(self, input):
+            return input
 
 
 def gelu(x):
@@ -198,13 +210,21 @@ class GPT2MultipleChoiceHead(nn.Module):
         self.n_embd = config.n_embd
         # self.multiple_choice_token = multiple_choice_token
         # self.dropout = nn.Dropout2d(config.resid_pdrop)  # To reproduce the noise_shape parameter of TF implementation
-        self.linear = nn.Linear(config.n_embd, config.n_class)
+        self.first_dropout = nn.Dropout(config.summary_last_dropout)
+        self.linear1 = nn.Linear(config.n_embd, 500)
+        self.linear2 = nn.Linear(500, config.n_class)
+        self.activation = nn.Tanh()
+        self.device = config.device
+        # self.last_dropout = Identity()#
         # self.loss = nn.CrossEntropyLoss()
 
-        nn.init.normal_(self.linear.weight, std=0.02)
-        nn.init.normal_(self.linear.bias, 0)
+        nn.init.normal_(self.linear1.weight, std=0.02)
+        nn.init.normal_(self.linear1.bias, 0)
+
+        nn.init.normal_(self.linear2.weight, std=0.02)
+        nn.init.normal_(self.linear2.bias, 0)
 
-    def forward(self, hidden_states):
+    def forward(self, hidden_states, mc_labels=None):
         # Classification logits
         # hidden_state (bsz, num_choices, seq_length, hidden_size)
                         # num_layer * (, , num_heads, seq_length, hidden_size)
@@ -216,11 +236,24 @@ class GPT2MultipleChoiceHead(nn.Module):
         # multiple_choice_h = hidden_states.gather(2, mc_token_ids).squeeze(2)
         # (bsz, num_choices, hidden_size)
         # multiple_choice_h = self.dropout(hidden_states.transpose(1, 2)).transpose(1, 2)
-        multiple_choice_logits = self.linear(hidden_states[:, -1, :])
+        
+        output = hidden_states[:, -1, :]
+        output = self.first_dropout(output)
+        output = self.linear1(output)
+        output = self.activation(output)
+        output = self.linear2(output)
+        output = self.activation(output)
+        # output = self.last_dropout(output)
+
         # self.loss(multiple_choice_logits, target)
+        if mc_labels is not None:
+            loss_fct = nn.CrossEntropyLoss()
+            loss = loss_fct(output.view(-1, output.size(-1)), mc_labels.view(-1))
+            # output = (loss,) + output
+            return loss, output
         
         # (bsz, num_choices)
-        return multiple_choice_logits
+        return output
 
 
 
diff --git a/model_clf.py b/model_clf.py
new file mode 100644
index 0000000..dd8543e
--- /dev/null
+++ b/model_clf.py
@@ -0,0 +1,1029 @@
+import os
+os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"
+import argparse
+from nltk.tokenize import sent_tokenize
+
+import re
+import dialog_config
+#from AgentProfile.profiles_in_dev import GlobalProfile
+import torch.nn as nn
+from utils import print_cm
+import tqdm
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.utils.data import DataLoader, Dataset
+from torch.nn.utils.rnn import pad_sequence
+import numpy as np
+import regex as re
+import random
+import itertools
+import tqdm
+import time
+
+import warnings
+import signal
+import sys
+from sys import exit
+warnings.filterwarnings('ignore')
+import pdb
+import os
+import csv
+import pickle as pkl
+
+# from gpt_model import GPT2SimpleLM
+# from pytorch_pretrained_bert import GPT2Tokenizer
+from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config
+from GPTModel import GPT2LMHeadModel_modified
+from pytorch_pretrained_bert import OpenAIAdam
+import config as cfg
+from torch.nn import Identity
+from utils import is_repetition_with_context
+
+from KnowledgeBase.KB import HumanRule
+from KnowledgeBase import KB
+from KnowledgeBase.KB import Domain
+from nltk.tokenize import sent_tokenize
+import logging
+
+from copy import deepcopy
+import tqdm
+import pandas as pd
+
+def load_model(cfg, model_size, tokenizer, device1, device2):
+    if model_size == "small":
+        model_A = GPT2LMHeadModel_modified.from_pretrained("gpt2", output_hidden_states=True)
+        model_B = GPT2LMHeadModel_modified.from_pretrained("gpt2", output_hidden_states=True)
+    elif model_size == "medium":
+        model_A = GPT2LMHeadModel_modified.from_pretrained("gpt2-medium", output_hidden_states=True)
+        model_B = GPT2LMHeadModel_modified.from_pretrained("gpt2-medium", output_hidden_states=True)
+
+    model_A.resize_token_embeddings(len(tokenizer)) 
+    model_B.resize_token_embeddings(len(tokenizer)) 
+    
+    # load the model
+    if False:
+        if cfg.model_size == "small":
+            model_A_states, model_B_states = torch.load(cfg.new_small_model_dir)
+        elif cfg.model_size == "medium":
+            if cfg.use_old_model:
+                model_A_states, model_B_states = torch.load(cfg.old_medium_model_dir)
+                model_A_states['transformer.wte.weight'] = model_A_states['transformer.wte.weight'][:50257,:]
+                model_B_states['transformer.wte.weight'] = model_B_states['transformer.wte.weight'][:50257,:]
+            else:
+                model_A_states, model_B_states = torch.load(cfg.new_medium_model_dir)
+
+        model_A.load_state_dict(model_A_states, strict=False)
+        model_B.load_state_dict(model_B_states, strict=False)
+    else:
+        # initialize
+        pass
+
+    model_A = model_A.to(device1)
+    model_B = model_B.to(device2)
+    model_A.device = device1
+    model_B.device = device2
+
+    return model_A, model_B
+
+def load_pkl(dir):
+    with open(dir, "rb") as fh:
+        obj = pkl.load(fh)
+    return obj
+
+def save_pkl(obj, dir):
+    with open(dir, "wb") as fh:
+        pkl.dump(obj, fh)
+
+
+def split_train_val():
+    import pickle as pkl
+    df = pd.read_excel("training/data/300_dialog.xlsx")
+    df = df[(df['er_label_1']!='off-task')&(df['er_label_1']!='other')]
+    df = df[(df['ee_label_1']!='off-task')&(df['ee_label_1']!='other')]
+    from sklearn import preprocessing
+    le_A = preprocessing.LabelEncoder()
+    labels_A = le_A.fit_transform(df[df['B4']==0]['er_label_1'])
+    df['er_label_1_num'] = None#labels
+    df['er_label_1_num'].loc[df['B4']==0] = labels_A
+    save_pkl(le_A, "training/data/labelencoder_A.pkl")
+    le_B = preprocessing.LabelEncoder()
+    labels_B = le_B.fit_transform(df[df['B4']==1]['ee_label_1'])
+    df['ee_label_1_num'] = None#labels
+    df['ee_label_1_num'].loc[df['B4']==1] = labels_B
+    save_pkl(le_B, "training/data/labelencoder_B.pkl")
+    def extract_data(df_dialogs):
+        data = {}
+        prev_dialog_id = "init"
+        # prev_role_id = 1
+        for i in tqdm.trange(len(df_dialogs)):
+            line = df.iloc[i]
+            cur_dialog_id, cur_role_id = line['B2'], line['B4']
+            if cur_dialog_id not in data:
+                data[cur_dialog_id] = []                
+            if cur_dialog_id == prev_dialog_id:
+                # the same dialog
+                # import pdb
+                # pdb.set_trace()
+                if prev_role_id != cur_role_id:
+                    # different person talking
+                    if line["B4"] == 0:
+                        data[cur_dialog_id].append([cur_sents, cur_acts, cur_roles])
+                        text = "A:" + line["Unit"].strip()
+                        cur_sents = [text]
+                        cur_acts = [line['er_label_1_num']]
+                        cur_roles = [0]
+                    else:
+                        data[cur_dialog_id].append([cur_sents, cur_acts, cur_roles])
+                        text = "B:" + line["Unit"].strip()
+                        cur_sents = [text]
+                        cur_acts = [line['ee_label_1_num']]
+                        cur_roles = [1]
+                else:
+                    # the same person
+                    text = line["Unit"].strip()
+                    if line["B4"] == 0:
+                        cur_sents.append(text)
+                        cur_acts.append(line['er_label_1_num'])
+                        cur_roles.append(0)
+                    else:
+                        cur_sents.append(text)
+                        cur_acts.append(line['ee_label_1_num'])
+                        cur_roles.append(1)
+            else:
+                # a new dialog
+                if prev_dialog_id != "init":
+                    data[prev_dialog_id].append([cur_sents, cur_acts, cur_roles])
+                if line["B4"] == 0:
+                    text = "A:" + line["Unit"].strip()
+                    cur_sents = [text]
+                    cur_acts = [line['er_label_1_num']]
+                    cur_roles = [0]
+                else:
+                    text = "B:" + line["Unit"].strip()
+                    cur_sents = [text]
+                    cur_acts = [line['ee_label_1_num']]
+                    cur_roles = [1]
+            prev_dialog_id = cur_dialog_id
+            prev_role_id = cur_role_id
+        return data
+    all_data = extract_data(df)
+    import random
+    random.seed(123)
+    ids = list(range(len(all_data)))
+    random.shuffle(ids)
+    all_data = [list(all_data.values())[i] for i in ids]
+    train_data = all_data[:int(len(all_data)*0.85)]
+    val_data = all_data[int(len(all_data)*0.85):]
+    save_pkl(train_data, "training/data/train_data.pkl")
+    save_pkl(val_data, "training/data/val_data.pkl")
+
+def split_train_val_TF():
+    import pickle as pkl
+    data = torch.load("demonstration/old_model/demonstration_torchsave.pkl", map_location='cuda:0')
+
+    contexts = []
+    sents = []
+    ys = []
+    for d in data:
+        for candidate in d['individual_features']:
+            contexts.append(d['shared_features']['context'])
+            sents.append(candidate['sent'])
+            ys.append(candidate['pick_or_not'])
+    all_data = list(zip(contexts, sents, ys))
+
+    import random
+    random.seed(123)
+    random.shuffle(all_data)
+    train_data = all_data[:int(len(all_data)*0.8)]
+    val_data = all_data[int(len(all_data)*0.8):]
+
+    torch.save(train_data, "demonstration/old_model/demonstration_train_with_text_only.pkl")
+    torch.save(val_data, "demonstration/old_model/demonstration_val_with_text_only.pkl")
+
+
+class SequenceSummary(nn.Module):
+    r""" Compute a single vector summary of a sequence hidden states according to various possibilities:
+        Args of the config class:
+            summary_type:
+                - 'last' => [default] take the last token hidden state (like XLNet)
+                - 'first' => take the first token hidden state (like Bert)
+                - 'mean' => take the mean of all tokens hidden states
+                - 'cls_index' => supply a Tensor of classification token position (GPT/GPT-2)
+                - 'attn' => Not implemented now, use multi-head attention
+            summary_use_proj: Add a projection after the vector extraction
+            summary_proj_to_labels: If True, the projection outputs to config.num_labels classes (otherwise to hidden_size). Default: False.
+            summary_activation: 'tanh' => add a tanh activation to the output, Other => no activation. Default
+            summary_first_dropout: Add a dropout before the projection and activation
+            summary_last_dropout: Add a dropout after the projection and activation
+    """
+    def __init__(self, num_labels, config):
+        super().__init__()
+        self.config = config
+        self.summary_type = config.summary_type if hasattr(config, "summary_type") else "last"
+        if self.summary_type == "attn":
+            # We should use a standard multi-head attention module with absolute positional embedding for that.
+            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276
+            # We can probably just use the multi-head attention module of PyTorch >=1.1.0
+            raise NotImplementedError
+        self.summary = Identity()
+        if hasattr(config, "summary_use_proj") and config.summary_use_proj:
+            if hasattr(config, "summary_proj_to_labels") and config.summary_proj_to_labels and num_labels > 0:
+                print(f"num_class: {num_labels}")
+                num_classes = num_labels
+            else:
+                print(f"num_class here: {config.hidden_size}")
+                num_classes = config.hidden_size
+            self.summary = nn.Linear(config.hidden_size, num_classes)
+        self.activation = nn.Tanh()
+        if hasattr(config, "summary_activation") and config.summary_activation == "tanh":
+            self.activation = nn.Tanh()
+        self.first_dropout = Identity()
+        if hasattr(config, "summary_first_dropout") and config.summary_first_dropout > 0:
+            self.first_dropout = nn.Dropout(config.summary_first_dropout)
+        self.last_dropout = Identity()
+        if hasattr(config, "summary_last_dropout") and config.summary_last_dropout > 0:
+            self.last_dropout = nn.Dropout(config.summary_last_dropout)
+        
+        self.apply(self.init_weights)
+
+    def init_weights(self, module):
+        """ Initialize the weights.
+        """
+        from transformers.modeling_gpt2 import Conv1D
+        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):
+            # Slightly different from the TF version which uses truncated_normal for initialization
+            # cf https://github.com/pytorch/pytorch/pull/5617
+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
+            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:
+                module.bias.data.zero_()
+        elif isinstance(module, nn.LayerNorm):
+            module.bias.data.zero_()
+            module.weight.data.fill_(1.0)
+            
+    def forward(self, hidden_states, cls_index=None):
+        """ hidden_states: float Tensor in shape [bsz, ..., seq_len, hidden_size], the hidden-states of the last layer.
+            cls_index: [optional] position of the classification token if summary_type == 'cls_index',
+                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.
+                if summary_type == 'cls_index' and cls_index is None:
+                    we take the last token of the sequence as classification token
+        """
+        if self.summary_type == "last":
+            output = hidden_states[:, -1]
+        elif self.summary_type == "first":
+            output = hidden_states[:, 0]
+        elif self.summary_type == "mean":
+            output = hidden_states.mean(dim=1)
+        elif self.summary_type == "cls_index":
+            if cls_index is None:
+                cls_index = torch.full_like(hidden_states[..., :1, :], hidden_states.shape[-2] - 1, dtype=torch.long)
+            else:
+                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)
+                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))
+            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states
+            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)
+        elif self.summary_type == "attn":
+            raise NotImplementedError
+        output = self.first_dropout(output)
+        output = self.summary(output)
+        output = self.activation(output)
+        output = self.last_dropout(output)
+        return output
+
+class PersuadeDataset(Dataset):
+    def __init__(self, data, tokenizer):
+        self.data = data
+        self.tokenizer = tokenizer
+        self.tokenizer.max_len = 1500
+        # tokenizer weird behavior
+        self.cls_token_id = tokenizer.cls_token_id#[628, 198]
+        self.turn_ending = [628, 198]
+        # tokenizer.encode("\n\n\n")        
+    def __len__(self):
+        return len(self.data)    
+    def __getitem__(self, index):
+        # dial_tokens = [tokenizer.encode(item) + self.turn_ending for item in self.data[index]]
+        role_ids = [turn[2][0] for turn in self.data[index]]
+        whole_sents = []
+        for turn in self.data[index]:
+            temp_sent = turn[0]
+            temp_sent = " ".join(temp_sent)
+            try:
+                assert temp_sent.startswith("A:") or temp_sent.startswith("B:")                
+            except:
+                pdb.set_trace()
+            whole_sents.append(self.tokenizer.encode(temp_sent[:2]) + self.tokenizer.encode(temp_sent[2:]) + self.turn_ending)
+            # assert whole_sents[-1]
+        separate_sents = []
+        for turn in self.data[index]:
+            turn_sents = []
+            for i, sent in enumerate(turn[0]):
+                if i == 0:
+                    turn_sents.append(self.tokenizer.encode(sent[:2]) + self.tokenizer.encode(sent[2:]))
+                else:
+                    turn_sents.append(self.tokenizer.encode(" "+sent))
+            separate_sents.append(turn_sents)
+        acts = []
+        for turn in self.data[index]:
+            turn_acts = []
+            for i, act in enumerate(turn[1]):
+                turn_acts.append(act)
+            acts.append(turn_acts)
+        return role_ids, whole_sents, separate_sents, acts        
+    def collate(self, unpacked_data):
+        return unpacked_data
+
+class TFDataset(Dataset):
+    def __init__(self, data, tokenizer):
+        self.data = data
+        self.tokenizer = tokenizer
+        self.tokenizer.max_len = 1500
+        # tokenizer weird behavior
+        self.cls_token_id = tokenizer.cls_token_id#[628, 198]
+        self.turn_ending = [628, 198]
+        # tokenizer.encode("\n\n\n")        
+    def __len__(self):
+        return len(self.data)    
+    def __getitem__(self, index):
+        # dial_tokens = [tokenizer.encode(item) + self.turn_ending for item in self.data[index]]
+        context = ["A:"+" ".join(s) if i%2==0 else "B:"+" ".join(s) for i, s in enumerate(self.data[index][0])]
+        context = [" ".join(s) for i, s in enumerate(self.data[index][0])]
+        context = [self.tokenizer.encode("A:") + self.tokenizer.encode(c) + self.turn_ending if i%2 == 0 else\
+                   self.tokenizer.encode("B:") + self.tokenizer.encode(c) + self.turn_ending for i, c in enumerate(context)]
+        candidate_sent = self.tokenizer.encode("A:") + self.tokenizer.encode(" ".join(self.data[index][1]))
+        y = int(self.data[index][2])
+        return context, candidate_sent, y
+    def collate(self, unpacked_data):
+        return unpacked_data
+    
+class ModelClassifier(object):
+    def __init__(self, config, which_to_train, model_A, model_B, tokenizer, device1, device2):
+        # config.num_labels = le.classes_.shape[0]
+        # label encode
+        # super().__init__()
+        self.config = config
+        self.le_A = load_pkl("training/data/labelencoder_A.pkl")
+        self.le_B = load_pkl("training/data/labelencoder_B.pkl")
+
+        self.clf_A = SequenceSummary(num_labels=self.le_A.classes_.shape[0], config=config)
+        self.clf_B = SequenceSummary(num_labels=self.le_B.classes_.shape[0], config=config)
+        self.clf_TF = SequenceSummary(num_labels=2, config=config)
+        
+        # self.apply(self.init_weight)
+        self.past = None
+        self.history = []
+
+        # model
+        self.model_A = model_A
+        self.model_B = model_B
+        self.tokenizer = tokenizer
+        self.cls_token_id = tokenizer.cls_token_id
+        self.device1 = device1
+        self.device2 = device2
+
+        self.to_device(self.device1)
+
+        # define loss
+        self.criterion = nn.CrossEntropyLoss()
+        
+        # optimizer parameters
+        self.num_gradients_accumulation = 1
+        self.batch_size = 1
+        self.batch_size_TF = 8
+
+        # load training data
+        self.load_data()
+
+    def reload(self):
+        self.past = None
+        self.history = []
+
+    def to_device(self, device):
+        # to device
+        self.clf_A = self.clf_A.to(device)
+        self.clf_B = self.clf_B.to(device)
+        self.clf_TF = self.clf_TF.to(device)
+
+        self.clf_A.device = device
+        self.clf_B.device = device
+        self.clf_TF.device = device
+        # self.model_A = self.model_A.to(self.device)
+        # self.model_B = self.model_B.to(self.device)
+
+    def load_data(self):
+        # load training data
+        self.train_data = load_pkl("training/data/train_data.pkl")
+        self.val_data = load_pkl("training/data/val_data.pkl")
+        self.train_data_TF, self.val_data_TF = torch.load("demonstration/old_model/demonstration_train_with_text_only.pkl", map_location="cpu"), \
+                                                torch.load("demonstration/old_model/demonstration_val_with_text_only.pkl", map_location="cpu")
+
+        self.train_dataset = PersuadeDataset(self.train_data, self.tokenizer)
+        self.val_dataset = PersuadeDataset(self.val_data, self.tokenizer)
+
+        self.train_dataset_TF, self.val_dataset_TF = TFDataset(self.train_data_TF, self.tokenizer), \
+                                                        TFDataset(self.val_data_TF, self.tokenizer)
+
+        self.train_dataloader = DataLoader(dataset=self.train_dataset, 
+                                            shuffle=True, 
+                                            batch_size=self.batch_size, 
+                                            collate_fn=self.train_dataset.collate)
+        self.val_dataloader = DataLoader(dataset=self.val_dataset, 
+                                            shuffle=False, 
+                                            batch_size=self.batch_size, 
+                                            collate_fn=self.train_dataset.collate)
+
+        self.train_dataloader_TF = DataLoader(dataset=self.train_dataset_TF, 
+                                            shuffle=True, 
+                                            batch_size=self.batch_size_TF, 
+                                            collate_fn=self.train_dataset_TF.collate)
+        self.val_dataloader_TF = DataLoader(dataset=self.val_dataset_TF, 
+                                            shuffle=False, 
+                                            batch_size=self.batch_size_TF, 
+                                            collate_fn=self.val_dataset_TF.collate)
+
+
+    def load_model(self, all_model_dir=None, clf_A_dir=None, clf_B_dir=None, clf_TF_dir=None):
+        if all_model_dir is None:
+            if clf_A_dir:
+                clf_A_state = torch.load(clf_A_dir)
+                self.clf_A.load_state_dict(clf_A_state)
+                print(f"clf_A loaded")
+
+            if clf_B_dir:
+                clf_B_state = torch.load(clf_B_dir)
+                self.clf_B.load_state_dict(clf_B_state)
+                print(f"clf_B loaded")
+
+            if clf_TF_dir:
+                clf_TF_state = torch.load(clf_TF_dir)
+                self.clf_TF.load_state_dict(clf_TF_state)
+                print(f"clf_TF loaded")
+        else:
+            model_A_state, model_B_state, clf_A_state, clf_B_state, clf_TF_state = torch.load(all_model_dir)
+            self.model_A.load_state_dict(model_A_state)
+            self.model_B.load_state_dict(model_B_state)
+            self.clf_A.load_state_dict(clf_A_state)
+            self.clf_B.load_state_dict(clf_B_state)
+            self.clf_TF.load_state_dict(clf_TF_state)
+            print(f"all models loaded")
+
+
+    def train(self, which_to_train, num_epochs=10):
+        # optimizer
+        param_optimizer = list(self.model_A.named_parameters()) + \
+                          list(self.model_B.named_parameters())
+        if "A" in which_to_train:
+            print("clf_A to optimize")
+            param_optimizer += list(self.clf_A.named_parameters()) 
+        if "B" in which_to_train:
+            print("clf_B to optimize")
+            param_optimizer += list(self.clf_B.named_parameters()) 
+        if "TF" in which_to_train:
+            print("clf_TF to optimize")
+            param_optimizer += list(self.clf_TF.named_parameters()) 
+
+        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
+        optimizer_grouped_parameters = [
+            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
+            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
+            ]
+
+        num_train_optimization_steps = len(self.train_dataset) * num_epochs // self.batch_size // self.num_gradients_accumulation
+
+        self.optimizer = OpenAIAdam(optimizer_grouped_parameters,
+                            lr=2e-5,
+                            warmup=0.1,
+                            max_grad_norm=1.0,
+                            weight_decay=0.01,
+                            t_total=num_train_optimization_steps)
+
+
+        update_count = 0
+        progress_bar = tqdm.tqdm
+        start = time.time()
+        best_acc_A = -float('Inf')
+        best_f1_A = -float('Inf')
+        best_acc_B = -float('Inf')
+        best_f1_B = -float('Inf')
+        best_acc_TF = -float('Inf')
+        best_f1_TF = -float('Inf')
+
+
+        for ep in tqdm.tqdm(range(num_epochs)):
+            
+            # set train mode
+            self.model_A.train()
+            self.model_B.train()
+            self.clf_A.train()
+            self.clf_B.train()
+            self.clf_TF.train()
+            
+            "Training"
+            pbar = progress_bar(self.train_dataloader)
+            train_dataloader_TF_list = list(self.train_dataloader_TF)
+            
+            for i, batch in enumerate(pbar):
+                batch = batch[0]
+                batch_TF = train_dataloader_TF_list[i%len(train_dataloader_TF_list)]
+                # without relative position
+                # if sum([len(item) for item in batch[1]]) > 1024:
+                #     input("1024 here!")
+                #     continue
+
+                record_loss = self.train_one_iter(batch, batch_TF, update_count, which_to_train, fp16=False)
+                update_count += 1
+
+                if update_count % self.num_gradients_accumulation == self.num_gradients_accumulation - 1:
+                    # update for gradient accumulation
+                    self.optimizer.step()
+                    self.optimizer.zero_grad()
+                    
+                    # speed measure
+                    end = time.time()
+                    speed = self.batch_size * self.num_gradients_accumulation / (end - start)
+                    start = end
+                    
+                    # show progress
+                    pbar.set_postfix(loss=record_loss, speed=speed)
+
+            "Evaluation"
+            self.model_A.eval()
+            self.model_B.eval()
+            self.clf_A.eval()
+            self.clf_B.eval()
+            self.clf_TF.eval()
+
+            (val_acc_A, val_f1_A), (val_acc_B, val_f1_B), (val_acc_TF, val_f1_TF) = self.validate(self.val_dataloader, self.val_dataloader_TF, ep, which_to_train)
+            print(f"A: val f1: {val_f1_A}, valid acc: {val_acc_A}")
+            print(f"B: val f1: {val_f1_B}, valid acc: {val_acc_B}")
+            print(f"TF: val f1: {val_f1_TF}, valid acc: {val_acc_TF}")
+            is_best_so_far_TF = val_f1_TF > best_f1_TF
+            is_best_so_far_A = val_f1_A > best_f1_A
+            is_best_so_far_B = val_f1_TF > best_f1_B
+            
+            if is_best_so_far_TF:
+                best_acc_TF = val_acc_TF
+                best_f1_TF = val_f1_TF
+            if is_best_so_far_A:
+                best_acc_A = val_acc_A
+                best_f1_A = val_f1_A
+            if is_best_so_far_B:
+                best_acc_B = val_acc_B
+                best_f1_B = val_f1_B
+            SAVED = False
+            if is_best_so_far_TF and not SAVED:
+                SAVED = True
+                torch.save((self.model_A.state_dict(), self.model_B.state_dict(), 
+                            self.clf_A.state_dict(), self.clf_B.state_dict(), 
+                            self.clf_TF.state_dict()),                             
+                            f"Checkpoint_act_clf/epoch{ep}_multitask_TF_best_acc_{val_acc_TF}_f1_{val_f1_TF}_A_acc_{val_acc_A}_f1_{val_f1_A}_B_acc_{val_acc_B}_f1_{val_f1_B}.pth")
+            if is_best_so_far_A and not SAVED:
+                SAVED = True
+                torch.save((self.model_A.state_dict(), self.model_B.state_dict(), 
+                            self.clf_A.state_dict(), self.clf_B.state_dict(), 
+                            self.clf_TF.state_dict()),                             
+                            f"Checkpoint_act_clf/epoch{ep}_multitask_TF_best_acc_{val_acc_TF}_f1_{val_f1_TF}_A_acc_{val_acc_A}_f1_{val_f1_A}_B_acc_{val_acc_B}_f1_{val_f1_B}.pth")
+            if is_best_so_far_B and not SAVED:
+                SAVED = True
+                torch.save((self.model_A.state_dict(), self.model_B.state_dict(), 
+                            self.clf_A.state_dict(), self.clf_B.state_dict(), 
+                            self.clf_TF.state_dict()),                             
+                            f"Checkpoint_act_clf/epoch{ep}_multitask_TF_best_acc_{val_acc_TF}_f1_{val_f1_TF}_A_acc_{val_acc_A}_f1_{val_f1_A}_B_acc_{val_acc_B}_f1_{val_f1_B}.pth")
+                 # if which_to_train == "A":
+                #     torch.save(model_A.state_dict(), f"Checkpoint_act_clf/A/best_acc_{best_acc}_f1_{best_f1}.pth")
+                # elif which_to_train == "B":
+                #     torch.save(model_A.state_dict(), f"Checkpoint_act_clf/B/best_acc_{best_acc}_f1_{best_f1}.pth")
+                # checkpointer.save_checkpoint(ep, model_A.state_dict(), {"None": None}, is_best_so_far)
+
+        print("finally")
+        print("A: \nbest acc: {}, best f1: {}".format(best_acc_A, best_f1_A))
+        print("B: \nbest acc: {}, best f1: {}".format(best_acc_B, best_f1_B))
+        print("TF: \nbest acc: {}, best f1: {}".format(best_acc_TF, best_f1_TF))
+
+    def validate(self, dataloader, dataloader_TF, ep, which_to_train):
+        from sklearn.metrics import f1_score
+        from sklearn.metrics import confusion_matrix
+        from utils import print_cm
+
+        # evaluation mode
+        self.model_A.eval()
+        self.model_B.eval()
+        self.clf_A.eval()
+        self.clf_B.eval()
+        self.clf_TF.eval()
+
+        def get_numbers_for_one_task(sents, logits, acts, x, y_true, y_pred, total, correct):
+            _, predicted_acts = torch.max(logits, 1)
+        
+            x.extend(sents)
+            y_true.extend(acts.tolist()[0])
+            y_pred.extend(predicted_acts.tolist())
+
+            total += len(acts.tolist()[0])
+            correct += (predicted_acts == acts).sum().item()
+
+            return x, y_true, y_pred, total, correct
+
+        progress_bar = tqdm.tqdm
+
+        with torch.no_grad():
+            pbar = progress_bar(dataloader)
+            dataloader_TF_list = list(dataloader_TF)
+            correct = 0
+            total = 0
+            x_A, y_true_A, y_pred_A, correct_A, total_A = [], [], [], 0, 0
+            x_B, y_true_B, y_pred_B, correct_B, total_B = [], [], [], 0, 0
+            x_TF, y_true_TF, y_pred_TF, correct_TF, total_TF = [], [], [], 0, 0
+
+            for i, batch in enumerate(pbar):
+                batch = batch[0]
+                batch_TF = dataloader_TF_list[i%len(dataloader_TF_list)]
+                # if sum([len(item) for item in batch[1]]) > 1024:
+                #     continue
+
+                sents_A, logits_A, acts_A,\
+                sents_B, logits_B, acts_B,\
+                sents_TF, logits_TF, acts_TF = self.train_one_iter(batch, batch_TF, None, which_to_train, fp16=False, 
+                                                   is_validation=True)
+                
+                x_A, y_true_A, y_pred_A, total_A, correct_A = get_numbers_for_one_task(sents_A, logits_A, acts_A,\
+                                                                                       x_A, y_true_A, y_pred_A, total_A, correct_A)
+                x_B, y_true_B, y_pred_B, total_B, correct_B = get_numbers_for_one_task(sents_B, logits_B, acts_B,\
+                                                                                       x_B, y_true_B, y_pred_B, total_B, correct_B)
+                x_TF, y_true_TF, y_pred_TF, total_TF, correct_TF = get_numbers_for_one_task(sents_TF, logits_TF, acts_TF,\
+                                                                                       x_TF, y_true_TF, y_pred_TF, total_TF, correct_TF)
+
+            f1_A = f1_score(y_true_A, y_pred_A, average="weighted")
+            f1_B = f1_score(y_true_B, y_pred_B, average="weighted")
+            f1_TF = f1_score(y_true_TF, y_pred_TF, average="binary")
+            # pdb.set_trace()
+            
+            pd.DataFrame(zip(x_A, self.le_A.inverse_transform(y_true_A).tolist(), self.le_A.inverse_transform(y_pred_A).tolist()),
+                        columns=['sent', 'y_true', 'y_pred']).to_csv(f"Checkpoint_act_clf/A/act_classifier_val_results_epoch{ep}.csv", index=None)
+            print(f"A: Epoch {ep} Validation accuracy: {correct_A/total_A}, f1: {f1_A}")
+            
+            pd.DataFrame(zip(x_B, self.le_B.inverse_transform(y_true_B).tolist(), self.le_B.inverse_transform(y_pred_B).tolist()),
+                        columns=['sent', 'y_true', 'y_pred']).to_csv(f"Checkpoint_act_clf/B/act_classifier_val_results_epoch{ep}.csv", index=None)
+            print(f"B: Epoch {ep} Validation accuracy: {correct_B/total_B}, f1: {f1_B}")
+            
+            pd.DataFrame(zip(x_TF, y_true_TF, y_pred_TF),
+                        columns=['sent', 'y_true', 'y_pred']).to_csv(f"Checkpoint_act_clf/TF/act_classifier_val_results_epoch{ep}.csv", index=None)
+            print(f"TF: Epoch {ep} Validation accuracy: {correct_TF/total_TF}, f1: {f1_TF}")
+            # print_cm(confusion_matrix(y_true, y_pred, labels=range(len(le.classes_))), labels=[l[:] for l in le.classes_.tolist()])
+            return (correct_A/total_A, f1_A), (correct_B/total_B, f1_B), (correct_TF/total_TF, f1_TF)
+
+    def set_past(self, sent, which_task):
+        "sent: str, a whole sent"
+        # assert sent.startswith("A:") or sent.startswith("B:")
+        if sent.startswith("A:") or sent.startswith("B:"):
+            pdb.set_trace()
+            sent = sent[2:]
+
+        if which_task == "A":
+            lm_model = self.model_A
+            prefix = "A:"
+            device = lm_model.device
+        elif which_task == "B":
+            lm_model = self.model_B
+            prefix = "B:"
+            device = lm_model.device
+        elif which_task == "TF":
+            lm_model = self.model_A
+            prefix = "A:"
+            # candidate_sent = prefix+" ".join(separate_sents)
+            device = lm_model.device
+        
+        # encode sent
+        self.history.append(prefix+sent)
+        sent = self.tokenizer.encode(prefix) + self.tokenizer.encode(sent) + self.train_dataset.turn_ending
+        sent = torch.LongTensor(sent).unsqueeze(0).to(device)
+
+        past = self.move_to_device(self.past, lm_model)
+        _, past, _ = lm_model(sent, past) 
+        
+        self.past = past
+        
+
+    def predict(self, separate_sents, which_task):
+        "separate_sents: list of sentences with no prefix"
+        past = self.past
+        
+        if which_task == "A":
+            lm_model = self.model_A
+            clf_head = self.clf_A
+            le = self.le_A
+            prefix = "A:"
+            device = lm_model.device
+        elif which_task == "B":
+            lm_model = self.model_B
+            clf_head = self.clf_B
+            le = self.le_B
+            prefix = "B:"
+            device = lm_model.device
+        elif which_task == "TF":
+            lm_model = self.model_A
+            clf_head = self.clf_TF
+            prefix = "A:"
+            candidate_sent = " ".join(separate_sents)
+            device = lm_model.device
+        
+        # evaluation mode
+        self.model_A.eval()
+        self.model_B.eval()
+        self.clf_A.eval()
+        self.clf_B.eval()
+        self.clf_TF.eval()
+
+        with torch.no_grad():
+            if which_task in ["A", "B"]:
+                all_logits = []
+                for i, sent in enumerate(separate_sents):
+                    if i == 0:
+                        sent = self.tokenizer.encode(prefix) + self.tokenizer.encode(sent)
+                    else:
+                        sent = self.tokenizer.encode(" "+sent)
+
+                    # pdb.set_trace()
+                    sent = torch.LongTensor(sent).unsqueeze(0).to(device)
+                    past = self.move_to_device(past, lm_model)
+                    logits, past, hidden_states = lm_model(sent, past)
+                    
+                    # encode [CLS]
+                    cls_token_tensor = torch.LongTensor([self.cls_token_id]).unsqueeze(0).to(device)
+                    _, _, hidden_states = lm_model(cls_token_tensor, past)
+                    hidden_states = self.move_to_device(hidden_states, clf_head)
+                    mc_logits = clf_head(hidden_states[-1], cls_index=None).squeeze(-1)
+                    
+                    all_logits.append(mc_logits)
+
+                # finish tail
+                end_input = torch.LongTensor(self.train_dataset.turn_ending).unsqueeze(0).to(device)
+                past = self.move_to_device(past, lm_model)
+                _, past, _ = lm_model(end_input, past) 
+
+                # get labels
+                all_logits = torch.cat(all_logits, dim=0)
+                # pdb.set_trace()
+                _, predicted_acts = torch.max(all_logits, 1)
+                predicted_acts = predicted_acts.tolist()
+                predicted_acts = le.inverse_transform(predicted_acts).tolist()
+
+                return predicted_acts, past
+            elif which_task == "TF":
+                # encode candidate
+                candidate = self.tokenizer.encode(prefix) + self.tokenizer.encode(candidate_sent)
+                # pdb.set_trace()
+                candidate = torch.LongTensor(candidate).unsqueeze(0).to(device)
+                past = self.move_to_device(past, self.model_A)
+                logits, past, hidden_states = self.model_A(candidate, past)
+                # encode [CLS]
+                cls_token_tensor = torch.LongTensor([self.cls_token_id]).unsqueeze(0).to(device)
+                _, _, hidden_states = self.model_A(cls_token_tensor, past)
+                hidden_states = self.move_to_device(hidden_states, self.clf_TF)
+                mc_logits = self.clf_TF(hidden_states[-1], cls_index=None).squeeze(-1)
+                # pdb.set_trace()
+                _, predicted_acts = torch.max(mc_logits, 1)
+                predicted_acts = predicted_acts.tolist()
+                assert len(predicted_acts) == 1
+                return predicted_acts[0], past
+
+    def train_one_iter(self, batch, batch_TF, update_count, which_to_train, fp16=False, is_validation=False):
+        # role_ids, whole_sents, separate_sents, acts = batch
+        past = None
+        all_sents_A, all_logits_A, all_acts_A = [], [], []
+        all_sents_B, all_logits_B, all_acts_B = [], [], []
+        for i, (role_id, whole_sent, separate_sents, acts) in enumerate(zip(*batch)):            
+            if role_id == 0:
+                whole_sent = torch.LongTensor(whole_sent).unsqueeze(0).to(self.device1)
+                try:
+                    assert self.tokenizer.decode(whole_sent[0][:2].tolist()) == "A:"
+                except:
+                    pdb.set_trace()
+                if "A" in which_to_train:
+                    past = self.move_to_device(past, self.model_A)
+                    _, real_past, _ = self.model_A(whole_sent, past)
+                    for act, sent in zip(acts, separate_sents):
+                        all_sents_A.append(self.tokenizer.decode(sent))
+                        # pdb.set_trace()
+                        # 'A:HI I would like to tell you About a childrens charity called Save the CHildren.'
+                        sent = torch.LongTensor(sent).unsqueeze(0).to(self.device1)
+                        past = self.move_to_device(past, self.model_A)
+                        logits, past, hidden_states = self.model_A(sent, past)
+                        
+                        # pdb.set_trace()
+                        # encode [CLS]
+                        cls_token_tensor = torch.LongTensor([self.cls_token_id]).unsqueeze(0).to(self.device1)
+                        past = self.move_to_device(past, self.model_A)
+                        _, _, hidden_states = self.model_A(cls_token_tensor, past)
+                        
+                        mc_logits = self.clf_A(hidden_states[-1], cls_index=None).squeeze(-1)
+                        all_logits_A.append(mc_logits)
+                        all_acts_A.append(act)
+                    # pdb.set_trace()
+                    past = real_past
+                    # # finish tail
+                    # end_input = torch.LongTensor(self.train_dataset.turn_ending).unsqueeze(0).to(self.device1)
+                    # _, past, _ = self.model_A(end_input, past) 
+                else:
+                    past = self.move_to_device(past, self.model_A)
+                    _, past, hidden_states = self.model_A(whole_sent, past)
+            else:
+                whole_sent = torch.LongTensor(whole_sent).unsqueeze(0).to(self.device2)
+                try:
+                    assert self.tokenizer.decode(whole_sent[0][:2].tolist()) == "B:"
+                except:
+                    pdb.set_trace()
+                if "B" in which_to_train:
+                    past = self.move_to_device(past, self.model_B)
+                    _, real_past, _ = self.model_B(whole_sent, past)
+                    for act, sent in zip(acts, separate_sents):
+                        all_sents_B.append(self.tokenizer.decode(sent))
+                        # pdb.set_trace()
+                        #'B:ok please do'
+                        sent = torch.LongTensor(sent).unsqueeze(0).to(self.device2)
+                        past = self.move_to_device(past, self.model_B)
+                        logits, past, hidden_states = self.model_B(sent, past)
+                        
+                        # encode [CLS]
+                        cls_token_tensor = torch.LongTensor([self.cls_token_id]).unsqueeze(0).to(self.device2)
+                        _, _, hidden_states = self.model_B(cls_token_tensor, past)
+                        
+                        hidden_states = self.move_to_device(hidden_states, self.clf_B)
+                        mc_logits = self.clf_B(hidden_states[-1], cls_index=None).squeeze(-1)
+                        all_logits_B.append(mc_logits)
+                        all_acts_B.append(act)
+                    past = real_past 
+                    # finish tail
+                    # end_input = torch.LongTensor(self.train_dataset.turn_ending).unsqueeze(0).to(self.device2)
+                    # past = self.move_to_device(past, self.model_B)
+                    # _, past, _ = self.model_B(end_input, past) 
+                else:
+                    past = self.move_to_device(past, self.model_B)
+                    _, past, hidden_states = self.model_B(whole_sent, past)
+        
+        all_logits_A = torch.cat(all_logits_A, dim=0)
+        all_acts_A = torch.tensor(all_acts_A).unsqueeze(0).to(self.device1)
+        # pdb.set_trace()
+        loss_A = self.criterion(all_logits_A.view(-1, all_logits_A.size(-1)), all_acts_A.view(-1))
+
+        all_logits_B = torch.cat(all_logits_B, dim=0)
+        all_acts_B = torch.tensor(all_acts_B).unsqueeze(0).to(self.device1)
+
+        loss_B = self.criterion(all_logits_B.view(-1, all_logits_B.size(-1)), all_acts_B.view(-1))
+        
+        # TF task
+        all_contexts_candidate_TF = []
+        all_logits_TF = []
+        all_acts_TF = []
+        for one_dial in batch_TF:
+            past = None
+            contexts, candidate, pick_or_not = one_dial
+            all_contexts_candidate_TF.append((" ".join([self.tokenizer.decode(c) for c in contexts]), 
+                                              self.tokenizer.decode(candidate)))
+            
+            # get past
+            for i, context in enumerate(contexts):
+                if i%2 == 0:
+                    # pdb.set_trace()
+                    #'A:Would you like to know more about the charity Save the Children?\n\n\n'
+                    context = torch.LongTensor(context).unsqueeze(0).to(self.device1)
+                    past = self.move_to_device(past, self.model_A)
+                    logits, past, hidden_states = self.model_A(context, past)
+                else:
+                    # pdb.set_trace()
+                    #'B:hello I am great.\n\n\n'
+                    context = torch.LongTensor(context).unsqueeze(0).to(self.device2)
+                    past = self.move_to_device(past, self.model_B)
+                    logits, past, hidden_states = self.model_B(context, past)
+            
+            # encode candidate
+            # pdb.set_trace()
+            # "A:Save the Children is an international non-governmental organization that promotes children's rights, provides relief and helps support children in developing countries."
+            candidate = torch.LongTensor(candidate).unsqueeze(0).to(self.device1)
+            past = self.move_to_device(past, self.model_A)
+            logits, past, hidden_states = self.model_A(candidate, past)
+            # encode [CLS]
+            cls_token_tensor = torch.LongTensor([self.cls_token_id]).unsqueeze(0).to(self.device1)
+            _, _, hidden_states = self.model_A(cls_token_tensor, past)
+            
+            mc_logits = self.clf_TF(hidden_states[-1], cls_index=None).squeeze(-1)
+            all_logits_TF.append(mc_logits)
+            all_acts_TF.append(pick_or_not)
+
+        all_logits_TF = torch.cat(all_logits_TF, dim=0)
+        all_acts_TF = torch.tensor(all_acts_TF).unsqueeze(0).to(self.device1)
+
+        loss_TF = self.criterion(all_logits_TF.view(-1, all_logits_TF.size(-1)), all_acts_TF.view(-1))
+        
+        if is_validation:
+            return all_sents_A, all_logits_A, all_acts_A,\
+                   all_sents_B, all_logits_B, all_acts_B,\
+                   all_contexts_candidate_TF, all_logits_TF, all_acts_TF
+
+        loss = loss_A.to(self.device1) + loss_B.to(self.device1) + loss_TF.to(self.device1)
+
+        loss /= self.num_gradients_accumulation
+        
+        if fp16:
+            with amp.scale_loss(loss, self.optimizer) as scaled_loss:
+                scaled_loss.backward()
+        else:
+            loss.backward()
+        
+        record_loss = loss.item() * self.num_gradients_accumulation
+        
+        return record_loss#, perplexity
+
+    def move_to_device(self, past, target):
+        if past is not None and target.device != past[0].device:
+            past = [p.to(target.device) for p in past]
+        return past
+
+def build_model_classifier(model_dir, device1, device2):
+    config = GPT2Config()
+    config = config.from_pretrained('gpt2')#config.from_pretrained('gpt2-medium')
+    config.summary_first_dropout = 0.2
+    config.summary_type = "cls_index"
+
+    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")#torch.load(tokenizer_dir)
+    tokenizer.add_special_tokens({'cls_token': '[CLS]'})
+    # device1 = torch.device("cuda:0")
+    # device2 = torch.device("cuda:1")
+    model_A, model_B = load_model(cfg, "small", tokenizer, device1, device2)
+
+    # pdb.set_trace()
+    print("model_clf device\n\n\n\n\n\n")
+    print(model_A.device)
+    print(model_B.device)
+    print("here\n\n\n")
+    which_to_train = ["A", "B", "TF"]
+    model_clf = ModelClassifier(config=config, which_to_train=which_to_train, 
+                                model_A=model_A, model_B=model_B,
+                                tokenizer=tokenizer, device1=device1, device2=device2)
+    
+    model_clf.load_model(all_model_dir=model_dir)
+
+    return model_clf
+
+if __name__ == "__main__": 
+
+    parser = argparse.ArgumentParser()
+    parser.add_argument('-l', '--load', action='store_true', 
+                        help="load model")
+    parser.add_argument('-t', '--test', action='store_true', 
+                        help="test model")
+    parser.add_argument('-v', '--validation', action='store_true', 
+                        help="validate model")
+    parser.add_argument('-n', '--num_epoch', type=int, 
+                        default=10,
+                        help='num of epoch')
+    # parser.add_argument('-d', '--device', type=str, default='cpu',
+    #                      help='device to use')
+    parser.add_argument('-m', '--model_dir', type=str, 
+                        # default="Checkpoint_act_clf/multitask_TF_best_acc_0.7777777777777778_f1_0.776536312849162_A_acc_0.6954838709677419_f1_0.6707423935799665_B_acc_0.6166134185303515_f1_0.5898033645875225.pth",
+                        help='model dir')
+    args = parser.parse_args()
+
+    torch.backends.cudnn.benchmark = True
+    torch.manual_seed(123)
+    np.random.seed(123)
+
+    config = GPT2Config()
+    config = config.from_pretrained('gpt2')#config.from_pretrained('gpt2-medium')
+    config.summary_first_dropout = 0.2
+    config.summary_type = "cls_index"
+
+    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")#torch.load(tokenizer_dir)
+    tokenizer.add_special_tokens({'cls_token': '[CLS]'})
+    device1 = torch.device("cuda:1")
+    device2 = torch.device("cuda:0")
+    model_A, model_B = load_model(cfg, "small", tokenizer, device1, device2)
+
+    print("device\n\n\n\n\n\n")
+    print(model_A.device)
+    print(model_B.device)
+    print("here\n\n\n")
+    which_to_train = ["A", "B", "TF"]
+    model_clf = ModelClassifier(config=config, which_to_train=which_to_train, 
+                                model_A=model_A, model_B=model_B,
+                                tokenizer=tokenizer, device1=device1, device2=device2)
+
+    # training
+    # model_clf.validate(model_clf.val_dataloader, model_clf.val_dataloader_TF, 0, which_to_train)
+    if not args.test and not args.validation:
+        if args.load:
+            model_clf.load_model(all_model_dir=args.model_dir)
+        model_clf.train(which_to_train=which_to_train, num_epochs=args.num_epoch)
+    elif args.validation:
+        model_clf.load_model(all_model_dir=args.model_dir)
+        model_clf.validate(model_clf.val_dataloader, model_clf.val_dataloader_TF, 0, which_to_train)
+    elif args.test:
+        model_clf.load_model(all_model_dir=args.model_dir)
+        # past = None
+        while True:
+            input_text = input("input: ")
+            which_task = input("task: ")
+            input_texts = sent_tokenize(input_text)            
+            predicted_acts, _ = model_clf.predict(separate_sents=input_texts, which_task=which_task)
+            print(predicted_acts)
\ No newline at end of file
diff --git a/results.py b/results.py
new file mode 100644
index 0000000..d458d79
--- /dev/null
+++ b/results.py
@@ -0,0 +1,4 @@
+def add(a, b):
+   summ = a + b
+   return summ
+
diff --git a/self_play.py b/self_play.py
new file mode 100644
index 0000000..ae5b081
--- /dev/null
+++ b/self_play.py
@@ -0,0 +1,312 @@
+from PersuasionInteract import PersuasiveBot
+
+#!/usr/bin/env python
+# coding: utf-8
+
+# In[1]:
+
+
+import os
+os.environ["CUDA_VISIBLE_DEVICES"] = "1,3,5"
+
+
+# In[2]:
+
+
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.utils.data import DataLoader, Dataset
+from torch.nn.utils.rnn import pad_sequence
+import numpy as np
+import regex as re
+import random
+import itertools
+import tqdm
+import time
+
+from torch.utils.tensorboard import SummaryWriter
+from apex import amp
+from gpt_model import GPT2SimpleLM
+from pytorch_pretrained_bert import GPT2Tokenizer, OpenAIAdam
+from allennlp.nn.beam_search import BeamSearch
+from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config
+from torchfly.modules.losses import SequenceFocalLoss, SequenceCrossEntropyLoss
+
+
+# In[3]:
+
+
+class PersuadeDataset(Dataset):
+    def __init__(self, data, tokenizer):
+        self.data = data
+        self.tokenizer = tokenizer
+        self.tokenizer.max_len = 1500
+        self.turn_ending = tokenizer.encode("\n\n\n")
+        self.dialog_ending = [tokenizer.encoder["[EOS]"]]
+        
+    def __len__(self):
+        return len(self.data)
+    
+    def __getitem__(self, index):
+        dial_tokens = [tokenizer.encode(item) + self.turn_ending for item in self.data[index]]
+        role_ids = [0 if item[0] == 32 else 1 for item in dial_tokens]
+        dial_tokens[-1] = dial_tokens[-1][:-2] + self.dialog_ending
+        return role_ids, dial_tokens
+        
+
+class Collate_Function:
+    """This function handles batch collate.
+    """
+    def __init__(self, tokenizer):
+        self.tokenizer = tokenizer
+        self.EOS = self.tokenizer.encoder["[EOS]"]
+
+    def __call__(self, unpacked_data):
+        return unpacked_data
+
+
+# In[4]:
+
+
+def top_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):
+    """ Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering
+        Args:
+            logits: logits distribution shape (vocabulary size)
+            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.
+            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset
+                whose total probability mass is greater than or equal to the threshold top_p.
+                In practice, we select the highest probability tokens whose cumulative probability mass exceeds
+                the threshold top_p.
+    """
+    # batch support!
+    if top_k > 0:
+        values, _ = torch.topk(logits, top_k)
+        min_values = values[:, -1].unsqueeze(1).repeat(1, logits.shape[-1])
+        logits = torch.where(logits < min_values, 
+                             torch.ones_like(logits, dtype=logits.dtype) * -float('Inf'), 
+                             logits)
+    if top_p > 0.0:
+        # Compute cumulative probabilities of sorted tokens
+        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
+        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
+
+        # Remove tokens with cumulative probability above the threshold
+        sorted_indices_to_remove = cumulative_probabilities > top_p
+        # Shift the indices to the right to keep also the first token above the threshold
+        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
+        sorted_indices_to_remove[..., 0] = 0
+        
+        sorted_logits = sorted_logits.masked_fill_(sorted_indices_to_remove, filter_value)
+        logits = torch.zeros_like(logits).scatter(1, sorted_indices, sorted_logits)
+    
+    return logits
+
+
+# In[5]:
+
+
+tokenizer = torch.load("DataProcess/special3_gpt2_tokenizer.pkl")
+
+class GPT2SmallConfig:
+    vocab_size = 50257 + len(tokenizer.__special_tokens__)
+    n_special = len(tokenizer.__special_tokens__)
+    n_positions = 1024
+    n_ctx = 1024
+    n_embd = 768
+    n_layer = 12
+    n_head = 12
+    resid_pdrop = 0.1
+    embd_pdrop = 0.1
+    attn_pdrop = 0.1
+    layer_norm_epsilon = 1e-5
+    initializer_range = 0.02
+    gradient_checkpointing = False
+    
+class GPT2MediumConfig:
+    vocab_size = 50257 + len(tokenizer.__special_tokens__)
+    n_special = len(tokenizer.__special_tokens__)
+    n_positions = 1024
+    n_ctx = 1024
+    n_embd = 1024
+    n_layer = 24
+    n_head = 16
+    resid_pdrop = 0.1
+    embd_pdrop = 0.1
+    attn_pdrop = 0.1
+    layer_norm_epsilon = 1e-5
+    initializer_range = 0.02
+    gradient_checkpointing = True
+
+
+# In[6]:
+
+
+tokenizer.encode("[name_slot]")
+
+
+# In[7]:
+
+
+tokenizer.decode([43384])
+
+
+# ### load the data
+
+# In[8]:
+
+
+train_data = torch.load("DataProcess/train_dialogs.pkl")
+val_data = torch.load("DataProcess/val_dialogs.pkl")
+
+train_dataset = PersuadeDataset(train_data, tokenizer)
+val_dataset = PersuadeDataset(val_data, tokenizer)
+
+batch_size = 1
+collate_func = Collate_Function(tokenizer)
+
+train_dataloader = DataLoader(dataset=train_dataset, 
+                              shuffle=True, 
+                              batch_size=batch_size, 
+                              collate_fn=collate_func)
+val_dataloader = DataLoader(dataset=val_dataset, 
+                            shuffle=False, 
+                            batch_size=batch_size, 
+                            collate_fn=collate_func)
+
+
+# In[9]:
+
+
+model_A = GPT2SimpleLM(GPT2SmallConfig)
+model_B = GPT2SimpleLM(GPT2SmallConfig)
+model_A_states, model_B_states = torch.load("Checkpoint/best.th", map_location="cuda:1")#torch.load("CheckpointMedium/model_state_epoch_3.th")
+
+# model_A = GPT2SimpleLM(GPT2MediumConfig)
+# model_B = GPT2SimpleLM(GPT2MediumConfig)
+# model_A_states, model_B_states = torch.load("Checkpoint/best.th")#torch.load("persuasion_medium_3.th")
+
+model_A.load_state_dict(model_A_states)
+model_B.load_state_dict(model_B_states)
+
+
+# In[10]:
+
+
+device = torch.device("cuda")
+model_A = model_A.to(device)
+model_B = model_B.to(device)
+
+
+# ## Test
+
+# In[30]:
+
+
+model_A.eval()
+model_B.eval()
+
+prev_input = tokenizer.encode("A:")
+prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(device)
+# past_position_ids = torch.LongTensor([[0, 1]]).to(device)
+
+temperature = 0.8
+past = None
+flag = True
+
+sep = tokenizer.encode("\n\n\n")
+
+while flag:
+    "Sampling based method"
+    sent = []
+    with torch.no_grad():
+        for i in range(200):
+            logits, past = model_A(prev_input, past=past)
+            logits = logits[:, -1, :] / temperature
+            logits = top_filtering(logits, top_k=200, top_p=0.9)
+            # prev_input = logits.argmax(-1).unsqueeze(1)
+            probs = F.softmax(logits, -1)
+            prev_input = torch.multinomial(probs, num_samples=1)
+            prev_word = prev_input.item()
+
+            if prev_word == 628:
+                break
+            elif prev_word == tokenizer.encoder["[EOS]"]:
+                flag = False
+                break
+            else:
+                sent.append(prev_word)
+            
+            # past_position_ids = past_position_ids[:, -1:] + 1
+
+    if not flag:
+        break
+
+    print("A:" + tokenizer.decode(sent))
+    
+    # finish tail
+    prev_input = torch.LongTensor(sep).unsqueeze(0).to(device)
+    _, past = model_A(prev_input, past=past)
+    
+    # input and update B's utterance
+#     user = input("B:")
+    
+#     if user == "quit":
+#         break
+        
+#     user = tokenizer.encode("B:" + user)
+#     prev_input = user + sep
+#     prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(device)
+    
+#     _, past = model_B(prev_input, past=past)
+    
+    prev_input = tokenizer.encode("B:")
+    prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(device)
+    
+    sent = []
+    with torch.no_grad():
+        for i in range(200):
+            logits, past = model_B(prev_input, past=past)
+            logits = logits[:, -1, :] / temperature
+            logits = top_filtering(logits, top_k=200, top_p=0.9)
+            # prev_input = logits.argmax(-1).unsqueeze(1)
+            probs = F.softmax(logits, -1)
+            prev_input = torch.multinomial(probs, num_samples=1)
+            prev_word = prev_input.item()
+
+            if prev_word == 628:
+                break
+            elif prev_word == tokenizer.encoder["[EOS]"]:
+                flag = False
+                break
+            else:
+                sent.append(prev_word)
+    
+    print("B:" + tokenizer.decode(sent))
+    
+    # finish tail
+    prev_input = torch.LongTensor(sep).unsqueeze(0).to(device)
+    _, past = model_B(prev_input, past=past)
+    
+    # start A's utterance
+    suffix = tokenizer.encode("A:")
+    prev_input = torch.LongTensor(suffix).unsqueeze(0).to(device)
+
+
+# In[12]:
+
+
+# past[0].shape
+
+
+# In[ ]:
+
+
+
+
+
+# In[ ]:
+
+
+
+
diff --git a/train_sent_predictor.py b/train_sent_predictor.py
index 60c26da..0aeebed 100644
--- a/train_sent_predictor.py
+++ b/train_sent_predictor.py
@@ -15,6 +15,7 @@ import random
 import itertools
 import tqdm
 import time
+import pickle as pkl
 
 from torch.utils.tensorboard import SummaryWriter
 from apex import amp
@@ -22,8 +23,8 @@ from allennlp.training.checkpointer import Checkpointer
 from gpt_model import GPT2SimpleLM, GPT2MultipleChoiceHead
 from pytorch_pretrained_bert import GPT2Tokenizer, OpenAIAdam, GPT2Model
 # from torchfly.criterions import SequenceFocalLoss, SequenceCrossEntropyLoss
-from torchfly.modules.losses import SequenceFocalLoss, SequenceCrossEntropyLoss
-from UnlikelihoodLoss import SequenceUnlikelihoodLoss
+# from torchfly.modules.losses import SequenceFocalLoss, SequenceCrossEntropyLoss
+# from UnlikelihoodLoss import SequenceUnlikelihoodLoss
 # In[2]:
 
 
@@ -36,80 +37,129 @@ os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,4,5,6,7"
 import pdb
 
 # In[3]:
-
-
-class PersuadeDataset(Dataset):
-    def __init__(self, data, tokenizer):
-        self.data = data
-        self.tokenizer = tokenizer
-        self.tokenizer.max_len = 1500
-        self.turn_ending = tokenizer.encode("\n\n\n")
-        self.dialog_ending = [tokenizer.encoder["[EOS]"]]
+def split_train_val():
+    import pickle as pkl
+    with open("demonstration/old_model/demonstration.pkl", "rb") as fh:
+        data = pkl.load(fh)
+
+    data[0]['individual_features'][0]['hidden_states_after_generation']
+    data[0]['individual_features'][0]['pick_or_not']
+
+    all_data = []
+    for turn in data:
+        for d in turn['individual_features']:
+            if d['different_from_edition'] != 'human_added_sentence':
+                # features = d['hidden_states_after_generation']
+                features = torch.cat([d['past_after_generation'][:, :, :, -1, :].reshape((1, 1, 16*64*2)), d['hidden_states_after_generation']], dim=2)
+                all_data.append([features, d['pick_or_not']])
+                if d['hidden_states_after_generation'].shape[1] != 1:
+                    print("here")
+            else:
+                print(d['hidden_states_after_generation'].shape)
+    import random
+    random.seed(123)
+    random.shuffle(all_data)
+    train_data = all_data[:754]
+    val_data = all_data[754:]
+
+    with open("demonstration/old_model/demonstration_train_with_past.pkl", "wb") as fh:
+        pkl.dump(train_data, fh)
+
+    with open("demonstration/old_model/demonstration_val_with_past.pkl", "wb") as fh:
+        pkl.dump(val_data, fh)
+
+split_train_val()
+# class PersuadeDataset(Dataset):
+#     def __init__(self, data, tokenizer):
+#         self.data = data
+#         self.tokenizer = tokenizer
+#         self.tokenizer.max_len = 1500
+#         self.turn_ending = tokenizer.encode("\n\n\n")
+#         self.dialog_ending = [tokenizer.encoder["[EOS]"]]
         
-    def __len__(self):
-        return len(self.data)
+#     def __len__(self):
+#         return len(self.data)
     
-    def __getitem__(self, index):
-        dial_tokens = [tokenizer.encode(item) + self.turn_ending for item in self.data[index]]
-        role_ids = [0 if item[0] == 32 else 1 for item in dial_tokens]
-        dial_tokens[-1] = dial_tokens[-1][:-2] + self.dialog_ending
-        return role_ids, dial_tokens
+#     def __getitem__(self, index):
+#         dial_tokens = [tokenizer.encode(item) + self.turn_ending for item in self.data[index]]
+#         role_ids = [0 if item[0] == 32 else 1 for item in dial_tokens]
+#         dial_tokens[-1] = dial_tokens[-1][:-2] + self.dialog_ending
+#         return role_ids, dial_tokens
         
 
-class Collate_Function:
-    """This function handles batch collate.
-    """
-    def __init__(self, tokenizer):
-        self.tokenizer = tokenizer
-        self.EOS = self.tokenizer.encoder["[EOS]"]
+# class Collate_Function:
+#     """This function handles batch collate.
+#     """
+#     def __init__(self, tokenizer):
+#         self.tokenizer = tokenizer
+#         self.EOS = self.tokenizer.encoder["[EOS]"]
         
-    def __call__(self, unpacked_data):
-        return unpacked_data
+#     def __call__(self, unpacked_data):
+#         return unpacked_data
 
 
-# In[4]:
-
+class SentenceDataset(Dataset):
+    def __init__(self, data):
+        self.data = data
+        # self.tokenizer = tokenizer
+        # self.tokenizer.max_len = 1500
+        # self.turn_ending = tokenizer.encode("\n\n\n")
+        # self.dialog_ending = [tokenizer.encoder["[EOS]"]]        
+    def __len__(self):
+        return len(self.data)    
+    def __getitem__(self, index):
+        return self.data[index][0], self.data[index][1]
+    def collate(self, unpacked_data):
+        return unpacked_data
 
-tokenizer = torch.load("DataProcess/special3_gpt2_tokenizer.pkl")
 
-class GPT2SmallConfig:
-    vocab_size = 50257 + len(tokenizer.__special_tokens__)
-    n_special = len(tokenizer.__special_tokens__)
-    n_positions = 1024
-    n_ctx = 1024
-    n_embd = 768
-    n_layer = 12
-    n_head = 12
-    resid_pdrop = 0.1
-    embd_pdrop = 0.1
-    attn_pdrop = 0.1
-    layer_norm_epsilon = 1e-5
-    initializer_range = 0.02
-    gradient_checkpointing = False
+# # tokenizer = torch.load("DataProcess/special3_gpt2_tokenizer.pkl")
+
+# class GPT2SmallConfig:
+#     vocab_size = 50257 + len(tokenizer.__special_tokens__)
+#     n_special = len(tokenizer.__special_tokens__)
+#     n_positions = 1024
+#     n_ctx = 1024
+#     n_embd = 768
+#     n_layer = 12
+#     n_head = 12
+#     resid_pdrop = 0.1
+#     embd_pdrop = 0.1
+#     attn_pdrop = 0.1
+#     layer_norm_epsilon = 1e-5
+#     initializer_range = 0.02
+#     gradient_checkpointing = False
     
-class GPT2MediumConfig:
-    vocab_size = 50257 + len(tokenizer.__special_tokens__)
-    n_special = len(tokenizer.__special_tokens__)
-    n_positions = 1024
-    n_ctx = 1024
-    n_embd = 1024
-    n_layer = 24
-    n_head = 16
-    resid_pdrop = 0.1
-    embd_pdrop = 0.1
-    attn_pdrop = 0.1
-    layer_norm_epsilon = 1e-5
-    initializer_range = 0.02
-    gradient_checkpointing = True
-
-
-# In[5]:
-
-
-model_A = GPT2SimpleLM(GPT2MediumConfig)
-model_B = GPT2SimpleLM(GPT2MediumConfig)
-model_A_states, model_B_states = torch.load("/home/wyshi/persuasion/consistency/ARDM/persuasion/persuasion_medium_3.th")#torch.load("CheckpointMedium/model_state_epoch_3.th")
-print("load success")
+# class GPT2MediumConfig:
+#     vocab_size = 50257 + len(tokenizer.__special_tokens__)
+#     n_special = len(tokenizer.__special_tokens__)
+#     n_positions = 1024
+#     n_ctx = 1024
+#     n_embd = 1024
+#     n_layer = 24
+#     n_head = 16
+#     resid_pdrop = 0.1
+#     embd_pdrop = 0.1
+#     attn_pdrop = 0.1
+#     layer_norm_epsilon = 1e-5
+#     initializer_range = 0.02
+#     gradient_checkpointing = True
+
+class ClassifierConfig:
+    n_embd = 1024*3
+    n_class = 2
+    summary_last_dropout = 0.2
+
+
+model_clf = GPT2MultipleChoiceHead(ClassifierConfig)
+model_state = torch.load("Checkpoint_clf/best_acc_0.7879746835443038_f1_0.7563636363636363_with_past.pth")
+model_clf.load_state_dict(model_state)
+import pdb
+
+# model_A = GPT2MultipleChoiceHead(GPT2MediumConfig)
+# model_B = GPT2SimpleLM(GPT2MediumConfig)
+# model_A_states, model_B_states = torch.load("/home/wyshi/persuasion/consistency/ARDM/persuasion/persuasion_medium_3.th")#torch.load("CheckpointMedium/model_state_epoch_3.th")
+# print("load success")
 # sent_selection_model = GPT2MultipleChoiceHead(GPT2MediumConfig)
 
 # model_A.load_state_dict(torch.load("/home/qingyang/Desktop/GPT2_Modification/special3_gpt2_small.pth"))
@@ -125,24 +175,26 @@ print("load success")
 
 # In[6]:
 
+with open("demonstration/old_model/demonstration_train_with_past.pkl", "rb") as fh:
+    train_data = pkl.load(fh)
 
-train_data = torch.load("DataProcess/train_dialogs.pkl")
-val_data = torch.load("DataProcess/val_dialogs.pkl")
+with open("demonstration/old_model/demonstration_val_with_past.pkl", "rb") as fh:
+    val_data = pkl.load(fh)
 
-train_dataset = PersuadeDataset(train_data, tokenizer)
-val_dataset = PersuadeDataset(val_data, tokenizer)
+train_dataset = SentenceDataset(train_data)
+val_dataset = SentenceDataset(val_data)
 
-batch_size = 1
-collate_func = Collate_Function(tokenizer)
+batch_size = 16
+# collate_func = Collate_Function(tokenizer)
 
 train_dataloader = DataLoader(dataset=train_dataset, 
                               shuffle=True, 
                               batch_size=batch_size, 
-                              collate_fn=collate_func)
+                              collate_fn=train_dataset.collate)
 val_dataloader = DataLoader(dataset=val_dataset, 
                             shuffle=False, 
                             batch_size=batch_size, 
-                            collate_fn=collate_func)
+                            collate_fn=train_dataset.collate)
 
 
 # ## Define the model
@@ -150,75 +202,40 @@ val_dataloader = DataLoader(dataset=val_dataset,
 # In[7]:
 
 
-device = torch.device("cuda:5")
+device = torch.device("cuda:2")
 torch.cuda.set_device(device)
-model_A = model_A.to(device)
-model_B = model_B.to(device)
+model_clf = model_clf.to(device)
+# model_A = model_A.to(device)
+# model_B = model_B.to(device)
 
 
 # In[8]:
 
 
 # define the losses
-criterion = SequenceFocalLoss(gamma=1.0, beta=0.0)
-eval_criterion = SequenceCrossEntropyLoss()
-unlikelihood_criterion = SequenceUnlikelihoodLoss(padding_idx=tokenizer.encoder["[PAD]"])
+# criterion = SequenceFocalLoss(gamma=1.0, beta=0.0)
+# eval_criterion = SequenceCrossEntropyLoss()
+# unlikelihood_criterion = SequenceUnlikelihoodLoss(padding_idx=tokenizer.encoder["[PAD]"])
 
 # In[9]:
 
 
 def train_one_iter(batch, update_count, fp16=False):
-    role_ids, dialog_tokens = batch
-    dial_inputs = [torch.LongTensor(item).unsqueeze(0).to(device) for item in dialog_tokens]
-    
-    past = None
-    all_logits = []
-    # A_logits = []
-    # B_logits = []
-    # A_target = []
-    # B_target = []
-#     user = tokenizer.encode("B:" + user)
-#     sep = tokenizer.encode("\n\n\n") 
-#     suffix = tokenizer.encode("A:")
-#     prev_input = sep + user + sep + suffix
-    
-#     prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(device)
-#     past_length = past_position_ids.item()
-    
-#     past_position_ids = np.arange(past_length, past_length+2).tolist() + \
-#                          np.arange(len(user) + 2).tolist() + \
-#                          np.arange(2).tolist()
-    
-#     past_position_ids = torch.LongTensor(past_position_ids).unsqueeze(0).to(device)
-    
-    for turn_num, dial_turn_inputs in enumerate(dial_inputs):
-        if role_ids[turn_num] == 0:
-            # breakpoint()
-            logits, past = model_A(dial_turn_inputs, past=past)
-            import pdb
-            pdb.set_trace()
-            all_logits.append(logits)
-        else:
-            # breakpoint()
-            logits, past = model_B(dial_turn_inputs, past=past)
-            all_logits.append(logits)
-
-    all_logits = torch.cat(all_logits, dim=1) # torch.Size([1, 505, 50260]), 505 = sum of tokens from 21 sentences
-    pdb.set_trace()
-    
-    
-    
-    # target
-    all_logits = all_logits[:, :-1].contiguous() # torch.Size([1, 504, 50260])
-    target = torch.cat(dial_inputs, dim=1)[:, 1:].contiguous()# torch.Size([1, 504])
-    target_mask = torch.ones_like(target).float()# torch.Size([1, 504])
-    
-    if False:
-        loss = criterion(all_logits, target, target_mask, label_smoothing=0.02, reduce=True) # torch.Size([])
-    else:
-        loss = unlikelihood_criterion(all_logits, target)
+    import pdb
+    # pdb.set_trace()
+    total, correct = (0, 0)
+    try:
+        hidden_states = torch.cat(list(map(lambda x: x[0], batch)))
+    except:
+        pdb.set_trace()
+    labels = torch.tensor(list(map(lambda x: int(x[1]), batch))).to(device)
+        
+    loss, outputs = model_clf(hidden_states=hidden_states, mc_labels=labels)
     loss /= num_gradients_accumulation
-    
+    _, predicted_labels = torch.max(outputs, 1)
+    total += labels.size(0)
+    correct += (predicted_labels == labels).sum().item()
+
     if fp16:
         with amp.scale_loss(loss, optimizer) as scaled_loss:
             scaled_loss.backward()
@@ -227,55 +244,38 @@ def train_one_iter(batch, update_count, fp16=False):
         
     record_loss = loss.item() * num_gradients_accumulation
     # print("record_loss: {}".format(record_loss))
-    perplexity = np.exp(record_loss)
+    # perplexity = np.exp(record_loss)
     
-    return record_loss, perplexity
-
+    return record_loss, correct/total, labels.tolist(), predicted_labels.tolist()
 
+from sklearn.metrics import f1_score
 def validate(dataloader):
     with torch.no_grad():
         pbar = progress_bar(dataloader)
 
-        total_ppl = []
-
+        correct = 0
+        total = 0
+        y_true, y_pred = [], []
         for batch in pbar:
-            
-            if sum([len(item) for item in batch[0][1]]) > 1024:
-                continue
-            
-            role_ids, dialog_tokens = batch[0]
-            dial_inputs = [torch.LongTensor(item).unsqueeze(0).to(device) for item in dialog_tokens]
-
-            past = None
-            all_logits = []
-            # A_logits = []
-            # B_logits = []
-            # A_target = []
-            # B_target = []
-
-            for turn_num, dial_turn_inputs in enumerate(dial_inputs):
-                if role_ids[turn_num] == 0:
-                    logits, past = model_A(dial_turn_inputs, past=past)
-                    all_logits.append(logits)
-                else:
-                    logits, past = model_B(dial_turn_inputs, past=past)
-                    all_logits.append(logits)
-
-            all_logits = torch.cat(all_logits, dim=1)
-            
-            # target
-            all_logits = all_logits[:, :-1].contiguous()
-            target = torch.cat(dial_inputs, dim=1)[:, 1:].contiguous()
-            target_mask = torch.ones_like(target).float()
-            
-            loss = eval_criterion(all_logits, target, target_mask, label_smoothing=-1, reduce="sentence")      
+            # batch = batch[0]
 
-            ppl = torch.exp(loss)
-            total_ppl.extend(ppl.tolist())
+            hidden_states = torch.cat(list(map(lambda x: x[0], batch))) 
+            labels = torch.tensor(list(map(lambda x: int(x[1]), batch))).to(device)
 
-        print(f"Epcoh {ep} Validation Perplexity: {np.mean(total_ppl)} Variance: {np.var(total_ppl)}")
+            import pdb
+            # pdb.set_trace()
+            y_true.extend(labels.tolist())
+            outputs = model_clf(hidden_states=hidden_states)
+
+            _, predicted_labels = torch.max(outputs, 1)
+            y_pred.extend(predicted_labels.tolist())
+            # print(predicted_labels)
+            total += labels.size(0)
+            correct += (predicted_labels == labels).sum().item()
+        f1 = f1_score(y_true, y_pred, average="binary")
+        print(f"Epcoh {ep} Validation accuracy: {correct/total}, f1: {f1}")
         
-        return np.mean(total_ppl)
+        return correct/total, f1
 
 
 # ### Training
@@ -283,7 +283,7 @@ def validate(dataloader):
 # In[10]:
 
 
-checkpointer = Checkpointer(serialization_dir="Checkpoint", 
+checkpointer = Checkpointer(serialization_dir="Checkpoint_clf", 
                             keep_serialized_model_every_num_seconds=3600*2, 
                             num_serialized_models_to_keep=5)
 
@@ -292,19 +292,19 @@ checkpointer = Checkpointer(serialization_dir="Checkpoint",
 
 
 # optimizer
-num_epochs = 10
+num_epochs = 600
 num_gradients_accumulation = 1
-num_train_optimization_steps = num_train_optimization_steps = len(train_dataset) * num_epochs // batch_size // num_gradients_accumulation
-
-param_optimizer = list(model_A.named_parameters()) + list(model_B.named_parameters())
-no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
-optimizer_grouped_parameters = [
-    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
-    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
-    ]
+num_train_optimization_steps = num_train_optimization_steps = len(train_dataset) * num_epochs // batch_size // num_gradients_accumulation *5
+print("num_train_optimization_steps: {}".format(num_train_optimization_steps))
+# param_optimizer = list(model_clf.named_parameters())# + list(model_B.named_parameters())
+# no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
+# optimizer_grouped_parameters = [
+#     {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
+#     {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
+#     ]
 
 
-optimizer = OpenAIAdam(optimizer_grouped_parameters,
+optimizer = OpenAIAdam(model_clf.parameters(),
                        lr=2e-5,
                        warmup=0.1,
                        max_grad_norm=1.0,
@@ -312,38 +312,30 @@ optimizer = OpenAIAdam(optimizer_grouped_parameters,
                        t_total=num_train_optimization_steps)
 
 
-# In[12]:
-
-
-# support fp16
-# [model_A, model_B], optimizer = amp.initialize([model_A, model_B], optimizer, opt_level="O1")
-
-
-# In[13]:
 
 from tqdm import tqdm as tqdm_bar
 update_count = 0
 progress_bar = tqdm.tqdm_notebook
 start = time.time()
-old_ppl = -float('Inf')
-
+best_acc = -float('Inf')
+best_f1 = -float('Inf')
 for ep in tqdm_bar(range(num_epochs)):
 
     "Training"
     pbar = progress_bar(train_dataloader)
-    model_A.train()
-    model_B.train()
-    
+    model_clf.train()
+    ys_true = []
+    ys_pred = []
     for batch in pbar:
-        batch = batch[0]
+        # batch = batch[0]
         # without relative position
-        if sum([len(item) for item in batch[1]]) > 1024:
-            continue
             
-        record_loss, perplexity = train_one_iter(batch, update_count, fp16=False)
-        
+        record_loss, acc, y_true, y_pred = train_one_iter(batch, update_count, fp16=False)
+        # print(f"train acc: {acc}")
+        ys_true.extend(y_true)
+        ys_pred.extend(y_pred)
         update_count += 1
-
+        
         if update_count % num_gradients_accumulation == num_gradients_accumulation - 1:
             # update for gradient accumulation
             optimizer.step()
@@ -355,17 +347,26 @@ for ep in tqdm_bar(range(num_epochs)):
             start = end
             
             # show progress
-            pbar.set_postfix(loss=record_loss, perplexity=perplexity, speed=speed)
-
+            pbar.set_postfix(loss=record_loss, accuracy=acc, speed=speed)
+    
+    from sklearn.metrics import accuracy_score
     "Evaluation"
-    model_A.eval()
-    model_B.eval()
-    ppl = validate(val_dataloader)
+    train_f1 = f1_score(ys_true, ys_pred, average="binary")
+    train_acc = accuracy_score(ys_true, ys_pred)
+    print(f"train acc: {train_acc}, train f1: {train_f1}")
+    model_clf.eval()
+    val_acc, val_f1 = validate(val_dataloader)
     
-    is_best_so_far = ppl > old_ppl
-    old_ppl = ppl
-    checkpointer.save_checkpoint(ep, [model_A.state_dict(), model_B.state_dict()], {"None": None}, is_best_so_far)
-
+    is_best_so_far = val_acc > best_acc
+    if is_best_so_far:
+        best_acc = val_acc
+        torch.save(model_clf.state_dict(), f"Checkpoint_clf/best_acc_{best_acc}_f1_{val_f1}_with_past.pth")
+    if val_f1 > best_f1:
+        best_f1 = val_f1
+        torch.save(model_clf.state_dict(), f"Checkpoint_clf/best_acc_{best_acc}_f1_{best_f1}_with_past.pth")
+    checkpointer.save_checkpoint(ep, model_clf.state_dict(), {"None": None}, is_best_so_far)
+
+print("best acc: {}, best f1: {}".format(best_acc, best_f1))
 
 # In[ ]:
 
diff --git a/training/act_classifier.py b/training/act_classifier.py
new file mode 100644
index 0000000..70df2fc
--- /dev/null
+++ b/training/act_classifier.py
@@ -0,0 +1,415 @@
+
+from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel
+import pandas as pd
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from torch.utils.data import DataLoader, Dataset
+from torch.nn.utils.rnn import pad_sequence
+import numpy as np
+import regex as re
+import random
+import itertools
+import tqdm
+import time
+import os
+
+import config as cfg
+from torch.nn import Identity
+from torch.utils.tensorboard import SummaryWriter
+from apex import amp
+from allennlp.training.checkpointer import Checkpointer
+# from gpt_model import GPT2SimpleLM
+from pytorch_pretrained_bert import OpenAIAdam
+# from torchfly.criterions import SequenceFocalLoss, SequenceCrossEntropyLoss
+from transformers import GPT2Model, GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, AdamW#, WarmupLinearSchedule
+# from transformers.modeling_gpt2 import SequenceSummary
+from torchfly.modules.losses import SequenceFocalLoss, SequenceCrossEntropyLoss
+import pickle as pkl
+import pdb
+# In[2]:
+def save_pkl(obj, dir):
+    with open(dir, "wb") as fh:
+        pkl.dump(obj, fh)
+
+def load_pkl(dir):
+    with open(dir, "rb") as fh:
+        obj = pkl.load(fh)
+    return obj
+
+import tqdm
+def split_train_val():
+    import pickle as pkl
+    df = pd.read_excel("training/data/300_dialog.xlsx")
+    from sklearn import preprocessing
+    le = preprocessing.LabelEncoder()
+    labels = le.fit_transform(df[df['B4']==0]['er_label_1'])
+    df['er_label_1_num'] = None#labels
+    df['er_label_1_num'].loc[df['B4']==0] = labels
+    save_pkl(le, "training/data/labelencoder.pkl")
+    def extract_data(df_dialogs):
+        data = []
+        for i in tqdm.trange(len(df_dialogs)):
+            line = df.iloc[i]
+            if line["B4"] == 0:
+                text = line["Unit"].strip()
+                data.append([text, line['er_label_1_num']])
+            # else:
+            #     text = "B:" + line["Unit"].strip()
+            #     data[line["B2"]].append([text, line['ee_label_1']])                
+        return data
+    all_data = extract_data(df)
+    import random
+    random.seed(123)
+    random.shuffle(all_data)
+    train_data = all_data[:int(len(all_data)*0.8)]
+    val_data = all_data[int(len(all_data)*0.8):]
+    save_pkl(train_data, "training/data/train_data.pkl")
+    save_pkl(val_data, "training/data/val_data.pkl")
+
+class SequenceSummary(nn.Module):
+    r""" Compute a single vector summary of a sequence hidden states according to various possibilities:
+        Args of the config class:
+            summary_type:
+                - 'last' => [default] take the last token hidden state (like XLNet)
+                - 'first' => take the first token hidden state (like Bert)
+                - 'mean' => take the mean of all tokens hidden states
+                - 'cls_index' => supply a Tensor of classification token position (GPT/GPT-2)
+                - 'attn' => Not implemented now, use multi-head attention
+            summary_use_proj: Add a projection after the vector extraction
+            summary_proj_to_labels: If True, the projection outputs to config.num_labels classes (otherwise to hidden_size). Default: False.
+            summary_activation: 'tanh' => add a tanh activation to the output, Other => no activation. Default
+            summary_first_dropout: Add a dropout before the projection and activation
+            summary_last_dropout: Add a dropout after the projection and activation
+    """
+    def __init__(self, config):
+        super().__init__()
+        self.summary_type = config.summary_type if hasattr(config, "summary_type") else "last"
+        if self.summary_type == "attn":
+            # We should use a standard multi-head attention module with absolute positional embedding for that.
+            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276
+            # We can probably just use the multi-head attention module of PyTorch >=1.1.0
+            raise NotImplementedError
+        self.summary = Identity()
+        if hasattr(config, "summary_use_proj") and config.summary_use_proj:
+            if hasattr(config, "summary_proj_to_labels") and config.summary_proj_to_labels and config.num_labels > 0:
+                print(f"num_class: {config.num_labels}")
+                num_classes = config.num_labels
+            else:
+                print(f"num_class here: {config.hidden_size}")
+                num_classes = config.hidden_size
+            self.summary = nn.Linear(config.hidden_size, num_classes)
+        self.activation = Identity()
+        if hasattr(config, "summary_activation") and config.summary_activation == "tanh":
+            self.activation = nn.Tanh()
+        self.first_dropout = Identity()
+        if hasattr(config, "summary_first_dropout") and config.summary_first_dropout > 0:
+            self.first_dropout = nn.Dropout(config.summary_first_dropout)
+        self.last_dropout = Identity()
+        if hasattr(config, "summary_last_dropout") and config.summary_last_dropout > 0:
+            self.last_dropout = nn.Dropout(config.summary_last_dropout)
+    def forward(self, hidden_states, cls_index=None):
+        """ hidden_states: float Tensor in shape [bsz, ..., seq_len, hidden_size], the hidden-states of the last layer.
+            cls_index: [optional] position of the classification token if summary_type == 'cls_index',
+                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.
+                if summary_type == 'cls_index' and cls_index is None:
+                    we take the last token of the sequence as classification token
+        """
+        if self.summary_type == "last":
+            output = hidden_states[:, -1]
+        elif self.summary_type == "first":
+            output = hidden_states[:, 0]
+        elif self.summary_type == "mean":
+            output = hidden_states.mean(dim=1)
+        elif self.summary_type == "cls_index":
+            if cls_index is None:
+                cls_index = torch.full_like(hidden_states[..., :1, :], hidden_states.shape[-2] - 1, dtype=torch.long)
+            else:
+                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)
+                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))
+            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states
+            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)
+        elif self.summary_type == "attn":
+            raise NotImplementedError
+        output = self.first_dropout(output)
+        output = self.summary(output)
+        output = self.activation(output)
+        output = self.last_dropout(output)
+        return output
+
+
+
+torch.backends.cudnn.benchmark = True
+torch.manual_seed(123)
+np.random.seed(123)
+import os
+os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3,4,5,6,7"
+
+class PersuadeDataset(Dataset):
+    def __init__(self, data, tokenizer):
+        self.data = data
+        self.tokenizer = tokenizer
+        self.tokenizer.max_len = 1500
+        # tokenizer weird behavior
+        self.turn_ending = tokenizer.cls_token_id#[628, 198]
+        # tokenizer.encode("\n\n\n")        
+    def __len__(self):
+        return len(self.data)    
+    def __getitem__(self, index):
+        dial_tokens = tokenizer.encode(self.data[index][0]) + [self.turn_ending]
+        cls_token_location = dial_tokens.index(self.tokenizer.cls_token_id)
+        dial_act = self.data[index][1]
+        return dial_tokens, cls_token_location, dial_act        
+    def collate(self, unpacked_data):
+        return unpacked_data
+
+tokenizer = GPT2Tokenizer.from_pretrained("gpt2-medium")
+tokenizer.add_special_tokens({'cls_token': '[CLS]'})
+
+
+class GPT2DoubleHeadsModel_modified(GPT2DoubleHeadsModel):
+    def __init__(self, config):
+        super().__init__(config)
+        # config.num_labels = 1
+        config.num_labels = le.classes_.shape[0]
+        self.transformer = GPT2Model(config)
+        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
+        self.multiple_choice_head = SequenceSummary(config)
+        self.init_weights()
+
+config = GPT2Config()
+config = config.from_pretrained('gpt2-medium')
+le = load_pkl("training/data/labelencoder.pkl")
+config.num_labels = le.classes_.shape[0]
+model_A = GPT2DoubleHeadsModel_modified(config)
+model_A.resize_token_embeddings(len(tokenizer)) 
+# model_B = GPT2DoubleHeadsModel.from_pretrained('gpt2')
+
+device = torch.device("cuda:5")
+torch.cuda.set_device(device)
+model_A = model_A.to(device)
+
+model_A_states, model_B_states = torch.load(cfg.old_medium_model_dir)
+model_A_states['transformer.wte.weight'] = torch.cat([model_A_states['transformer.wte.weight'][:50257,:],
+                                                     torch.randn([1, 1024]).to(device)], dim=0)
+model_A.load_state_dict(model_A_states, strict=False)
+
+# model_B_states['transformer.wte.weight'] = model_B_states['transformer.wte.weight'][:50257,:]
+
+
+# load training data
+train_data = load_pkl("training/data/train_data.pkl")
+val_data = load_pkl("training/data/val_data.pkl")
+
+train_dataset = PersuadeDataset(train_data, tokenizer)
+val_dataset = PersuadeDataset(val_data, tokenizer)
+
+batch_size = 1
+
+train_dataloader = DataLoader(dataset=train_dataset, 
+                              shuffle=True, 
+                              batch_size=batch_size, 
+                              collate_fn=train_dataset.collate)
+val_dataloader = DataLoader(dataset=val_dataset, 
+                            shuffle=False, 
+                            batch_size=batch_size, 
+                            collate_fn=train_dataset.collate)
+
+
+
+
+
+# define the losses
+import torch.nn as nn
+criterion = nn.CrossEntropyLoss()
+# eval_criterion = SequenceCrossEntropyLoss()
+
+# In[9]:
+
+
+def train_one_iter(batch, update_count, fp16=False):
+    dial_tokens, cls_token_location, dial_act = batch
+    input_ids = torch.LongTensor(dial_tokens).unsqueeze(0).to(device)
+    # [torch.LongTensor(item).unsqueeze(0).to(device) for item in dialog_tokens]
+    mc_token_ids = torch.tensor(cls_token_location).unsqueeze(0).to(device)
+    mc_labels = torch.tensor(dial_act).unsqueeze(0).to(device)
+
+    outputs = model_A(input_ids, mc_token_ids=mc_token_ids)
+    lm_prediction_scores, mc_logits = outputs[:2]
+    loss = criterion(mc_logits.view(-1, mc_logits.size(-1)), mc_labels.view(-1))
+
+    # past = None
+    # all_logits = []
+   
+    # for turn_num, (dial_turn_inputs, dialog_turn_act) in enumerate(zip(dial_inputs, dialog_acts)):
+    #     # if role_ids[turn_num] == 0:
+    #     #     # breakpoint()
+    #     #     logits, past = model_A(dial_turn_inputs, past=past)
+    #     #     all_logits.append(logits)
+    #     # else:
+    #     #     # breakpoint()
+    #     #     logits, past = model_B(dial_turn_inputs, past=past)
+    #     #     all_logits.append(logits)
+
+    # all_logits = torch.cat(all_logits, dim=1) # torch.Size([1, 505, 50260]), 505 = sum of tokens from 21 sentences
+    
+    
+    
+    # # target
+    # all_logits = all_logits[:, :-1].contiguous() # torch.Size([1, 504, 50260])
+    # target = torch.cat(dial_inputs, dim=1)[:, 1:].contiguous()# torch.Size([1, 504])
+    # target_mask = torch.ones_like(target).float()# torch.Size([1, 504])
+    
+    loss /= num_gradients_accumulation
+    
+    if fp16:
+        with amp.scale_loss(loss, optimizer) as scaled_loss:
+            scaled_loss.backward()
+    else:
+        loss.backward()
+        
+    record_loss = loss.item() * num_gradients_accumulation
+    # print("record_loss: {}".format(record_loss))
+    # perplexity = np.exp(record_loss)
+    
+    return record_loss#, perplexity
+
+from sklearn.metrics import f1_score
+from sklearn.metrics import confusion_matrix
+from utils import print_cm
+
+def validate(dataloader):
+    with torch.no_grad():
+        pbar = progress_bar(dataloader)
+
+        correct = 0
+        total = 0
+        y_true, y_pred = [], []
+
+        for batch in pbar:
+            
+            dial_tokens, cls_token_location, dial_act = batch[0]
+            input_ids = torch.LongTensor(dial_tokens).unsqueeze(0).to(device)
+            # [torch.LongTensor(item).unsqueeze(0).to(device) for item in dialog_tokens]
+            mc_token_ids = torch.tensor(cls_token_location).unsqueeze(0).to(device)
+            labels = torch.tensor(dial_act).unsqueeze(0).to(device)
+
+            outputs = model_A(input_ids, mc_token_ids=mc_token_ids)
+            lm_prediction_scores, mc_logits = outputs[:2]
+            y_true.extend([dial_act])
+            _, predicted_labels = torch.max(outputs, 1)
+            y_pred.extend(predicted_labels.tolist())
+
+            total += labels.size(0)
+            correct += (predicted_labels == labels).sum().item()
+        f1 = f1_score(y_true, y_pred, average="binary")
+        print(f"Epcoh {ep} Validation accuracy: {correct/total}, f1: {f1}")
+        # print_cm(confusion_matrix(y_true, y_pred, labels=range(len(le.classes_))), labels=[l[-5:] for l in le.classes_.tolist()])
+        return correct/total, f1
+
+
+# ### Training
+
+# In[10]:
+
+
+checkpointer = Checkpointer(serialization_dir="Checkpoint_act_clf", 
+                            keep_serialized_model_every_num_seconds=3600*2, 
+                            num_serialized_models_to_keep=5)
+
+
+# In[11]:
+
+
+# optimizer
+num_epochs = 10
+num_gradients_accumulation = 1
+num_train_optimization_steps = num_train_optimization_steps = len(train_dataset) * num_epochs // batch_size // num_gradients_accumulation
+
+param_optimizer = list(model_A.named_parameters()) 
+no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
+optimizer_grouped_parameters = [
+    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
+    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
+    ]
+
+
+optimizer = OpenAIAdam(optimizer_grouped_parameters,
+                       lr=2e-5,
+                       warmup=0.1,
+                       max_grad_norm=1.0,
+                       weight_decay=0.01,
+                       t_total=num_train_optimization_steps)
+
+
+# In[12]:
+
+
+# support fp16
+# [model_A, model_B], optimizer = amp.initialize([model_A, model_B], optimizer, opt_level="O1")
+
+
+# In[13]:
+
+import tqdm 
+update_count = 0
+progress_bar = tqdm.tqdm
+start = time.time()
+best_acc = -float('Inf')
+best_f1 = -float('Inf')
+
+for ep in tqdm_bar(range(num_epochs)):
+
+    "Training"
+    pbar = progress_bar(train_dataloader)
+    model_A.train()
+    # model_B.train()
+    
+    for batch in pbar:
+        batch = batch[0]
+        # without relative position
+        # if sum([len(item) for item in batch[1]]) > 1024:
+        #     continue
+            
+        record_loss = train_one_iter(batch, update_count, fp16=False)
+        
+        update_count += 1
+
+        if update_count % num_gradients_accumulation == num_gradients_accumulation - 1:
+            # update for gradient accumulation
+            optimizer.step()
+            optimizer.zero_grad()
+            
+            # speed measure
+            end = time.time()
+            speed = batch_size * num_gradients_accumulation / (end - start)
+            start = end
+            
+            # show progress
+            pbar.set_postfix(loss=record_loss, speed=speed)
+
+    "Evaluation"
+    model_A.eval()
+    # model_B.eval()
+    val_acc, val_f1 = validate(val_dataloader)
+    print(f"val f1: {val_f1}, valid acc: {val_acc}")
+    is_best_so_far = val_f1 > best_f1
+    # if is_best_so_far:
+    #     best_acc = val_acc
+    #     # torch.save(model_clf.state_dict(), f"Checkpoint_clf/best_acc_{best_acc}_f1_{val_f1}_with_past.pth")
+    if is_best_so_far:
+        best_f1 = val_f1
+        torch.save(model_A.state_dict(), f"Checkpoint_act_clf/best_acc_{best_acc}_f1_{best_f1}.pth")
+        # checkpointer.save_checkpoint(ep, model_A.state_dict(), {"None": None}, is_best_so_far)
+
+print("best acc: {}, best f1: {}".format(best_acc, best_f1))
+
+
+
+
+
+# In[ ]:
+
+
+
+
diff --git a/training/data/300_dialog.xlsx b/training/data/300_dialog.xlsx
new file mode 100644
index 0000000..263e6f7
Binary files /dev/null and b/training/data/300_dialog.xlsx differ
diff --git a/training/test.py b/training/test.py
new file mode 100644
index 0000000..a83d0a2
--- /dev/null
+++ b/training/test.py
@@ -0,0 +1,22 @@
+import torch
+from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel
+
+tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
+model = GPT2DoubleHeadsModel.from_pretrained('gpt2')
+
+# Add a [CLS] to the vocabulary (we should train it also!)
+tokenizer.add_special_tokens({'cls_token': '[CLS]'})
+model.resize_token_embeddings(len(tokenizer))  # Update the model embeddings with the new vocabulary size
+print(tokenizer.cls_token_id, len(tokenizer))  # The newly token the last token of the vocabulary
+
+choices = ["Hello, my dog is cute [CLS]", "Hello, my cat is cute [CLS]"]
+encoded_choices = [tokenizer.encode(s) for s in choices]
+cls_token_location = [tokens.index(tokenizer.cls_token_id) 
+for tokens in encoded_choices]
+
+input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2
+mc_token_ids = torch.tensor([cls_token_location])  # Batch size: 1
+
+outputs = model(input_ids, mc_token_ids=mc_token_ids)
+lm_prediction_scores, mc_prediction_scores = outputs[:2]
+
diff --git a/utils.py b/utils.py
index 0c540ed..70315dc 100644
--- a/utils.py
+++ b/utils.py
@@ -58,14 +58,14 @@ def is_repetition_with_context(sent, context_list, threshold=0.5):
         is_match, ratio = is_ci_token_stopword_set_match(sent, c_sent, threshold=threshold)
         max_ratio = max(ratio, max_ratio)
         if is_match:
-            if cfg.debug:
+            if cfg.verbose:
                 print("\n\n\n--- repetition occurs between these sents: ratio {} ---".format(ratio))
                 print("|context: {}|\n|candidate: {}|".format(c_sent, sent))
                 print("---------------------------------------------\n\n\n")
 
-                logging.debug("\n\n\n--- repetition occurs between these sents: ratio {} ---".format(ratio))
-                logging.debug("|context: {}|\n|candidate: {}|".format(c_sent, sent))
-                logging.debug("---------------------------------------------\n\n\n")
+            logging.debug("\n\n\n--- repetition occurs between these sents: ratio {} ---".format(ratio))
+            logging.debug("|context: {}|\n|candidate: {}|".format(c_sent, sent))
+            logging.debug("---------------------------------------------\n\n\n")
             return True, max_ratio
     return False, max_ratio
 
@@ -122,3 +122,37 @@ def compare_two_sent_embeddings(sent1, sent2):
     sent_embeddings = model.encode(sents)
 
     return cosine_similarity(sent_embeddings[0].T.reshape(1,-1), sent_embeddings[1].T.reshape(1, -1))
+
+import pickle as pkl
+
+def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):
+    """pretty print for confusion matrixes"""
+    columnwidth = max([len(x) for x in labels] + [5])  # 5 is value length
+    empty_cell = " " * columnwidth    
+    # Begin CHANGES
+    fst_empty_cell = (columnwidth-3)//2 * " " + "t/p" + (columnwidth-3)//2 * " "
+    
+    if len(fst_empty_cell) < len(empty_cell):
+        fst_empty_cell = " " * (len(empty_cell) - len(fst_empty_cell)) + fst_empty_cell
+    # Print header
+    print("    " + fst_empty_cell, end=" ")
+    # End CHANGES    
+    for label in labels:
+        print("%{0}s".format(columnwidth) % label, end=" ")        
+    print()
+    # Print rows
+    for i, label1 in enumerate(labels):
+        print("    %{0}s".format(columnwidth) % label1, end=" ")
+        for j in range(len(labels)):
+            cell = "%{0}.1f".format(columnwidth) % cm[i, j]
+            if hide_zeroes:
+                cell = cell if float(cm[i, j]) != 0 else empty_cell
+            if hide_diagonal:
+                cell = cell if i != j else empty_cell
+            if hide_threshold:
+                cell = cell if cm[i, j] > hide_threshold else empty_cell
+            print(cell, end=" ")
+        print()
+
+
+
